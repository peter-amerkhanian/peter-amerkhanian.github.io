
@book{strang_calculus_2016,
	title = {Calculus {Volume} 1},
	isbn = {978-1-938168-02-4},
	abstract = {"Calculus Volume 1 is designed for the two- or three-semester calculus course. For many students, this course provides the foundation to a career in mathematics, science, or engineering. As such, this textbook provides an important opportunity for students to learn the core concepts of calculus and understand how those concepts apply to their lives and the world around them. The text has been developed to meet the scope and sequence of most general calculus courses. At the same time, the book includes several innovative features designed to enhance student learning. A strength of Calculus Volume 1 is that instructors can customize the book, adapting it to the approach that works best in their classroom."--Preface},
	language = {en},
	publisher = {OpenStax College},
	author = {Strang, Gilbert and Herman, Edwin},
	month = mar,
	year = {2016},
	note = {Google-Books-ID: By72jwEACAAJ},
}

@misc{noauthor_newtons_2023,
	title = {Newton's method},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Newton%27s_method&oldid=1177135929},
	language = {en},
	urldate = {2023-11-03},
	journal = {Wikipedia},
	month = sep,
	year = {2023},
	note = {Page Version ID: 1177135929},
	file = {Snapshot:C\:\\Users\\peteramerkhanian\\Zotero\\storage\\BPRKSHMF\\Newton's_method.html:text/html},
}

@book{strang_calculus_2016-1,
	title = {Calculus {Volume} 3},
	isbn = {978-1-938168-07-9},
	abstract = {Calculus is designed for the typical two- or three-semester general calculus course, incorporating innovative features to enhance student learning. The book guides students through the core concepts of calculus and helps them understand how those concepts apply to their lives and the world around them. Due to the comprehensive nature of the material, we are offering the book in three volumes for flexibility and efficiency. Volume 3 covers parametric equations and polar coordinates, vectors, functions of several variables, multiple integration, and second-order differential equations.},
	language = {en},
	publisher = {OpenStax},
	author = {Strang, Gilbert and Herman, Edwin},
	month = mar,
	year = {2016},
	note = {Google-Books-ID: 7nu6DAEACAAJ},
	keywords = {Education / General, Mathematics / General},
}

@misc{noauthor_linear_2023,
	title = {Linear approximation},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Linear_approximation&oldid=1184279580#Optics},
	abstract = {In mathematics, a linear approximation is an approximation of a general function using a linear function (more precisely, an affine function). They are widely used in the method of finite differences to produce first order methods for solving or approximating solutions to equations.},
	language = {en},
	urldate = {2024-03-17},
	journal = {Wikipedia},
	month = nov,
	year = {2023},
	note = {Page Version ID: 1184279580},
	file = {Snapshot:C\:\\Users\\peteramerkhanian\\Zotero\\storage\\9Y3PIDQT\\Linear_approximation.html:text/html},
}

@book{blitzstein_introduction_2019,
	address = {Boca Raton},
	edition = {2nd edition},
	title = {Introduction to {Probability}, {Second} {Edition}},
	isbn = {978-1-138-36991-7},
	abstract = {Developed from celebrated Harvard statistics lectures, Introduction to Probability provides essential language and toolsfor understanding statistics, randomness, and uncertainty. The book explores a wide variety of applications and examples, ranging from coincidences and paradoxes to Google PageRank and Markov chain Monte Carlo (MCMC). Additional application areas explored include genetics, medicine, computer science, and information theory. The authors present the material in an accessible style and motivate concepts using real-world examples. Throughout, they use stories to uncover connections between the fundamental distributions in statistics and conditioning to reduce complicated problems to manageable pieces.The book includes many intuitive explanations, diagrams, and practice problems. Each chapter ends with a section showing how to perform relevant simulations and calculations in R, a free statistical software environment.The second edition adds many new examples, exercises, and explanations, to deepen understanding of the ideas, clarify subtle concepts, and respond to feedback from many students and readers. New supplementary online resources have been developed, including animations and interactive visualizations, and the book has been updated to dovetail with these resources. Supplementary material is available on Joseph Blitzstein’s website www. stat110.net. The supplements include:Solutions to selected exercisesAdditional practice problemsHandouts including review material and sample exams Animations and interactive visualizations created in connection with the edX online version of Stat 110.Links to lecture videos available on ITunes U and YouTube There is also a complete instructor's solutions manual available to instructors who require the book for a course.},
	language = {English},
	publisher = {Chapman and Hall/CRC},
	author = {Blitzstein, Joseph K. and Hwang, Jessica},
	month = feb,
	year = {2019},
}

@book{cohen_practical_2022,
	address = {Beijing},
	edition = {1st edition},
	title = {Practical {Linear} {Algebra} for {Data} {Science}: {From} {Core} {Concepts} to {Applications} {Using} {Python}},
	isbn = {978-1-09-812061-0},
	shorttitle = {Practical {Linear} {Algebra} for {Data} {Science}},
	abstract = {If you want to work in any computational or technical field, you need to understand linear algebra. As the study of matrices and operations acting upon them, linear algebra is the mathematical basis of nearly all algorithms and analyses implemented in computers. But the way it's presented in decades-old textbooks is much different from how professionals use linear algebra today to solve real-world modern applications.  This practical guide from Mike X Cohen teaches the core concepts of linear algebra as implemented in Python, including how they're used in data science, machine learning, deep learning, computational simulations, and biomedical data processing applications. Armed with knowledge from this book, you'll be able to understand, implement, and adapt myriad modern analysis methods and algorithms.  Ideal for practitioners and students using computer technology and algorithms, this book introduces you to: The interpretations and applications of vectors and matrices Matrix arithmetic (various multiplications and transformations) Independence, rank, and inverses Important decompositions used in applied linear algebra (including LU and QR) Eigendecomposition and singular value decomposition Applications including least-squares model fitting and principal components analysis},
	language = {English},
	publisher = {O'Reilly Media},
	author = {Cohen, Mike},
	month = oct,
	year = {2022},
}

@book{james_introduction_2013,
	address = {New York Heidelberg Dordrecht London},
	edition = {1st ed. 2013, Corr. 7th printing 2017 edition},
	title = {An {Introduction} to {Statistical} {Learning}: with {Applications} in {R}},
	isbn = {978-1-4614-7137-0},
	shorttitle = {An {Introduction} to {Statistical} {Learning}},
	abstract = {An Introduction to Statistical Learning provides an accessible overview of the field of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fields ranging from biology to finance to marketing to astrophysics in the past twenty years. This book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in science, industry, and other fields, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical software platform.Two of the authors co-wrote The Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2nd edition 2009), a popular reference book for statistics and machine learning researchers. An Introduction to Statistical Learning covers many of the same topics, but at a level accessible to a much broader audience. This book is targeted at statisticians and non-statisticians alike who wish to use cutting-edge statistical learning techniques to analyze their data. The text assumes only a previous course in linear regression and no knowledge of matrix algebra.},
	language = {English},
	publisher = {Springer},
	author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	month = jun,
	year = {2013},
}

@book{hastie_elements_2016,
	address = {New York, NY},
	edition = {2nd edition},
	title = {The {Elements} of {Statistical} {Learning}: {Data} {Mining}, {Inference}, and {Prediction}, {Second} {Edition}},
	isbn = {978-0-387-84857-0},
	shorttitle = {The {Elements} of {Statistical} {Learning}},
	abstract = {This book describes the important ideas in a variety of fields such as medicine, biology, finance, and marketing in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of colour graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book.This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression \& path algorithms for the lasso, non-negative matrix factorisation, and spectral clustering. There is also a chapter on methods for "wide'' data (p bigger than n), including multiple testing and false discovery rates.},
	language = {English},
	publisher = {Springer},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	month = jan,
	year = {2016},
}

@book{kuttler_first_2017,
	edition = {Version 2017 Revision A edition},
	title = {A {First} {Course} in {Linear} {Algebra}},
	isbn = {978-1-5428-9552-1},
	abstract = {This text, originally by K. Kuttler, has been redesigned by the Lyryx editorial team as a first course in linear algebra for science and engineering students who have an understanding of basic algebra. All major topics of linear algebra are available in detail, as well as proofs of important theorems. In addition, connections to topics covered in advanced courses are introduced. The text is designed in a modular fashion to maximize flexibility and facilitate adaptation to a given course outline and student profile. Each chapter begins with a list of student learning outcomes, and examples and diagrams are given throughout the text to reinforce ideas and provide guidance on how to approach various problems. Suggested exercises are included at the end of each section, with selected answers at the end of the text. Lyryx develops and supports open texts, with editorial services to adapt the text for each particular course. In addition, Lyryx provides content-specific formative online assessment, a wide variety of supplements, and in-house support available 7 days/week for both students and instructors.},
	language = {English},
	publisher = {CreateSpace Independent Publishing Platform},
	author = {Kuttler, Ken},
	editor = {Learning, Lyryx},
	month = feb,
	year = {2017},
}

@misc{noauthor_cumulative_2024,
	title = {Cumulative distribution function},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Cumulative_distribution_function&oldid=1215839028},
	abstract = {In probability theory and statistics, the cumulative distribution function (CDF) of a real-valued random variable 
  
    
      
        X
      
    
    \{{\textbackslash}displaystyle X\}
  
, or just distribution function of 
  
    
      
        X
      
    
    \{{\textbackslash}displaystyle X\}
  
, evaluated at 
  
    
      
        x
      
    
    \{{\textbackslash}displaystyle x\}
  
, is the probability that 
  
    
      
        X
      
    
    \{{\textbackslash}displaystyle X\}
  
 will take a value less than or equal to 
  
    
      
        x
      
    
    \{{\textbackslash}displaystyle x\}
  
.
Every probability distribution supported on the real numbers, discrete or "mixed" as well as continuous, is uniquely identified by a right-continuous monotone increasing function (a càdlàg function) 
  
    
      
        F
        :
        
          R
        
        →
        [
        0
        ,
        1
        ]
      
    
    \{{\textbackslash}displaystyle F{\textbackslash}colon {\textbackslash}mathbb \{R\} {\textbackslash}rightarrow [0,1]\}
  
 satisfying 
  
    
      
        
          lim
          
            x
            →
            −
            ∞
          
        
        F
        (
        x
        )
        =
        0
      
    
    \{{\textbackslash}displaystyle {\textbackslash}lim \_\{x{\textbackslash}rightarrow -{\textbackslash}infty \}F(x)=0\}
  
 and 
  
    
      
        
          lim
          
            x
            →
            ∞
          
        
        F
        (
        x
        )
        =
        1
      
    
    \{{\textbackslash}displaystyle {\textbackslash}lim \_\{x{\textbackslash}rightarrow {\textbackslash}infty \}F(x)=1\}
  
.
In the case of a scalar continuous distribution, it gives the area under the probability density function from negative infinity to 
  
    
      
        x
      
    
    \{{\textbackslash}displaystyle x\}
  
. Cumulative distribution functions are also used to specify the distribution of multivariate random variables.},
	language = {en},
	urldate = {2024-04-27},
	journal = {Wikipedia},
	month = mar,
	year = {2024},
	note = {Page Version ID: 1215839028},
	file = {Snapshot:C\:\\Users\\peteramerkhanian\\Zotero\\storage\\5XGAYN28\\Cumulative_distribution_function.html:text/html},
}

@book{noauthor_chapter_nodate,
	title = {Chapter 9 {ARIMA} models {\textbar} {Forecasting}: {Principles} and {Practice} (3rd ed)},
	shorttitle = {Chapter 9 {ARIMA} models {\textbar} {Forecasting}},
	url = {https://otexts.com/fpp3/arima.html},
	abstract = {3rd edition},
	urldate = {2024-06-22},
	file = {Snapshot:C\:\\Users\\peteramerkhanian\\Zotero\\storage\\V3HAC885\\arima.html:text/html},
}

@book{hyndman_forecasting_2021,
	address = {Melbourne, Australia},
	edition = {3rd ed. edition},
	title = {Forecasting: {Principles} and {Practice}},
	isbn = {978-0-9875071-3-6},
	shorttitle = {Forecasting},
	abstract = {Forecasting is required in many situations. Deciding whether to build another power generation plant in the next five years requires forecasts of future demand. Scheduling staff in a call centre next week requires forecasts of call volumes. Stocking an inventory requires forecasts of stock requirements. Telecommunication routing requires traffic forecasts a few minutes ahead. Whatever the circumstances or time horizons involved, forecasting is an important aid in effective and efficient planning. This textbook provides a comprehensive introduction to forecasting methods and presents enough information about each method for readers to use them sensibly. Examples use R with many data sets taken from the authors' own consulting experience. In this third edition, all chapters have been updated to cover the latest research and forecasting methods. One new chapter has been added on time series features. The latest version of the book is freely available online at http: //OTexts.com/fpp3.},
	language = {English},
	publisher = {Otexts},
	author = {Hyndman, Rob J. and Athanasopoulos, George},
	month = may,
	year = {2021},
}

@book{walker_analyzing_2023,
	address = {Boca Raton},
	edition = {1st edition},
	title = {Analyzing {US} {Census} {Data}},
	isbn = {978-1-03-236644-9},
	abstract = {Census data is widely used by practitioners to understand demographic change, allocate resources, address inequalities, and make sound business decisions. Until recently, projects using US Census data have required proficiency with multiple web interfaces and software platforms to prepare, map, and present data products. This book introduces readers to tools in the R programming language for accessing and analyzing Census data and shows how to carry out demographic analyses in a single computing environment.Chapters in this book cover the following key topics:• Rapidly acquiring data from the decennial US Census and American Community Survey using R, then analyzing these datasets using tidyverse tools;• Visualizing US Census data with a wide range of methods including charts in ggplot2 as well as both static and interactive maps;• Using R as a geographic information system (GIS) to manage, analyze, and model spatial demographic data from the US Census;• Working with and modeling individual-level microdata from the American Community Survey’s PUMS datasets;• Applying these tools and workflows to the analysis of historical Census data, other US government datasets, and international Census data from countries like Canada, Brazil, Kenya, and Mexico.},
	language = {English},
	publisher = {Chapman and Hall/CRC},
	author = {Walker, Kyle},
	month = feb,
	year = {2023},
}

@misc{greg_freedman_ellis_ipumsr_2024,
	title = {ipumsr: {An} {R} {Interface} for {Downloading}, {Reading}, and {Handling} {IPUMS} {Data}},
	url = {https://tech.popdata.org/ipumsr/},
	author = {{Greg Freedman Ellis} and {Derek Burk} and {Finn Roberts}},
	year = {2024},
	annote = {R package version 0.8.1, https://github.com/ipums/ipumsr, https://www.ipums.org},
}

@misc{ruggles_ipums_2024,
	address = {Minneapolis, MN},
	title = {{IPUMS} {USA}: {Version} 15.0},
	url = {https://doi.org/10.18128/D010.V15.0},
	publisher = {IPUMS},
	author = {Ruggles, Steven and Flood, Sarah and Sobek, Matthew and Backman, Daniel and Chen, Annie and Cooper, Grace and Richards, Stephanie and Rogers, Renae and Schouweiler, Megan},
	year = {2024},
}

@misc{flood_integrated_2024,
	address = {Minneapolis, MN},
	title = {Integrated {Public} {Use} {Microdata} {Series}, {Current} {Population} {Survey}: {Version} 12.0},
	url = {https://doi.org/10.18128/D030.V12.0},
	publisher = {IPUMS},
	author = {Flood, Sarah and King, Miriam and Rodgers, Renae and Ruggles, Steven and Warren, J. Robert and Backman, Daniel and Chen, Annie and Cooper, Grace and Richards, Stephanie and Schouweiler, Megan and Westberry, Michael},
	year = {2024},
}

@misc{pastoor_how_2024,
	title = {How can {I} pull data at the zip code or city level?},
	shorttitle = {How can {I} pull data at the zip code or city level?},
	url = {https://forum.ipums.org/t/how-can-i-pull-data-at-the-zip-code-or-city-level/5650/2},
	abstract = {IPUMS USA provides microdata from the U.S. decennial census and American Community Survey. Microdata are person or household level data. The smallest geographic area explicitly identified in the microdata is public use microdata area (PUMA), a geographic area defined based on population. Each PUMA includes about 100,000 residents. IPUMS geographers are sometimes able to infer other small geographic areas, such as city, metro area, and county, from PUMA, depending on how the boundaries of PUMAs a...},
	language = {en},
	urldate = {2024-11-16},
	journal = {IPUMS Forum},
	author = {Pastoor, Isabel},
	month = feb,
	year = {2024},
	file = {Snapshot:C\:\\Users\\peteramerkhanian\\Zotero\\storage\\KRIW2HEN\\2.html:text/html},
}

@misc{mihalik_missouri_2022,
	title = {The {Missouri} {Census} {Data} {Center} ({MCDC}) 2022 {Geographic} {Correspondence} {Engine} ({GEOCORR})},
	url = {https://mcdc.missouri.edu/applications/geocorr.html},
	abstract = {The Missouri Census Data Center (MCDC) is a cooperative program among state agencies in Missouri and the U.S. Bureau of the Census. Its purpose is to enhance awareness of and access to public data, especially Census Bureau products.},
	urldate = {2024-11-16},
	author = {Mihalik, Cory and Rice, Glenn and Hesser, Matt},
	month = oct,
	year = {2022},
	file = {Geocorr 2022 - MCDC:C\:\\Users\\peteramerkhanian\\Zotero\\storage\\VQZP3XSJ\\geocorr2022.html:text/html},
}

@article{noauthor_worked_2019,
	title = {Worked {Examples} for {Approximating} {Standard} {Errors} {Using} {American} {Community} {Survey} {Data}},
	language = {en},
	year = {2019},
	file = {Worked Examples for Approximating Standard Errors .pdf:C\:\\Users\\peteramerkhanian\\Zotero\\storage\\UL9X92YU\\Worked Examples for Approximating Standard Errors .pdf:application/pdf},
}

@inproceedings{fay_aspects_1995,
	address = {Alexandria, VA},
	title = {Aspects of {Survey} and {Model}-{Based} {Postcensal} {Estimation} of {Income} and {Poverty} {Characteristics} for {States} and {Counties}},
	abstract = {Congress has charged the Census Bureau with the responsibility to develop postcensal estimates of basic income and poverty statistics by state, county, and subcounty areas. The March Supplement to the Current Population Survey (CPS), based on a sample size of about 65,000 households, provides national estimates for these characteristics annually, but the sample size is insufficient to provide reliable direct estimates at all or even most levels of detail requested by Congress. The paper reports on two aspects of the estimation problem: 1) variance estimation for the CPS sample estimates, and 2) features of some candidate small area models that might be used to produce the required estimates at the state and county level. The two problems are interrelated: many small area models employ estimated variances for the survey estimates both in the estimation of the small area models and in their evaluation. The paper reports progress in developing a first set of estimates to meet the Congressional request.},
	language = {en},
	author = {Fay, Robert E and Train, George F},
	year = {1995},
	pages = {154--159},
	file = {Fay and Train - ASPECTS OF SURVEY AND MODEL-BASED POSTCENSAL ESTIM.pdf:C\:\\Users\\peteramerkhanian\\Zotero\\storage\\AT88IMB8\\Fay and Train - ASPECTS OF SURVEY AND MODEL-BASED POSTCENSAL ESTIM.pdf:application/pdf},
}

@misc{us_census_bureau_calculating_2020,
	title = {Calculating {Margins} of {Error} the {ACS} {Way}: {Using} {Replicate} {Methodology} to {Calculate} {Uncertainty}},
	url = {https://www.youtube.com/watch?v=nM2RZJag320},
	abstract = {Learn how the American Community Survey (ACS) calculates margin of error (MOE) for estimates published on data.census.gov. See how to calculate MOEs when combining ACS estimates within a table or across geographies using the ACS Variance Replicate Estimates (VRE) tables. 

In addition, learn how to calculate your own estimates and MOEs using the Public Use Microdata Sample (PUMS) data.},
	urldate = {2025-01-05},
	author = {{U.S. Census Bureau}},
	month = feb,
	year = {2020},
}

@misc{noauthor_weighted_2025,
	title = {Weighted arithmetic mean},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Weighted_arithmetic_mean&oldid=1267190443#Variance_of_the_weighted_mean_(%CF%80-estimator_for_ratio-mean)},
	abstract = {The weighted arithmetic mean is similar to an ordinary arithmetic mean (the most common type of average), except that instead of each of the data points contributing equally to the final average, some data points contribute more than others. The notion of weighted mean plays a role in descriptive statistics and also occurs in a more general form in several other areas of mathematics.
If all the weights are equal, then the weighted mean is the same as the arithmetic mean.  While weighted means generally behave in a similar fashion to arithmetic means, they do have a few counterintuitive properties, as captured for instance in Simpson's paradox.},
	language = {en},
	urldate = {2025-01-16},
	journal = {Wikipedia},
	month = jan,
	year = {2025},
	note = {Page Version ID: 1267190443},
	file = {Snapshot:C\:\\Users\\peteramerkhanian\\Zotero\\storage\\CDWKMTTI\\Weighted_arithmetic_mean.html:text/html},
}

@misc{noauthor_sas_nodate,
	title = {{SAS} {Help} {Center}: {Replication} {Methods} for {Variance} {Estimation}},
	url = {https://documentation.sas.com/doc/en/statcdc/14.2/statug/statug_surveymeans_details49.htm#statug.surveymeans.faymethoddetails},
	urldate = {2025-01-17},
	file = {SAS Help Center\: Replication Methods for Variance Estimation:C\:\\Users\\peteramerkhanian\\Zotero\\storage\\JXME6VVB\\statug_surveymeans_details49.html:text/html},
}

@article{lumley_analysis_2004,
	title = {Analysis of {Complex} {Survey} {Samples}},
	volume = {9},
	copyright = {Copyright (c) 2003 Thomas Lumley},
	issn = {1548-7660},
	url = {https://doi.org/10.18637/jss.v009.i08},
	doi = {10.18637/jss.v009.i08},
	abstract = {I present software for analysing complex survey samples in R. The sampling scheme can be explicitly described or represented by replication weights. Variance estimation uses either replication or linearisation.},
	language = {en},
	urldate = {2025-02-04},
	journal = {Journal of Statistical Software},
	author = {Lumley, Thomas},
	month = apr,
	year = {2004},
	pages = {1--19},
	file = {Submitted Version:C\:\\Users\\peteramerkhanian\\Zotero\\storage\\GWZVQFF2\\Lumley - 2004 - Analysis of Complex Survey Samples.pdf:application/pdf},
}

@book{lumley_complex_2010,
	address = {Hoboken, N.J},
	edition = {1st edition},
	title = {Complex {Surveys}: {A} {Guide} to {Analysis} {Using} {R}},
	isbn = {978-0-470-28430-8},
	shorttitle = {Complex {Surveys}},
	abstract = {A complete guide to carrying out complex survey analysis using RAs survey analysis continues to serve as a core component of sociological research, researchers are increasingly relying upon data gathered from complex surveys to carry out traditional analyses. Complex Surveys is a practical guide to the analysis of this kind of data using R, the freely available and downloadable statistical programming language. As creator of the specific survey package for R, the author provides the ultimate presentation of how to successfully use the software for analyzing data from complex surveys while also utilizing the most current data from health and social sciences studies to demonstrate the application of survey research methods in these fields.The book begins with coverage of basic tools and topics within survey analysis such as simple and stratified sampling, cluster sampling, linear regression, and categorical data regression. Subsequent chapters delve into more technical aspects of complex survey analysis, including post-stratification, two-phase sampling, missing data, and causal inference. Throughout the book, an emphasis is placed on graphics, regression modeling, and two-phase designs. In addition, the author supplies a unique discussion of epidemiological two-phase designs as well as probability-weighting for causal inference. All of the book's examples and figures are generated using R, and a related Web site provides the R code that allows readers to reproduce the presented content. Each chapter concludes with exercises that vary in level of complexity, and detailed appendices outline additional mathematical and computational descriptions to assist readers with comparing results from various software systems.Complex Surveys is an excellent book for courses on sampling and complex surveys at the upper-undergraduate and graduate levels. It is also a practical reference guide for applied statisticians and practitioners in the social and health sciences who use statistics in their everyday work.},
	language = {English},
	publisher = {Wiley},
	author = {Lumley, Thomas},
	month = mar,
	year = {2010},
}

@book{lohr_sampling_2021,
	address = {Boca Raton},
	edition = {3rd edition},
	title = {Sampling: {Design} and {Analysis}},
	isbn = {978-0-367-27950-9},
	shorttitle = {Sampling},
	abstract = {"The level is appropriate for an upper-level undergraduate or graduate-level statistics major. Sampling: Design and Analysis (SDA) will also benefit a non-statistics major with a desire to understand the concepts of sampling from a finite population. A student with patience to delve into the rigor of survey statistics will gain even more from the content that SDA offers. The updates to SDA have potential to enrich traditional survey sampling classes at both the undergraduate and graduate levels. The new discussions of low response rates, non-probability surveys, and internet as a data collection mode hold particular value, as these statistical issues have become increasingly important in survey practice in recent years… I would eagerly adopt the new edition of SDA as the required textbook." (Emily Berg, Iowa State University)What is the unemployment rate? What is the total area of land planted with soybeans? How many persons have antibodies to the virus causing COVID-19? Sampling: Design and Analysis, Third Edition shows you how to design and analyze surveys to answer these and other questions. This authoritative text, used as a standard reference by numerous survey organizations, teaches the principles of sampling with examples from social sciences, public opinion research, public health, business, agriculture, and ecology. Readers should be familiar with concepts from an introductory statistics class including probability and linear regression; optional sections contain statistical theory for readers familiar with mathematical statistics.Key Features:Has been thoroughly revised to incorporate recent research and applications.Includes a new chapter on nonprobability samples, and more than 200 new examples and exercises have been added.Teaches the principles of sampling with examples from social sciences, public opinion research, public health, business, agriculture, and ecology. SDA’s companion website contains data sets, computer code, and links to two free downloadable supplementary books (also available in paperback) that provide step-by-step guides―with code, annotated output, and helpful tips―for working through the SDA examples. Instructors can use either R or SAS® software.SAS® Software Companion for Sampling: Design and Analysis, Third Edition by Sharon L. Lohr (2022, CRC Press)R Companion for Sampling: Design and Analysis, Third Edition by Yan Lu and Sharon L. Lohr (2022, CRC Press)},
	language = {English},
	publisher = {Chapman and Hall/CRC},
	author = {Lohr, Sharon L.},
	month = nov,
	year = {2021},
}

@misc{noauthor_replicate_nodate,
	title = {Replicate {Weights} in the {American} {Community} {Survey} / {Puerto} {Rican} {Community} {Survey}},
	url = {https://usa.ipums.org/usa/repwt.shtml},
	urldate = {2025-02-14},
	journal = {IPUMS USA},
	file = {IPUMS USA:C\:\\Users\\peteramerkhanian\\Zotero\\storage\\8G5DDS57\\repwt.html:text/html},
}

@misc{replicat,
	title = {Replicate Weights in the American Community Survey / Puerto Rican Community Survey},
	url = {https://usa.ipums.org/usa/repwt.shtml}
}
