
@book{strang_calculus_2016,
	title = {Calculus {Volume} 1},
	isbn = {978-1-938168-02-4},
	abstract = {"Calculus Volume 1 is designed for the two- or three-semester calculus course. For many students, this course provides the foundation to a career in mathematics, science, or engineering. As such, this textbook provides an important opportunity for students to learn the core concepts of calculus and understand how those concepts apply to their lives and the world around them. The text has been developed to meet the scope and sequence of most general calculus courses. At the same time, the book includes several innovative features designed to enhance student learning. A strength of Calculus Volume 1 is that instructors can customize the book, adapting it to the approach that works best in their classroom."--Preface},
	language = {en},
	publisher = {OpenStax College},
	author = {Strang, Gilbert and Herman, Edwin},
	month = mar,
	year = {2016},
	note = {Google-Books-ID: By72jwEACAAJ},
}

@misc{noauthor_newtons_2023,
	title = {Newton's method},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Newton%27s_method&oldid=1177135929},
	language = {en},
	urldate = {2023-11-03},
	journal = {Wikipedia},
	month = sep,
	year = {2023},
	note = {Page Version ID: 1177135929},
	file = {Snapshot:C\:\\Users\\peteramerkhanian\\Zotero\\storage\\BPRKSHMF\\Newton's_method.html:text/html},
}

@book{strang_calculus_2016-1,
	title = {Calculus {Volume} 3},
	isbn = {978-1-938168-07-9},
	abstract = {Calculus is designed for the typical two- or three-semester general calculus course, incorporating innovative features to enhance student learning. The book guides students through the core concepts of calculus and helps them understand how those concepts apply to their lives and the world around them. Due to the comprehensive nature of the material, we are offering the book in three volumes for flexibility and efficiency. Volume 3 covers parametric equations and polar coordinates, vectors, functions of several variables, multiple integration, and second-order differential equations.},
	language = {en},
	publisher = {OpenStax},
	author = {Strang, Gilbert and Herman, Edwin},
	month = mar,
	year = {2016},
	note = {Google-Books-ID: 7nu6DAEACAAJ},
	keywords = {Education / General, Mathematics / General},
}

@misc{noauthor_linear_2023,
	title = {Linear approximation},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Linear_approximation&oldid=1184279580#Optics},
	abstract = {In mathematics, a linear approximation is an approximation of a general function using a linear function (more precisely, an affine function). They are widely used in the method of finite differences to produce first order methods for solving or approximating solutions to equations.},
	language = {en},
	urldate = {2024-03-17},
	journal = {Wikipedia},
	month = nov,
	year = {2023},
	note = {Page Version ID: 1184279580},
	file = {Snapshot:C\:\\Users\\peteramerkhanian\\Zotero\\storage\\9Y3PIDQT\\Linear_approximation.html:text/html},
}

@book{blitzstein_introduction_2019,
	address = {Boca Raton},
	edition = {2nd edition},
	title = {Introduction to {Probability}, {Second} {Edition}},
	isbn = {978-1-138-36991-7},
	abstract = {Developed from celebrated Harvard statistics lectures, Introduction to Probability provides essential language and toolsfor understanding statistics, randomness, and uncertainty. The book explores a wide variety of applications and examples, ranging from coincidences and paradoxes to Google PageRank and Markov chain Monte Carlo (MCMC). Additional application areas explored include genetics, medicine, computer science, and information theory. The authors present the material in an accessible style and motivate concepts using real-world examples. Throughout, they use stories to uncover connections between the fundamental distributions in statistics and conditioning to reduce complicated problems to manageable pieces.The book includes many intuitive explanations, diagrams, and practice problems. Each chapter ends with a section showing how to perform relevant simulations and calculations in R, a free statistical software environment.The second edition adds many new examples, exercises, and explanations, to deepen understanding of the ideas, clarify subtle concepts, and respond to feedback from many students and readers. New supplementary online resources have been developed, including animations and interactive visualizations, and the book has been updated to dovetail with these resources. Supplementary material is available on Joseph Blitzstein’s website www. stat110.net. The supplements include:Solutions to selected exercisesAdditional practice problemsHandouts including review material and sample exams Animations and interactive visualizations created in connection with the edX online version of Stat 110.Links to lecture videos available on ITunes U and YouTube There is also a complete instructor's solutions manual available to instructors who require the book for a course.},
	language = {English},
	publisher = {Chapman and Hall/CRC},
	author = {Blitzstein, Joseph K. and Hwang, Jessica},
	month = feb,
	year = {2019},
}

@book{cohen_practical_2022,
	address = {Beijing},
	edition = {1st edition},
	title = {Practical {Linear} {Algebra} for {Data} {Science}: {From} {Core} {Concepts} to {Applications} {Using} {Python}},
	isbn = {978-1-09-812061-0},
	shorttitle = {Practical {Linear} {Algebra} for {Data} {Science}},
	abstract = {If you want to work in any computational or technical field, you need to understand linear algebra. As the study of matrices and operations acting upon them, linear algebra is the mathematical basis of nearly all algorithms and analyses implemented in computers. But the way it's presented in decades-old textbooks is much different from how professionals use linear algebra today to solve real-world modern applications.  This practical guide from Mike X Cohen teaches the core concepts of linear algebra as implemented in Python, including how they're used in data science, machine learning, deep learning, computational simulations, and biomedical data processing applications. Armed with knowledge from this book, you'll be able to understand, implement, and adapt myriad modern analysis methods and algorithms.  Ideal for practitioners and students using computer technology and algorithms, this book introduces you to: The interpretations and applications of vectors and matrices Matrix arithmetic (various multiplications and transformations) Independence, rank, and inverses Important decompositions used in applied linear algebra (including LU and QR) Eigendecomposition and singular value decomposition Applications including least-squares model fitting and principal components analysis},
	language = {English},
	publisher = {O'Reilly Media},
	author = {Cohen, Mike},
	month = oct,
	year = {2022},
}

@book{james_introduction_2013,
	address = {New York Heidelberg Dordrecht London},
	edition = {1st ed. 2013, Corr. 7th printing 2017 edition},
	title = {An {Introduction} to {Statistical} {Learning}: with {Applications} in {R}},
	isbn = {978-1-4614-7137-0},
	shorttitle = {An {Introduction} to {Statistical} {Learning}},
	abstract = {An Introduction to Statistical Learning provides an accessible overview of the field of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fields ranging from biology to finance to marketing to astrophysics in the past twenty years. This book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in science, industry, and other fields, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical software platform.Two of the authors co-wrote The Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2nd edition 2009), a popular reference book for statistics and machine learning researchers. An Introduction to Statistical Learning covers many of the same topics, but at a level accessible to a much broader audience. This book is targeted at statisticians and non-statisticians alike who wish to use cutting-edge statistical learning techniques to analyze their data. The text assumes only a previous course in linear regression and no knowledge of matrix algebra.},
	language = {English},
	publisher = {Springer},
	author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	month = jun,
	year = {2013},
}

@book{hastie_elements_2016,
	address = {New York, NY},
	edition = {2nd edition},
	title = {The {Elements} of {Statistical} {Learning}: {Data} {Mining}, {Inference}, and {Prediction}, {Second} {Edition}},
	isbn = {978-0-387-84857-0},
	shorttitle = {The {Elements} of {Statistical} {Learning}},
	abstract = {This book describes the important ideas in a variety of fields such as medicine, biology, finance, and marketing in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of colour graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book.This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression \& path algorithms for the lasso, non-negative matrix factorisation, and spectral clustering. There is also a chapter on methods for "wide'' data (p bigger than n), including multiple testing and false discovery rates.},
	language = {English},
	publisher = {Springer},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	month = jan,
	year = {2016},
}

@book{kuttler_first_2017,
	edition = {Version 2017 Revision A edition},
	title = {A {First} {Course} in {Linear} {Algebra}},
	isbn = {978-1-5428-9552-1},
	abstract = {This text, originally by K. Kuttler, has been redesigned by the Lyryx editorial team as a first course in linear algebra for science and engineering students who have an understanding of basic algebra. All major topics of linear algebra are available in detail, as well as proofs of important theorems. In addition, connections to topics covered in advanced courses are introduced. The text is designed in a modular fashion to maximize flexibility and facilitate adaptation to a given course outline and student profile. Each chapter begins with a list of student learning outcomes, and examples and diagrams are given throughout the text to reinforce ideas and provide guidance on how to approach various problems. Suggested exercises are included at the end of each section, with selected answers at the end of the text. Lyryx develops and supports open texts, with editorial services to adapt the text for each particular course. In addition, Lyryx provides content-specific formative online assessment, a wide variety of supplements, and in-house support available 7 days/week for both students and instructors.},
	language = {English},
	publisher = {CreateSpace Independent Publishing Platform},
	author = {Kuttler, Ken},
	editor = {Learning, Lyryx},
	month = feb,
	year = {2017},
}

@misc{noauthor_cumulative_2024,
	title = {Cumulative distribution function},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Cumulative_distribution_function&oldid=1215839028},
	abstract = {In probability theory and statistics, the cumulative distribution function (CDF) of a real-valued random variable 
  
    
      
        X
      
    
    \{{\textbackslash}displaystyle X\}
  
, or just distribution function of 
  
    
      
        X
      
    
    \{{\textbackslash}displaystyle X\}
  
, evaluated at 
  
    
      
        x
      
    
    \{{\textbackslash}displaystyle x\}
  
, is the probability that 
  
    
      
        X
      
    
    \{{\textbackslash}displaystyle X\}
  
 will take a value less than or equal to 
  
    
      
        x
      
    
    \{{\textbackslash}displaystyle x\}
  
.
Every probability distribution supported on the real numbers, discrete or "mixed" as well as continuous, is uniquely identified by a right-continuous monotone increasing function (a càdlàg function) 
  
    
      
        F
        :
        
          R
        
        →
        [
        0
        ,
        1
        ]
      
    
    \{{\textbackslash}displaystyle F{\textbackslash}colon {\textbackslash}mathbb \{R\} {\textbackslash}rightarrow [0,1]\}
  
 satisfying 
  
    
      
        
          lim
          
            x
            →
            −
            ∞
          
        
        F
        (
        x
        )
        =
        0
      
    
    \{{\textbackslash}displaystyle {\textbackslash}lim \_\{x{\textbackslash}rightarrow -{\textbackslash}infty \}F(x)=0\}
  
 and 
  
    
      
        
          lim
          
            x
            →
            ∞
          
        
        F
        (
        x
        )
        =
        1
      
    
    \{{\textbackslash}displaystyle {\textbackslash}lim \_\{x{\textbackslash}rightarrow {\textbackslash}infty \}F(x)=1\}
  
.
In the case of a scalar continuous distribution, it gives the area under the probability density function from negative infinity to 
  
    
      
        x
      
    
    \{{\textbackslash}displaystyle x\}
  
. Cumulative distribution functions are also used to specify the distribution of multivariate random variables.},
	language = {en},
	urldate = {2024-04-27},
	journal = {Wikipedia},
	month = mar,
	year = {2024},
	note = {Page Version ID: 1215839028},
	file = {Snapshot:C\:\\Users\\peteramerkhanian\\Zotero\\storage\\5XGAYN28\\Cumulative_distribution_function.html:text/html},
}
