---
title: "Getting census data with the ipums API"
bibliography: "../../blog.bib"
author: "Peter Amerkhanian"
date: "2024-9-1"
draft: false
image: thumbnail.png
engine: knitr
categories: ['R', 'Data Management']
format:
  html:
    df-print: kable
    toc: true
    toc-depth: 3
    code-fold: false
    code-tools: true
editor: 
  markdown: 
    wrap: 72
---

```{r}
#| output: false
library(ipumsr)
library(dplyr)
# Note -- I store my ipums api key in the .Renviron file
set_ipums_api_key(Sys.getenv("api_key"))
```

## What is IPUMS?

> IPUMS provides census and survey data from around the world integrated
> across time and space. IPUMS integration and documentation makes it
> easy to study change, conduct comparative research, merge information
> across data types, and analyze individuals within family and community
> contexts. Data and services available free of charge. -- \[\@\]

## How is IPUMS Different than census.gov

[@walker_analyzing_2023, chapter 9]

```{r}
ipums_data_collections() %>%
  filter(collection_type == "microdata",
         api_support == TRUE)
```

For analysis of populations in the U.S., the IPUMS USA and IPUMS CPS collections are of particular interest:

IPUMS USA: IPUMS USA collects, preserves and harmonizes U.S. census
microdata and provides easy access to this data with enhanced
documentation. Data includes decennial censuses from 1790 to 2010 and American Community Surveys (ACS) from 2000 to the present [@ruggles_ipums_2024].

We can check out the newest products they have in the USA collection as
follows:

```{r}
get_sample_info(collection="usa") %>% arrange(desc(name)) %>% head(10)
```

IPUMS CPS: Current Population Survey microdata including basic monthly
surveys and supplements from 1962 to the present. \[\@\]

```{r}
get_sample_info(collection="cps") %>% arrange(desc(name)) %>% head(10)
```

```{r}
get_sample_info(collection="usa") %>%
  filter(stringr::str_detect(description, "ACS")) %>%
  arrange(desc(name)) %>%
  head(10)
```


```{r}
#| eval: false
# Define an extract
extract <- define_extract_usa(
    description = "Alameda County Incomes by PUMA",
    samples = c("us2022a"),
    variables = list(
      "COUNTYFIP",
      "PUMA",
      "FAMUNIT",
      "RELATE",
      "AGE",
      "SEX",
      "POVERTY",
      "FTOTINC",
      var_spec("STATEFIP", case_selections = "06")
    )
  )
# Submit the extract request to ipums and download locally
data_path <- extract %>%
    submit_extract() %>%
    wait_for_extract() %>%
    download_extract(download_dir = here::here("data"),
                     overwrite = TRUE)
  data <- read_ipums_micro(data_path)
```
I'll define a function, `retrieve_alameda_sample()` that accomplishes that and only run the IPUMS request if I don't already have the data locally.

```{r}
#| code-fold: true
retrieve_alameda_sample <- function(){
  extract <- define_extract_usa(
    description = "Alameda County Incomes by PUMA",
    samples = c("us2022a"),
    variables = list(
      "COUNTYFIP",
      "PUMA",
      "FAMUNIT",
      "RELATE",
      "AGE",
      "SEX",
      "POVERTY",
      "FTOTINC",
      var_spec("STATEFIP", case_selections = "06")
    )
  )
  data_path <- extract %>%
    submit_extract() %>%
    wait_for_extract() %>%
    download_extract(download_dir = here::here("data"),
                     overwrite = TRUE)
  data <- read_ipums_micro(data_path)
  return(data)
  }
```

```{r}
local_ipums_extracts <- list.files(
  path <- here::here('data'),
  pattern = "\\.xml$",
  full.names = TRUE)
```

```{r}
if (length(local_ipums_extracts) > 0) {
  existing_path <- local_ipums_extracts[1]
  data <- read_ipums_micro(existing_path)
} else {
  data <- retrieve_alameda_sample()
}
```

Use the [2020 PUMA
Names](https://www2.census.gov/geo/pdfs/reference/puma2020/2020_PUMA_Names.pdf).

```{r}
library(dplyr)
oak <- data %>%
  filter(PUMA %in% c(111, 112, 113, 123))

families <- oak %>%
  filter(FTOTINC != 9999999) %>%
  distinct(SERIAL, FAMUNIT, .keep_all = TRUE) %>%
  mutate(in_pov = (POVERTY <= 100) %>% as.factor)
```




```{r}
# library(reticulate)
# use_condaenv("base")
```

```{python}
# import pandas as pd
# import matplotlib.pyplot as plt
# import numpy as np
# import skl
# 
# x = np.random.normal(0, 1, 10)
# y = np.random.normal(0, 1, 10)

```

```{python}
# fig, ax = plt.subplots()
# df['FTOTINC'].hist(ax=ax)
# plt.show()
```

```{r}
# plot(mtcars$mpg * 3)
```


```{r}
# <!-- null_dist <- families %>% -->
# <!--   specify(response = in_pov, success = "TRUE") %>%  -->
# <!--   hypothesize(null = "point", p=0) %>%  -->
# <!--   generate(reps = 1000, type = "bootstrap") %>%  -->
# <!--   calculate(stat = "prop") -->
# 
# <!-- point_estimate <- families %>% -->
# <!--   specify(response =in_pov, success = "TRUE") %>%  -->
# <!--   calculate(stat = "prop") -->
# 
# <!-- null_dist %>% visualise() -->
```

```{r}
# null_dist %>%
#   # calculate the confidence interval around the point estimate
#   get_confidence_interval(point_estimate = point_estimate,
#                           # at the 95% confidence level
#                           level = .95,
#                           # using the standard error
#                           type = "se")
```


