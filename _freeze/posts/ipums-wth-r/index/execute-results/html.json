{
  "hash": "539eee801cf4a83fbe73c55491d0dcb7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Working with ACS microdata in R\"\nbibliography: \"../../blog.bib\"\nauthor: \"Peter Amerkhanian\"\ndate: \"2025-02-20\"\ndescription: \"Using `ipumsr`, geocorr, and `survey` to analyze a small-geography in IPUMS data.\"\ndraft: false\nimage: thumbnail.png\nengine: knitr\nexecute: \n  cache: false\n  freeze: auto\ncategories: ['R', 'Data Management']\nformat:\n  html:\n    df-print: kable\n    toc: true\n    toc-depth: 3\n    code-fold: false\n    code-tools: true\neditor: \n  markdown: \n    wrap: 72\n---\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\npacman::p_load(dplyr,\n               ggplot2,\n               # Statistics\n               modelsummary,\n               srvyr,\n               survey,\n               # Webscraping\n               httr,\n               rvest,\n               readr,\n               glue,\n               # Census\n               tidycensus,\n               ipumsr,\n               # XML Parsing\n               xml2,\n               purrr,\n               stringr)\n```\n:::\n\n\n\nThe following are my notes on how to use American Community Survey (ACS)\nmicrodata, leveraging the University of Minnesota's Integrated Public\nUse Microdata Series (IPUMS).[^1] I cover:\n\n[^1]: I want to note that several of the points I cover here are things\n    I learned from some coworkers-- [Bert\n    Wilden](https://www.bwilden.com/) and Stephanie Peng.\n\n1.  **Retrieval**: how to choose which ACS product is relevant, how to\n    submit a request to IPUMS, and how to filter down to relevant levels\n    of geography/granularity.\n2.  **Analysis**: how to properly weight ACS data using sample and\n    replication weights for accurate analysis and uncertainty\n    estimation.\n\nI'll start with a motivating question:  \n\n::: {.callout-tip}\n## Motivating Question\n**What was the median household\nincome in Oakland, California in 2022?**  \n:::\n\n## Aggregate data with `tidycensus`\n\nAnswering that question is straightforward using the Census' *aggregate*\ndata -- pre-calculated descriptive statistics for aggregate geographies.\nI typically retrieve aggregate data via the [Census\nAPI](https://www.census.gov/programs-surveys/acs/data/data-via-api.html).[^2]\nIn R, the `tidycensus` package provides an easy-to-use wrapper for that\nAPI. Note that I set up an API key for the U.S. Census and I'm storing\nit in my `.Renviron` file as `census_api_key=\"yourkeyhere\"`.\n\n[^2]: For an informal analysis, I might just use a web-based tool like\n    [Census Reporter](https://censusreporter.org/) to quickly look\n    something up.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncensus_api_key(Sys.getenv(\"census_api_key\"))\n```\n:::\n\n\n\nI query `B19013_001`, the median household income variable, using the\n2022 1-year American Community Survey sample and I filter down to\nOakland's GEOID, `0653000`, which is a combination of the state code for\nCalifornia, `06`, and the place code for Oakland, `53000`.[^3] I'll\nthrow in the total population variable for good measure:\n\n[^3]: See the [full variable list for the 2022 1-year\n    ACS](https://api.census.gov/data/2022/acs/acs1/variables.html) for\n    the available variables, and see the [Census Place\n    table](https://www.census.gov/library/reference/code-lists/ansi.html#place)\n    for looking up GEOIDs\n\n\n\n::: {#tbl-agg-data .cell tbl-cap='Total Population and Median Household Income in Oakand, CA via Aggregate Data'}\n\n```{.r .cell-code}\noakland_stats <- get_acs(\n  geography = \"place\",\n  variables = c(\n    median_hh_income = \"B19013_001\",\n    total_pop = \"B17001_001\"\n  ),\n  state = \"CA\",\n  year = 2022,\n  survey = \"acs1\"\n)\noakland_stats <- oakland_stats %>% filter(GEOID == '0653000')\noakland_stats %>% select(c(variable, estimate, moe))\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|variable         | estimate|  moe|\n|:----------------|--------:|----:|\n|total_pop        |   426323|  811|\n|median_hh_income |    93146| 6232|\n\n</div>\n:::\n:::\n\n\n\nDone! We now know the population and median household income for Oakland\nin 2022, along with a margin of error. See [@walker_analyzing_2023] for\na comprehensive treatment of working with aggregate census data.\n\n## Microdata with IPUMS/`ipumsr`\n\nWhat if I wanted to have access to the underlying data used to calculate\nthe median? Maybe I want to try a different standard error specification\nfor that statistic, or calculate other statistics, like the average\nhousehold income. These tasks would all entail accessing census\n*microdata* -- household and/or individual level census data.\n\nOne of the most popular sources for downloading census microdata is the\nUniversity of Minnesota's Integrated Public Use Microdata Series\n(IPUMS). The IPUMS team provides a centralized API for downloading\ncensus microdata, comprehensive documentation for working with the data,\nand harmonized variables across time [@walker_analyzing_2023, chapter\n9].\n\nThe easiest way to access IPUMS data in R is with the\n[`ipumsr`](https://tech.popdata.org/ipumsr/) package, which the IPUMS\nteam maintains and which allows users to submit API requests to IPUMS\ndirectly from R [@greg_freedman_ellis_ipumsr_2024] . To get set up, I\n[registered for an IPUMS API\nkey](https://developer.ipums.org/docs/v2/get-started/), stored the key\nin my `.Renviron` file, and will configure the key in `ipumsr` as\nfollows:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset_ipums_api_key(Sys.getenv(\"ipums_api_key\"))\n```\n:::\n\n\n\nThe `ipumsr` website [provides\ndetails](https://tech.popdata.org/ipumsr/articles/ipums.html#obtaining-data-via-the-ipums-api)\non what survey products the project currently supports, as does the\n`ipums_data_collections()` function:\n\n\n\n::: {#tbl-ipums-available .cell tbl-cap='IPUMS API Collections'}\n\n```{.r .cell-code}\nipums_data_collections() %>%\n  filter(api_support == TRUE) %>% \n  arrange(desc(collection_type))\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|collection_name     |collection_type |code_for_api |api_support |\n|:-------------------|:---------------|:------------|:-----------|\n|IPUMS USA           |microdata       |usa          |TRUE        |\n|IPUMS CPS           |microdata       |cps          |TRUE        |\n|IPUMS International |microdata       |ipumsi       |TRUE        |\n|IPUMS ATUS          |microdata       |atus         |TRUE        |\n|IPUMS AHTUS         |microdata       |ahtus        |TRUE        |\n|IPUMS MTUS          |microdata       |mtus         |TRUE        |\n|IPUMS NHIS          |microdata       |nhis         |TRUE        |\n|IPUMS MEPS          |microdata       |meps         |TRUE        |\n|IPUMS NHGIS         |aggregate data  |nhgis        |TRUE        |\n\n</div>\n:::\n:::\n\n\n\nI'll look at IPUMS USA since my motivating question involves median\nhousehold income for a year (2022), and IPUMS USA offers annual data\nfrom decennial censuses 1790-2010 and American Community Surveys (ACS)\n2000-present [@ruggles_ipums_2024]. We can check out the newest products\nthey have in the USA collection as follows:\n\n\n\n::: {#tbl-acs-datasets .cell tbl-cap='IPUMS USA Products'}\n\n```{.r .cell-code}\nget_sample_info(collection=\"usa\") %>%\n  arrange(desc(name)) %>%\n  head(5)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|name    |description            |\n|:-------|:----------------------|\n|us2023d |2019-2023, PRCS 5-year |\n|us2023c |2019-2023, ACS 5-year  |\n|us2023b |2023 PRCS              |\n|us2023a |2023 ACS               |\n|us2022d |2018-2022, PRCS 5-year |\n\n</div>\n:::\n:::\n\n\n\nNote that \"PRCS\" refers to the Puerto Rico Community Survey (an ACS\nequivalent specifically tailored to Puerto Rico). We are principally\ninterested in the ACS, which comes in either one-year (e.g. 2023 ACS) or\nfive-year (e.g. 2018-2022, ACS 5-year) estimates. The differences\nbetween these two estimates are described in detail in [Census Data: An\nOverview](https://walker-data.com/census-r/the-united-states-census-and-the-r-programming-language.html#census-data-an-overview)\nin [@walker_analyzing_2023]. One differentiating point is that one-year\nestimates come from a smaller, but more contemporary sample. In our case\nwe'll use the one-year to get the best sense of the 2022 income\ndynamics.\n\nLet's return to the motivating question for this post:  \n\n::: {.callout-tip}\n## Motivating Question\n**What was the median household\nincome in Oakland, California in 2022?**  \n:::\n\nTo answer that we will:\n\n1.  Get income data from the 2022 1-year ACS (@sec-step-1).\n2.  Filter our data down to households in the city of Oakland\n    (@sec-step-2 and @sec-step-3).\n3.  Calculate the median (@sec-step-4 and @sec-step-5).\n\n## Step 1: Retrieving data {#sec-step-1}\n\nFor the first task, I'll define several helper functions:\n\n-   `retrieve_sample()` retrieves a list of variables from a given ACS\n    sample from the IPUMS API. This is the only truly necessary code.\n-   `check_description()` and `ipums_data()` are functions for checking\n    if the target extract already exists locally. Retrieving an extract\n    can take some time, and I'd like to avoid doing it repeatedly when\n    I've already downloaded the extract in the past.\n\nEach of these are defined in the following folded code block.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nretrieve_sample <- function(sample, variables, description) {\n  extract <- define_extract_micro(\n    description = description,\n    collection = \"usa\",\n    samples = c(sample),\n    variables = variables\n  )\n  data_path <- extract %>%\n    submit_extract() %>%\n    wait_for_extract() %>%\n    download_extract(download_dir = here::here(\"data\"),\n                     overwrite = TRUE)\n  data <- read_ipums_micro(data_path)\n  return(data)\n}\n\ncheck_description <- function(file, description) {\n  xml_text <- readLines(file, warn = FALSE) %>% paste(collapse = \"\\n\")\n  user_description <- str_match(xml_text, \"User-provided description:\\\\s*(.*)]]\")[, 2]\n  # Check if it matches the target\n  if (str_detect(user_description, fixed(description, ignore_case = TRUE))) {\n    return(file)\n  } else {\n    return(NULL)\n  }\n}\n\nipums_data <- function(sample, variables, description) {\n  local_ipums_extracts <- list.files(\n    path = here::here('data'),\n    pattern = \"\\\\.xml$\",\n    full.names = TRUE\n  )\n  matching_files <- compact(map(local_ipums_extracts, \\(x) check_description(x, description)))\n  # If there is a match, open the first matching file\n  if (length(matching_files) > 0) {\n    matched_file <- matching_files[[1]]\n    message(\"Opening: \", matched_file)\n    data <- read_ipums_micro(matched_file)\n  } else {\n    message(\"No matching dataset found.\")\n    data <- retrieve_sample(sample, variables, description)\n  }\n}\n```\n:::\n\n\n\nI'll proceed to define a list of variables that I want, including\n`HHINCOME` (household income) and `INCTOT` (individual income). Some of\nthese variables refer to census-specific language â€“ i.e. `PUMA` ,\n`REPWT` , `REPWTP`. I'll cover exactly what each of these represent\nlater in the post, but in the meantime I'll also note that the IPUMS\nwebsite has [full reference\nmaterials](https://usa.ipums.org/usa-action/variables/group) for all\navailable variables.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvariables <- list(\n  \"PUMA\",\n  \"AGE\",\n  \"SEX\",\n  \"EDUC\",\n  \"HHINCOME\",\n  \"INCTOT\",\n  \"REPWT\",\n  \"REPWTP\",\n  var_spec(\"STATEFIP\",\n           case_selections = \"06\")\n)\n```\n:::\n\n\n\nNote the line, `var_spec(\"STATEFIP\", case_selections = \"06\")`. This\nselects the variable `STATEFIP`, while also specifying that we want to\nrestrict our request to data where `STATEFIP=='06'` (California). Using\n[`var_spec()`](https://tech.popdata.org/ipumsr/reference/var_spec.html)\nis important, as accidentally/unnecessarily downloading an unfiltered,\nfull sample of the entire U.S. is time-consuming.\n\nAnyways, I can request these variables from the 2022 1-year ACS via\nmy`ipums_data()` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- ipums_data(\"us2022a\", variables, \"Incomes by PUMA\")\n```\n:::\n\n\n\nHere are the resulting data, the 2022 1-year ACS for California.\n\n\n\n::: {#tbl-raw .cell tbl-cap='Unfiltered 2022 1-year ACS data for California'}\n\n```{.r .cell-code}\ndata %>% head()\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| YEAR| SAMPLE| SERIAL|    CBSERIAL| HHWT| REPWT|      CLUSTER| STATEFIP| PUMA| STRATA| GQ| HHINCOME| REPWT1| REPWT2| REPWT3| REPWT4| REPWT5| REPWT6| REPWT7| REPWT8| REPWT9| REPWT10| REPWT11| REPWT12| REPWT13| REPWT14| REPWT15| REPWT16| REPWT17| REPWT18| REPWT19| REPWT20| REPWT21| REPWT22| REPWT23| REPWT24| REPWT25| REPWT26| REPWT27| REPWT28| REPWT29| REPWT30| REPWT31| REPWT32| REPWT33| REPWT34| REPWT35| REPWT36| REPWT37| REPWT38| REPWT39| REPWT40| REPWT41| REPWT42| REPWT43| REPWT44| REPWT45| REPWT46| REPWT47| REPWT48| REPWT49| REPWT50| REPWT51| REPWT52| REPWT53| REPWT54| REPWT55| REPWT56| REPWT57| REPWT58| REPWT59| REPWT60| REPWT61| REPWT62| REPWT63| REPWT64| REPWT65| REPWT66| REPWT67| REPWT68| REPWT69| REPWT70| REPWT71| REPWT72| REPWT73| REPWT74| REPWT75| REPWT76| REPWT77| REPWT78| REPWT79| REPWT80| PERNUM| PERWT| REPWTP| FAMUNIT| RELATE| RELATED| SEX| AGE| EDUC| EDUCD| INCTOT| REPWTP1| REPWTP2| REPWTP3| REPWTP4| REPWTP5| REPWTP6| REPWTP7| REPWTP8| REPWTP9| REPWTP10| REPWTP11| REPWTP12| REPWTP13| REPWTP14| REPWTP15| REPWTP16| REPWTP17| REPWTP18| REPWTP19| REPWTP20| REPWTP21| REPWTP22| REPWTP23| REPWTP24| REPWTP25| REPWTP26| REPWTP27| REPWTP28| REPWTP29| REPWTP30| REPWTP31| REPWTP32| REPWTP33| REPWTP34| REPWTP35| REPWTP36| REPWTP37| REPWTP38| REPWTP39| REPWTP40| REPWTP41| REPWTP42| REPWTP43| REPWTP44| REPWTP45| REPWTP46| REPWTP47| REPWTP48| REPWTP49| REPWTP50| REPWTP51| REPWTP52| REPWTP53| REPWTP54| REPWTP55| REPWTP56| REPWTP57| REPWTP58| REPWTP59| REPWTP60| REPWTP61| REPWTP62| REPWTP63| REPWTP64| REPWTP65| REPWTP66| REPWTP67| REPWTP68| REPWTP69| REPWTP70| REPWTP71| REPWTP72| REPWTP73| REPWTP74| REPWTP75| REPWTP76| REPWTP77| REPWTP78| REPWTP79| REPWTP80|\n|----:|------:|------:|-----------:|----:|-----:|------------:|--------:|----:|------:|--:|--------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|------:|-----:|------:|-------:|------:|-------:|---:|---:|----:|-----:|------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|\n| 2022| 202201|  74692| 2.02201e+12|   14|     1| 2.022001e+12|        6| 6509| 650906|  4|  9999999|     12|     14|     14|     12|     14|     14|     12|     12|     11|      12|      13|      13|      11|      13|      13|      11|      13|      12|      12|      14|      13|      13|      13|      13|      11|      11|      13|      12|      14|      13|      10|      12|      13|      10|      13|      13|      15|      12|      14|      14|      12|      11|      12|      12|      10|      14|      13|      13|      12|      12|      11|      13|      12|      11|      12|      14|      13|      12|      14|      12|      13|      13|      12|      12|      12|      12|      12|      12|      12|      11|      12|      13|      13|      11|      14|      12|      11|      11|      13|      10|      1|    14|      1|       1|     12|    1270|   2|  56|    6|    64|  14500|      12|      14|      14|      12|      14|      14|      12|      12|      11|       12|       13|       13|       11|       13|       13|       11|       13|       12|       12|       14|       13|       13|       13|       13|       11|       11|       13|       12|       14|       13|       10|       12|       13|       10|       13|       13|       15|       12|       14|       14|       12|       11|       12|       12|       10|       14|       13|       13|       12|       12|       11|       13|       12|       11|       12|       14|       13|       12|       14|       12|       13|       13|       12|       12|       12|       12|       12|       12|       12|       11|       12|       13|       13|       11|       14|       12|       11|       11|       13|       10|\n| 2022| 202201|  74693| 2.02201e+12|   27|     1| 2.022001e+12|        6| 6501| 650106|  3|  9999999|     27|     28|     48|     49|      5|     47|     26|     26|      6|       7|       6|      28|      26|      48|       6|      28|      47|      27|      26|      44|      50|      50|      25|      28|      27|      27|       6|      47|      25|      27|      29|       6|      49|      27|      27|       6|      29|       6|       6|      26|      28|      28|       6|       6|      51|       6|      26|      27|      48|      46|      47|      28|      27|       6|      51|      27|       6|      26|      29|       4|       6|       6|      28|      27|      29|      27|      48|       5|      27|      29|      27|      46|       6|      25|      27|      48|      27|      43|      47|      25|      1|    27|      1|       1|     13|    1301|   1|  52|    0|     2|      0|      27|      28|      48|      49|       5|      47|      26|      26|       6|        7|        6|       28|       26|       48|        6|       28|       47|       27|       26|       44|       50|       50|       25|       28|       27|       27|        6|       47|       25|       27|       29|        6|       49|       27|       27|        6|       29|        6|        6|       26|       28|       28|        6|        6|       51|        6|       26|       27|       48|       46|       47|       28|       27|        6|       51|       27|        6|       26|       29|        4|        6|        6|       28|       27|       29|       27|       48|        5|       27|       29|       27|       46|        6|       25|       27|       48|       27|       43|       47|       25|\n| 2022| 202201|  74694| 2.02201e+12|   70|     1| 2.022001e+12|        6| 8101| 810106|  3|  9999999|     19|     11|     11|     59|     71|     71|     71|     71|     78|      91|      89|      91|      19|      57|      80|      89|      70|      78|      90|      11|      20|      12|      11|      58|      70|      71|      71|      71|      80|      91|      90|      91|      18|      60|      79|      89|      70|      79|      89|      92|      80|      92|      90|      90|      71|      72|      69|      71|      20|      11|      11|      59|      80|      92|      19|      58|      71|      18|      58|      90|      79|      90|      90|      90|      70|      69|      70|      71|      20|      12|      10|      57|      78|      91|      20|      58|      71|      20|      56|      12|      1|    70|      1|       1|     13|    1301|   1|  61|    7|    71|     80|      19|      11|      11|      59|      71|      71|      71|      71|      78|       91|       89|       91|       19|       57|       80|       89|       70|       78|       90|       11|       20|       12|       11|       58|       70|       71|       71|       71|       80|       91|       90|       91|       18|       60|       79|       89|       70|       79|       89|       92|       80|       92|       90|       90|       71|       72|       69|       71|       20|       11|       11|       59|       80|       92|       19|       58|       71|       18|       58|       90|       79|       90|       90|       90|       70|       69|       70|       71|       20|       12|       10|       57|       78|       91|       20|       58|       71|       20|       56|       12|\n| 2022| 202201|  74695| 2.02201e+12|   22|     1| 2.022001e+12|        6| 8303| 830306|  4|  9999999|     22|     26|     29|     36|      2|     32|     36|     27|      3|       2|       4|      22|      18|      55|       3|      22|      43|      18|      27|       4|       2|       3|      19|      26|      20|      20|      46|       5|      21|      23|      20|      38|       2|      27|      23|      45|      20|      31|      53|      20|      20|      20|      54|      42|       6|      49|      16|      19|       3|       3|       4|      21|      27|      33|       4|      25|      37|      29|      18|       2|       4|       3|      29|      18|      27|      28|      34|       2|      26|      23|      22|      38|       5|      19|      21|      35|      25|      56|      30|      24|      1|    22|      1|       1|     12|    1270|   2|  26|    7|    71|   9000|      22|      26|      29|      36|       2|      32|      36|      27|       3|        2|        4|       22|       18|       55|        3|       22|       43|       18|       27|        4|        2|        3|       19|       26|       20|       20|       46|        5|       21|       23|       20|       38|        2|       27|       23|       45|       20|       31|       53|       20|       20|       20|       54|       42|        6|       49|       16|       19|        3|        3|        4|       21|       27|       33|        4|       25|       37|       29|       18|        2|        4|        3|       29|       18|       27|       28|       34|        2|       26|       23|       22|       38|        5|       19|       21|       35|       25|       56|       30|       24|\n| 2022| 202201|  74696| 2.02201e+12|    8|     1| 2.022001e+12|        6| 6712| 671206|  3|  9999999|      9|     15|     10|      8|      8|      2|      2|      8|      2|      15|       1|      16|       8|      14|      15|      10|       8|       7|       9|      10|      14|       9|       3|      16|      16|       9|      10|       1|       7|       8|       8|       8|       3|       9|      10|      16|      15|       1|       1|       8|      16|       9|       2|      15|      15|       9|       8|       1|       8|       8|       8|       8|       1|       9|       8|      15|      16|       3|       2|       2|       8|      15|       8|       8|       8|       1|       1|       9|       2|      16|       3|      15|       8|      14|      17|       9|       8|       8|       8|       3|      1|     8|      1|       1|     13|    1301|   2|  38|    6|    63|  48000|       9|      15|      10|       8|       8|       2|       2|       8|       2|       15|        1|       16|        8|       14|       15|       10|        8|        7|        9|       10|       14|        9|        3|       16|       16|        9|       10|        1|        7|        8|        8|        8|        3|        9|       10|       16|       15|        1|        1|        8|       16|        9|        2|       15|       15|        9|        8|        1|        8|        8|        8|        8|        1|        9|        8|       15|       16|        3|        2|        2|        8|       15|        8|        8|        8|        1|        1|        9|        2|       16|        3|       15|        8|       14|       17|        9|        8|        8|        8|        3|\n| 2022| 202201|  74697| 2.02201e+12|   49|     1| 2.022001e+12|        6| 7301| 730106|  4|  9999999|     52|     51|     47|      5|     79|     45|     45|      4|     49|       5|       3|     103|      91|      53|      49|      49|      46|       5|     102|      88|      47|      48|      55|       4|      98|      49|      47|       5|      44|       5|       3|      93|     104|      48|      54|      46|      53|       4|     103|       3|      46|      50|      51|      93|       4|      48|      50|      77|      49|      93|     108|       5|       4|      46|      47|      51|      57|      89|       4|       4|      54|      42|      56|     101|       4|      45|      55|      94|      48|      96|      99|       5|       4|      42|      49|      47|      50|      75|       4|      90|      1|    49|      1|       1|     12|    1270|   1|  23|    6|    63|  24000|      52|      51|      47|       5|      79|      45|      45|       4|      49|        5|        3|      103|       91|       53|       49|       49|       46|        5|      102|       88|       47|       48|       55|        4|       98|       49|       47|        5|       44|        5|        3|       93|      104|       48|       54|       46|       53|        4|      103|        3|       46|       50|       51|       93|        4|       48|       50|       77|       49|       93|      108|        5|        4|       46|       47|       51|       57|       89|        4|        4|       54|       42|       56|      101|        4|       45|       55|       94|       48|       96|       99|        5|        4|       42|       49|       47|       50|       75|        4|       90|\n\n</div>\n:::\n:::\n\n\n\n## Step 2: Using Geocorr to identify small geographies {#sec-step-2}\n\nWe now have microdata for all of California, but we need to filter down\nto just Oakland. Unfortunately, this isn't as simple as just running\n`filter(CITY == 'Oakland')` -- ACS microdata does not include a field\nfor explicitly identifying cities (note that a city is typically\nreferred to as a \"place\" in census data).\n\nThe smallest geographic area explicitly identified in the microdata is\nsomething called a public use microdata area (PUMA) [@pastoor_how_2024].\nPUMAS are unique geographies that always aggregate to the state-level\n(e.g. California can be constructed with a collection of PUMAs), but\nonly sometimes aggregate to other small geographic areas, such as city,\nmetro area, and county [See [Census\nHierarchies](https://walker-data.com/census-r/the-united-states-census-and-the-r-programming-language.html#census-hierarchies)\nin @walker_analyzing_2023, chapter 1].\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tigris)\nlibrary(sf)\nlibrary(cowplot)\noptions(tigris_use_cache = TRUE)\n\nbay_area_counties <- c(\n  \"Alameda\", \"Contra Costa\", \"Marin\", \"Napa\",\n  \"San Francisco\", \"San Mateo\", \"Santa Clara\", \"Solano\", \"Sonoma\"\n)\n\nbay_area <- counties(state = \"CA\", class = \"sf\", year=2022) %>%\n  filter(NAME %in% bay_area_counties) %>%\n  st_union()  # Merge counties into a single boundary\n\n# Get all California Places & PUMAs\nplaces <- places(state = \"CA\", class = \"sf\", year=2022)\npumas <- pumas(state = \"CA\", class = \"sf\",  year=2022)\n# Filter only those in the Bay Area\nplaces_bay <- places[st_intersects(places, bay_area, sparse = FALSE), ]\npumas_bay <- pumas[st_intersects(pumas, bay_area, sparse = FALSE), ]\n\npumas_bbox <- st_bbox(bay_area)\n\ncrop_theme <- theme(\n    panel.spacing = unit(0, \"lines\"),      \n    plot.margin = margin(0, 0, 0, 0, \"cm\")\n)\n\nmap_places <- ggplot() +\n  geom_sf(data = places_bay, fill = \"lightblue\", color = \"darkgrey\", size = 0.1) +\n  geom_sf(data = bay_area, fill = NA, color = \"red\", size = 4) +  \n  coord_sf(ylim = c(pumas_bbox[\"ymin\"], pumas_bbox[\"ymax\"]), xlim = c(pumas_bbox[\"xmin\"], pumas_bbox[\"xmax\"]), expand=FALSE) +  # Apply PUMAs y-limits\n  crop_theme +\n  theme_minimal() +\n  ggtitle(\"Places\")\n\nmap_pumas <- ggplot() +\n  geom_sf(data = pumas_bay, fill = \"lightblue\", color = \"darkgrey\", size = 0.1) +\n  geom_sf(data = bay_area, fill = NA, color = \"red\", size = 4) +  \n  coord_sf(ylim = c(pumas_bbox[\"ymin\"], pumas_bbox[\"ymax\"]), xlim = c(pumas_bbox[\"xmin\"], pumas_bbox[\"xmax\"]), expand=FALSE) +  # Ensure y-axis is the same\n  crop_theme +\n  theme_minimal() +\n  ggtitle(\"PUMAs\")\n\n\nplot_grid(map_places, map_pumas, nrow = 1, align = \"h\", axis=\"t\", vjust = 0)\n```\n\n::: {.cell-output-display}\n![Places and PUMAS in the Bay Area (region border in red)](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\nTo find out if a city corresponds to a collection of PUMAs and which\nPUMAs those are, we'll use\n[Geocorr](https://mcdc.missouri.edu/applications/geocorr2022.html)\n(geographic correspondence engine), an application that generates\ncorrelation lists showing relationships between two or more geographic\ncoverages in the United States [@mihalik_missouri_2022]. Geocorr is a\nsponsored program of the Missouri State library and published by the\nUniversity of Missouri Center for Health Policy.[^4]\n\n[^4]: I'll to note that the combination of IPUMS and Geocorr is a\n    fantastic public good, and it's extremely generous of the public\n    Universities of Minnesota and Missouri to publish these.\n\n![Geocorr 2022: Geographic Correspondence\nEngine](geocorr.png){width=\"80%\"}\n\nTo use Geocorr, I'll define a function, `geocorr_2022()` that queries\nGeocorr 2022 and retrieves a .csv file establishing the relationships\nbetween two sets of geographies within a given state. See the following\nfolded code chunk for the specifics:\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ngeocorr_2022 <- function(state, geo_1, geo_2, weight_var) {\n  base_url <- \"https://mcdc.missouri.edu\"\n  params <- glue(\n    \"cgi-bin/broker?_PROGRAM=apps.geocorr2022.sas&\",\n    \"_SERVICE=MCDC_long&_debug=0&\",\n    \"state={state}&g1_={geo_1}&g2_={geo_2}&wtvar={weight_var}&\",\n    \"nozerob=1&fileout=1&filefmt=csv&lstfmt=txt&title=&\",\n    \"counties=&metros=&places=&oropt=&latitude=&longitude=&\",\n    \"distance=&kiloms=0&locname=\")\n  initial_url <- params %>% url_absolute(base = base_url)\n  initial_response <- GET(initial_url)\n  html_content <- content(initial_response, as = \"text\")\n  parsed_html <- read_html(html_content)\n  # Extract the one link\n  csv_url <- parsed_html %>%\n    html_node(\"a\") %>%\n    html_attr(\"href\") %>%\n    stringr::str_trim() %>%\n    url_absolute(base = base_url)\n  csv_data <- read_csv(csv_url)\n  return(csv_data)\n}\n```\n:::\n\n\n\nWe'll use that function to establish the relationships between\nCalifornia's 2022 PUMAs and its \"places,\" using individual population as\nmeasured in the 2020 Decenial Census to weight the relationships.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncsv_data <- geocorr_2022(\"Ca06\", \"puma22\", \"place\", \"pop20\")\n```\n:::\n\n\n\nWith that, we can see which PUMAs correspond to the City of Oakland.\n\n\n\n::: {#tbl-correspondence .cell tbl-cap='PUMA to Place correspondence for Oakand, CA'}\n\n```{.r .cell-code}\ncsv_data %>%\n  select(-c(state, stab, place, PUMA22name)) %>%\n  filter(PlaceName == 'Oakland city, CA')\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|puma22 |PlaceName        |pop20  |afact |\n|:------|:----------------|:------|:-----|\n|00111  |Oakland city, CA |106433 |1     |\n|00112  |Oakland city, CA |106896 |1     |\n|00113  |Oakland city, CA |125840 |1     |\n|00114  |Oakland city, CA |9      |0     |\n|00123  |Oakland city, CA |101468 |1     |\n\n</div>\n:::\n:::\n\n\n\nThe `AFACT` (allocation factor) column shows the proportion of the\nsource area contained in the target area -- in this case the proportion\nof the PUMA population that belongs to Oakland. In this case, 100% of\nthe populations in PUMAs 111, 112, 113, and 123 belong to Oakland, and\n0% of PUMA 114. GEOCORR does believe that 9 individuals from 114 live in\nOakland, but based on the AFACT of 0 and the fact that I know the PUMA\noverlays the neighboring city of Piedmont, I'll feel comfortable\ndropping that PUMA.[^5]\n\n[^5]: Were the AFACT higher, e.g. 1%, I could randomly sample 1% of the\n    individuals from that PUMA and include them in my Oakland sample.\n\nNote that when you plot place to PUMA equivalence, you may find slight\ndifferences due to the fact that places can include un-populated areas,\nsuch as bodies of water [in the case of\nOakland](https://www.google.com/maps/place/Oakland,+CA/@37.7831602,-122.3045548,11z/data=!4m6!3m5!1s0x80857d8b28aaed03:0x71b415d535759367!8m2!3d37.8043514!4d-122.2711639!16zL20vMGRjOTU?entry=ttu&g_ep=EgoyMDI1MDIxMi4wIKXMDSoJLDEwMjExNDU1SAFQAw%3D%3D).\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Filter PUMAs for Oakland\noakland_pumas <- pumas_bay %>% filter(PUMACE20 %in% c('00111', '00112', '00113', '00123'))\noakland_place <- places_bay %>% filter(NAME == 'Oakland')\n# Get bounding box for Oakland PUMAs\noakland_bbox <- st_bbox(oakland_place)\n\ncrop_theme <- theme(\n    panel.spacing = unit(0, \"lines\"),      \n    plot.margin = margin(0, 0, 0, 0, \"cm\"),\n    axis.title.y = element_blank(),\n    axis.title.x = element_blank()\n)\n# Oakland Places Map\nmap_places <- ggplot() +\n  geom_sf(data = oakland_place, fill = \"lightblue\", color = \"darkgrey\", size = 0.1) +\n  geom_sf(data = oakland_place, fill = NA, color = \"red\", size = 4) +  \n  coord_sf(ylim = c(oakland_bbox[\"ymin\"], oakland_bbox[\"ymax\"]), \n           xlim = c(oakland_bbox[\"xmin\"], oakland_bbox[\"xmax\"]), \n           expand = FALSE) +\n  theme_minimal() +\n  crop_theme +\n  ggtitle(\"Oakland (Place)\")\n\n\n# Oakland PUMAs Map\nmap_pumas <- ggplot() +\n  geom_sf(data = oakland_pumas, fill = \"lightblue\", color = \"darkgrey\", size = 0.1) +\n  geom_sf_text(data = oakland_pumas, aes(label = PUMACE20)) +\n  geom_sf(data = oakland_place, fill = NA, color = \"red\", size = 4) +\n  #geom_sf(data = oakland_pumas, fill = NA, color = \"red\", size = 4) +  \n  coord_sf(ylim = c(oakland_bbox[\"ymin\"], oakland_bbox[\"ymax\"]), \n           xlim = c(oakland_bbox[\"xmin\"], oakland_bbox[\"xmax\"]), \n           expand = FALSE) +\n  theme_minimal() +\n  crop_theme +\n  ggtitle(\"Oakland (PUMAs)\")\n\nplot_grid(map_places, map_pumas, nrow = 1, align = \"h\", axis = \"t\", vjust = 0)\n```\n\n::: {.cell-output-display}\n![Place to PUMAs Correspondence in Oakland (place border in red)](index_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\nFiltering to those PUMAs gets us the 2022 1-year ACS microdata for the\nCity of Oakland (I use `haven::zap_labels()` just to remove some\nunnecessary formatting that comes with the data).\n\n\n\n::: {#tbl-oakland-raw .cell tbl-cap='2022 1-year ACS data for Oakland, CA'}\n\n```{.r .cell-code}\noakland_pumas <- c(111, 112, 113, 123)\noak <- data %>%\n  filter(PUMA %in% oakland_pumas) %>% \n  haven::zap_labels()\noak %>% head()\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| YEAR| SAMPLE| SERIAL|    CBSERIAL| HHWT| REPWT|      CLUSTER| STATEFIP| PUMA| STRATA| GQ| HHINCOME| REPWT1| REPWT2| REPWT3| REPWT4| REPWT5| REPWT6| REPWT7| REPWT8| REPWT9| REPWT10| REPWT11| REPWT12| REPWT13| REPWT14| REPWT15| REPWT16| REPWT17| REPWT18| REPWT19| REPWT20| REPWT21| REPWT22| REPWT23| REPWT24| REPWT25| REPWT26| REPWT27| REPWT28| REPWT29| REPWT30| REPWT31| REPWT32| REPWT33| REPWT34| REPWT35| REPWT36| REPWT37| REPWT38| REPWT39| REPWT40| REPWT41| REPWT42| REPWT43| REPWT44| REPWT45| REPWT46| REPWT47| REPWT48| REPWT49| REPWT50| REPWT51| REPWT52| REPWT53| REPWT54| REPWT55| REPWT56| REPWT57| REPWT58| REPWT59| REPWT60| REPWT61| REPWT62| REPWT63| REPWT64| REPWT65| REPWT66| REPWT67| REPWT68| REPWT69| REPWT70| REPWT71| REPWT72| REPWT73| REPWT74| REPWT75| REPWT76| REPWT77| REPWT78| REPWT79| REPWT80| PERNUM| PERWT| REPWTP| FAMUNIT| RELATE| RELATED| SEX| AGE| EDUC| EDUCD| INCTOT| REPWTP1| REPWTP2| REPWTP3| REPWTP4| REPWTP5| REPWTP6| REPWTP7| REPWTP8| REPWTP9| REPWTP10| REPWTP11| REPWTP12| REPWTP13| REPWTP14| REPWTP15| REPWTP16| REPWTP17| REPWTP18| REPWTP19| REPWTP20| REPWTP21| REPWTP22| REPWTP23| REPWTP24| REPWTP25| REPWTP26| REPWTP27| REPWTP28| REPWTP29| REPWTP30| REPWTP31| REPWTP32| REPWTP33| REPWTP34| REPWTP35| REPWTP36| REPWTP37| REPWTP38| REPWTP39| REPWTP40| REPWTP41| REPWTP42| REPWTP43| REPWTP44| REPWTP45| REPWTP46| REPWTP47| REPWTP48| REPWTP49| REPWTP50| REPWTP51| REPWTP52| REPWTP53| REPWTP54| REPWTP55| REPWTP56| REPWTP57| REPWTP58| REPWTP59| REPWTP60| REPWTP61| REPWTP62| REPWTP63| REPWTP64| REPWTP65| REPWTP66| REPWTP67| REPWTP68| REPWTP69| REPWTP70| REPWTP71| REPWTP72| REPWTP73| REPWTP74| REPWTP75| REPWTP76| REPWTP77| REPWTP78| REPWTP79| REPWTP80|\n|----:|------:|------:|-----------:|----:|-----:|------------:|--------:|----:|------:|--:|--------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|------:|-----:|------:|-------:|------:|-------:|---:|---:|----:|-----:|------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|\n| 2022| 202201|  74718| 2.02201e+12|    5|     1| 2.022001e+12|        6|  111|  11106|  3|  9999999|      5|      4|      5|      4|      5|      5|      5|      2|      6|       5|       5|       3|       5|       2|       2|       5|       4|       4|       4|       5|       4|       4|       5|       5|       4|       5|       5|       2|       2|       2|       5|       4|       5|       3|       3|       4|       5|       5|       4|       3|       5|       5|       4|       5|       5|       5|       4|       2|       5|       4|       4|       4|       5|       2|       3|       4|       5|       5|       4|       5|       5|       4|       5|       5|       4|       5|       5|       2|       2|       3|       3|       4|       4|       2|       2|       5|       4|       4|       5|       4|      1|     5|      1|       1|     13|    1301|   1|  20|    7|    71|      0|       5|       4|       5|       4|       5|       5|       5|       2|       6|        5|        5|        3|        5|        2|        2|        5|        4|        4|        4|        5|        4|        4|        5|        5|        4|        5|        5|        2|        2|        2|        5|        4|        5|        3|        3|        4|        5|        5|        4|        3|        5|        5|        4|        5|        5|        5|        4|        2|        5|        4|        4|        4|        5|        2|        3|        4|        5|        5|        4|        5|        5|        4|        5|        5|        4|        5|        5|        2|        2|        3|        3|        4|        4|        2|        2|        5|        4|        4|        5|        4|\n| 2022| 202201|  74737| 2.02201e+12|   56|     1| 2.022001e+12|        6|  111|  11106|  3|  9999999|      6|     81|     56|     66|     67|     81|     65|      7|     55|      80|      79|      11|      55|      83|      43|      12|       6|      42|      56|      56|       7|      82|      56|      65|      66|      79|      65|       7|      56|      81|      80|      12|      55|      83|      42|      12|       6|      43|      55|      55|       7|      82|      55|      68|      68|      81|      67|       6|      56|      78|      80|      12|      55|      82|      43|      11|       6|      42|      56|      54|       6|      82|      56|      66|      67|      78|      67|       6|      56|      81|      80|      12|      55|      82|      44|      12|       7|      43|      54|      55|      1|    56|      1|       1|     13|    1301|   1|  56|    6|    63|    480|       6|      81|      56|      66|      67|      81|      65|       7|      55|       80|       79|       11|       55|       83|       43|       12|        6|       42|       56|       56|        7|       82|       56|       65|       66|       79|       65|        7|       56|       81|       80|       12|       55|       83|       42|       12|        6|       43|       55|       55|        7|       82|       55|       68|       68|       81|       67|        6|       56|       78|       80|       12|       55|       82|       43|       11|        6|       42|       56|       54|        6|       82|       56|       66|       67|       78|       67|        6|       56|       81|       80|       12|       55|       82|       44|       12|        7|       43|       54|       55|\n| 2022| 202201|  74738| 2.02201e+12|   15|     1| 2.022001e+12|        6|  113|  11306|  4|  9999999|     15|     13|     15|     15|     15|     15|     14|     15|     13|      15|      15|      15|      14|      15|      15|      13|      14|      13|      15|      17|      13|      15|      17|      15|      14|      13|      14|      14|      17|      15|      14|      14|      15|      12|      14|      15|      16|      15|      15|      16|      13|      15|      15|      15|      14|      15|      13|      14|      13|      13|      12|      14|      15|      12|      13|      15|      16|      15|      15|      12|      13|      15|      15|      15|      15|      14|      15|      15|      14|      15|      15|      15|      15|      15|      16|      13|      14|      12|      14|      13|      1|    15|      1|       1|     12|    1270|   1|  34|    6|    63|   1200|      15|      13|      15|      15|      15|      15|      14|      15|      13|       15|       15|       15|       14|       15|       15|       13|       14|       13|       15|       17|       13|       15|       17|       15|       14|       13|       14|       14|       17|       15|       14|       14|       15|       12|       14|       15|       16|       15|       15|       16|       13|       15|       15|       15|       14|       15|       13|       14|       13|       13|       12|       14|       15|       12|       13|       15|       16|       15|       15|       12|       13|       15|       15|       15|       15|       14|       15|       15|       14|       15|       15|       15|       15|       15|       16|       13|       14|       12|       14|       13|\n| 2022| 202201|  75005| 2.02201e+12|   38|     1| 2.022001e+12|        6|  113|  11306|  4|  9999999|     36|     38|     36|     37|     35|     36|     38|     39|     33|      39|      37|      37|      37|      37|      37|      36|      37|      35|      39|      37|      37|      34|      36|      37|      38|      35|      36|      35|      39|      35|      36|      34|      38|      37|      36|      38|      36|      36|      36|      36|      35|      36|      39|      36|      36|      37|      37|      37|      38|      35|      33|      38|      36|      34|      36|      37|      35|      37|      37|      36|      35|      39|      37|      39|      37|      38|      38|      36|      37|      37|      37|      35|      37|      37|      35|      37|      35|      37|      37|      37|      1|    38|      1|       1|     12|    1270|   2|  40|    2|    23|  41300|      36|      38|      36|      37|      35|      36|      38|      39|      33|       39|       37|       37|       37|       37|       37|       36|       37|       35|       39|       37|       37|       34|       36|       37|       38|       35|       36|       35|       39|       35|       36|       34|       38|       37|       36|       38|       36|       36|       36|       36|       35|       36|       39|       36|       36|       37|       37|       37|       38|       35|       33|       38|       36|       34|       36|       37|       35|       37|       37|       36|       35|       39|       37|       39|       37|       38|       38|       36|       37|       37|       37|       35|       37|       37|       35|       37|       35|       37|       37|       37|\n| 2022| 202201|  75119| 2.02201e+12|   20|     1| 2.022001e+12|        6|  111|  11106|  3|  9999999|     22|     21|     21|     22|     20|     20|     21|     21|     20|      22|      22|      22|      20|      22|      21|      21|      22|      22|      20|      22|      22|      22|      22|      22|      20|      21|      20|      22|      21|      21|      20|      22|      21|      22|      22|      20|      21|      22|      22|      22|      20|      22|      22|      22|      21|      21|      22|      22|      21|      22|      21|      20|      22|      21|      20|      20|      22|      23|      21|      22|      22|      22|      22|      20|      20|      22|      22|      21|      22|      22|      22|      21|      20|      22|      22|      20|      20|      22|      22|      22|      1|    20|      1|       1|     13|    1301|   2|  88|    2|    23|   5800|      22|      21|      21|      22|      20|      20|      21|      21|      20|       22|       22|       22|       20|       22|       21|       21|       22|       22|       20|       22|       22|       22|       22|       22|       20|       21|       20|       22|       21|       21|       20|       22|       21|       22|       22|       20|       21|       22|       22|       22|       20|       22|       22|       22|       21|       21|       22|       22|       21|       22|       21|       20|       22|       21|       20|       20|       22|       23|       21|       22|       22|       22|       22|       20|       20|       22|       22|       21|       22|       22|       22|       21|       20|       22|       22|       20|       20|       22|       22|       22|\n| 2022| 202201|  75131| 2.02201e+12|   11|     1| 2.022001e+12|        6|  123|  12306|  3|  9999999|     12|      0|      8|     20|      8|      0|      1|     14|     14|       1|      16|      11|      13|      20|       8|      14|       0|       8|      13|      12|      11|      15|       8|       1|      10|      13|      17|      15|      12|      19|       1|      12|       0|       1|       8|       1|      16|       8|      12|      10|      11|       1|       9|      21|       7|       0|       1|      13|      14|       1|      16|      10|      13|      18|      10|      13|       1|       9|      14|      11|      10|      13|       9|       1|       9|      15|      17|      15|      13|      19|       0|      14|       0|       1|       9|       1|      16|       9|      11|       9|      1|    11|      1|       1|     13|    1301|   2|  86|    2|    23|      0|      12|       0|       8|      20|       8|       0|       1|      14|      14|        1|       16|       11|       13|       20|        8|       14|        0|        8|       13|       12|       11|       15|        8|        1|       10|       13|       17|       15|       12|       19|        1|       12|        0|        1|        8|        1|       16|        8|       12|       10|       11|        1|        9|       21|        7|        0|        1|       13|       14|        1|       16|       10|       13|       18|       10|       13|        1|        9|       14|       11|       10|       13|        9|        1|        9|       15|       17|       15|       13|       19|        0|       14|        0|        1|        9|        1|       16|        9|       11|        9|\n\n</div>\n:::\n:::\n\n\n\n## Step 3: Specify the unit of analysis {#sec-step-3}\n\nEach row in the ACS microdata represents an individual, identified by a\nunique combination of `SAMPLE`, which defines the year when the\nindividual was surveyed, `SERIAL`, a unique identifier for that\nindividual's household, and `PERNUM`, a unique identifier for the\nindividual within their household [@ruggles_ipums_2024]. Given that, we\ncan define a straightforward definition for unique individuals in the\nACS:\n\n-   **Individuals**: The combination of `SAMPLE`, `SERIAL`, and `PERNUM`\n    provides a unique identifier for every person in the IPUMS. ACS\n    microdata is typically individual-level, so this filtering should\n    not be necessary.\n\nDefining a household is slightly more complex.\n\n-   **Housing Units**: The combination of `SAMPLE` and `SERIAL` provides\n    a unique identifier for every housing unit in the IPUMS. However,\n    this includes both households and general group quarter housing,\n    e.g. correctional facilities. Housing units are further categorized\n    using the\n    [`GQ`]((https://usa.ipums.org/usa-action/variables/GQ#description_section))\n    variable.\n    -   **Households**: Non group quarter housing units, defined by\n        filtering housing units to those where `GQ`, is 1 or 2\n        [@bloem_matching_2018]. The [comparability\n        section](https://usa.ipums.org/usa-action/variables/GQ#comparability_section)\n        of the `GQ` variable documentation explains this choice in\n        greater detail, but in short it represents non-institutional\n        housing-units where 9 or fewer people unrelated to the\n        head-of-household reside.\n\nWe can group/filter by these variable combinations and see how many\nindividuals, housing units, and households were surveyed across PUMAs in\nOakland for the 2022 1-year ACS.\n\n\n\n::: {#tbl-granularity .cell tbl-cap='Oakland Dataset Granularity by PUMA'}\n\n```{.r .cell-code}\noak %>%\n  group_by(PUMA) %>%\n  summarise(\n    n_rows = n(),\n    n_individuals = n_distinct(SAMPLE, SERIAL, PERNUM),\n    n_housing_units = n_distinct(SAMPLE, SERIAL)) %>%\n  inner_join(oak %>%\n               filter(GQ <= 2) %>%\n               group_by(PUMA) %>%\n               summarise(n_households = n_distinct(SAMPLE, SERIAL)),\n             by = join_by(PUMA))\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| PUMA| n_rows| n_individuals| n_housing_units| n_households|\n|----:|------:|-------------:|---------------:|------------:|\n|  111|   1083|          1083|             578|          539|\n|  112|   1152|          1152|             547|          517|\n|  113|    989|           989|             385|          337|\n|  123|    905|           905|             389|          354|\n\n</div>\n:::\n:::\n\n\n\nWe see that each row in the data represents an individual (`n_rows`\nequals `n_individuals`) and, as we would expect, the number of housing\nunits and households are successively lower than the number of\nindividuals.\n\n### Households, group quarters, and institutions\n\nSo, given that our motivating question concerns household income, we can\napply the following filtering, per [@bloem_matching_2018], and isolate just the heads of all the households in Oakland.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhouseholds <- oak %>%\n  filter(\n    PERNUM == 1,\n    GQ <= 2\n    )\n```\n:::\n\n\n\nBut to build a little intuition around that selection, let's also look at what different housing units look like, and *why* the household definition is restrictive. \n#### A household\nI'll randomly select a household in the data to see what such a unit\nlooks like in practice.\n\n\n\n::: {#tbl-example-hh .cell tbl-cap='An example household with income data'}\n\n```{.r .cell-code  code-fold=\"true\"}\nhousehold_serials <- oak %>%\n  filter(GQ <= 2) %>%\n  group_by(SERIAL) %>%\n  count() %>%\n  filter(n > 1) %>%\n  pull(SERIAL)\nset.seed(5)\nsample_household_true <- sample(household_serials, 1)\nn <- oak %>% filter(SERIAL == sample_household_true) %>% dim() %>% .[1]\noak %>% filter(SERIAL == sample_household_true) %>% \n  select(c(SERIAL, GQ, PERNUM, RELATE, AGE, SEX, HHINCOME, INCTOT))\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| SERIAL| GQ| PERNUM| RELATE| AGE| SEX| HHINCOME|  INCTOT|\n|------:|--:|------:|------:|---:|---:|--------:|-------:|\n| 196265|  1|      1|      1|  41|   2|   184500|  104500|\n| 196265|  1|      2|      2|  40|   1|   184500|   60000|\n| 196265|  1|      3|      3|   6|   2|   184500| 9999999|\n| 196265|  1|      4|      3|   4|   1|   184500| 9999999|\n| 196265|  1|      5|      6|  65|   2|   184500|   20000|\n\n</div>\n:::\n:::\n\n\n\nThis household, with `SERIAL`\n196265 has 5\nmembers, each with a unique `PERNUM`. To identify household members'\nrelationship to the head-of-household, [we reference\n`RELATE`](https://usa.ipums.org/usa-action/variables/RELATE#codes_section).\nThe first row, where `RELATE==1` denotes a 41 year old female (`SEX==2`)\nhead-of-household. The second, where `RELATE==2` denotes their spouse,\nin this case a 40 year old male. This couple has two children\n(`RELATE==3`) and the husband's mother (`RELATE==6`), also living in the\nhousehold.\n\n#### Non-institutional group-quarters\nWe can contrast that fairly \"nuclear family\" household with a\nnon-institutional group-quarters housing unit in Oakland:\n\n\n\n::: {#tbl-example-gq .cell tbl-cap='An example group quarters unit with income data'}\n\n```{.r .cell-code  code-fold=\"true\"}\nhousehold_serials <- oak %>%\n  filter(GQ == 5) %>%\n  pull(SERIAL)\nset.seed(6)\nsample_household <- sample(household_serials, 1)\nsampled_data <- oak %>% \n  filter(SERIAL == sample_household) %>% \n  select(SERIAL, PUMA, GQ, PERNUM, RELATE, AGE, SEX, HHINCOME, INCTOT)\nsampled_data\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| SERIAL| PUMA| GQ| PERNUM| RELATE| AGE| SEX| HHINCOME| INCTOT|\n|------:|----:|--:|------:|------:|---:|---:|--------:|------:|\n| 165880|  111|  5|      1|      1|  26|   1|   474800|      0|\n| 165880|  111|  5|      2|     11|  26|   2|   474800| 125000|\n| 165880|  111|  5|      3|     11|  39|   1|   474800|  35000|\n| 165880|  111|  5|      4|     11|  33|   2|   474800|  60000|\n| 165880|  111|  5|      5|     11|  31|   2|   474800|  35000|\n| 165880|  111|  5|      6|     11|  26|   1|   474800|  21000|\n| 165880|  111|  5|      7|     11|  26|   1|   474800|      0|\n| 165880|  111|  5|      8|     11|  26|   2|   474800|  80000|\n| 165880|  111|  5|      9|     11|  26|   1|   474800|  80800|\n| 165880|  111|  5|     10|     11|  25|   2|   474800|  38000|\n| 165880|  111|  5|     11|     11|  22|   1|   474800|      0|\n\n</div>\n:::\n:::\n\n\n\nWe see that there is a head-of-household, a 26 year old male, but all 10\nother housing unit members are \"partners, friends, \\[or\\] visitors\"\n(`RELATE==11`). These are largely adults with earnings, so this group\nquarters unit would have a very high \"household income\" and bias the estimated median income up were it included\nfor that calculation.\n\n#### Institutional housing units\nGroup quarters also include institutional housing units, such as\ncorrectional facilities.\n\n\n\n::: {#tbl-example-ih .cell tbl-cap='An example institutional housing unit with income data'}\n\n```{.r .cell-code  code-fold=\"true\"}\nhousehold_serials <- oak %>%\n  filter(GQ == 3) %>%\n  pull(SERIAL)\nset.seed(1)\nsample_household <- sample(household_serials, 1)\nsampled_data <- oak %>% \n  filter(SERIAL == sample_household) %>% \n  select(SERIAL, PUMA, GQ, PERNUM, RELATE, AGE, SEX, HHINCOME, INCTOT)\nsampled_data\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| SERIAL| PUMA| GQ| PERNUM| RELATE| AGE| SEX| HHINCOME| INCTOT|\n|------:|----:|--:|------:|------:|---:|---:|--------:|------:|\n|  75131|  123|  3|      1|     13|  86|   2|  9999999|      0|\n\n</div>\n:::\n:::\n\n\n\nThis housing unit only includes one member, an 86 year old woman who is\nan \"institutional inmate\" (`RELATE==13`) with no income. This unit would likely bias median household income down, were it included.\n\n### Household Income in the ACS\n\nLet's return to the motivating question:  \n\n::: {.callout-tip}\n## Motivating Question\n**What was the median household\nincome in Oakland, California in 2022?**  \n:::  \n\nNote the income\nvariables we observe for this family:\n\n1.  `INCTOT` reports each respondent's total pre-tax personal income or\n    losses from all sources for the previous year. `9999999` is code to\n    denote that the value is missing, which makes sense given that the\n    missing values above correspond to children in the household [See\n    [INCTOT](https://usa.ipums.org/usa-action/variables/INCTOT) in\n    @ruggles_ipums_2024].\n\n2.  `HHINCOME` reports the total money income of all household members\n    age 15+ during the previous year. The amount should equal the sum of\n    all household members' individual incomes, as recorded in the\n    person-record variable INCTOT [See\n    [HHINCOME](https://usa.ipums.org/usa-action/variables/HHINCOME) in\n    @ruggles_ipums_2024]\n\nGiven what we know about the unique identifier for households, and the\n`HHINCOME` variable, we can construct the appropriate dataset for\nanswering our motivating question â€“ every household in Oakland that had\nhousehold income.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhouseholds_w_income <- households %>%\n  filter(\n    HHINCOME != 9999999,\n    HHINCOME >= 0,\n    )\n```\n:::\n\n\n\nIt seems we can proceed to simply calculate the median of the `HHINCOME`\ncolumn? Not so fast... Data in the ACS microdata are not what they seem.\nBefore we do any analysis, we have to account for **sample weights**.\n\n## Step 4: Applying sample weights {#sec-step-4}\n\nLet's return to our sample family from above, but also examine the\nvariables `PERWT` and `HHWT`.\n\n\n\n::: {#tbl-example-hh-weights .cell tbl-cap='An example household with weight data'}\n\n```{.r .cell-code}\noak %>% filter(SERIAL == sample_household_true) %>% \n  select(c(AGE, SEX, HHINCOME, INCTOT, PERWT, HHWT))\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| AGE| SEX| HHINCOME|  INCTOT| PERWT| HHWT|\n|---:|---:|--------:|-------:|-----:|----:|\n|  41|   2|   184500|  104500|    51|   51|\n|  40|   1|   184500|   60000|    64|   51|\n|   6|   2|   184500| 9999999|    51|   51|\n|   4|   1|   184500| 9999999|    51|   51|\n|  65|   2|   184500|   20000|    76|   51|\n\n</div>\n:::\n:::\n\n\n\nThese are the two primary sample weights in ACS microdata, and they can\nbe interpreted fairly directly.\n\n-   `PERWT` gives the population represented by each individual in the\n    sample, thus in the first row of the sample household, the 41 year\n    old woman with an individual income of \\$104,500 represents 51\n    individuals in the PUMA.\n-   `HHWT` gives the number of households in the general population\n    represented by each household in the sample, thus this household is\n    representative of 51 households in that PUMA.\n\nAny analysis or visualization of ACS microdata should be weighted by\n`PERWT` for work at the person-level, and `HHWT` for the household-level\n[See [Sample Weights](https://usa.ipums.org/usa/intro.shtml#weights) in\n@ruggles_ipums_2024].\n\nFor visualizations, we pass the weights to the plotting API. For\nexample, in `ggplot` many chart types support a `weight` argument.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(households_w_income, aes(x = HHINCOME, weight = HHWT)) +\n  geom_histogram(\n    binwidth = 50000,\n    fill = \"lightblue\",\n    alpha = 1,\n    color = \"grey\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(\n    labels = scales::label_currency(scale_cut = scales::cut_short_scale()),\n    breaks = seq(0, 1600000, 200000)\n  ) +\n  scale_y_continuous(labels=scales::label_comma()) +\n  labs(x = \"Household Income\", y = \"Number of Households\")\n```\n\n::: {.cell-output-display}\n![Household Income Distribution in Oakland (with household weights)](index_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\n\nFor analysis, we use the\n[`srvyr`](http://gdfe.co/srvyr/reference/index.html) package to define\nthe survey weights before using them to calculate statistics. For\nexample, here we'll finally address the motivating question.  \n\n::: {.callout-tip}\n## Motivating Question\n**What was the median household\nincome in Oakland, California in 2022?**  \n:::\n\nThe answer, as measured in the IPUMS\nmicrodata is as follows:\n\n\n\n::: {#tbl-answer .cell tbl-cap='2022 Median Household Income in Oakland, CA'}\n\n```{.r .cell-code}\nhouseholds_w_income %>%\n  as_survey(weights=HHWT) %>%\n  summarise(weighted_median = survey_median(HHINCOME)) %>% \n  select(weighted_median)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| weighted_median|\n|---------------:|\n|           89000|\n\n</div>\n:::\n:::\n\n\n\nLet's do a quick comparison of our IPUMS results to the aggregate census\ndata we retrieved in the first section. Here are our full IPUMS results\nfor both median household income and population:\n\n\n\n::: {#tbl-ipums-res .cell tbl-cap='IPUMS versus ACS aggregate results'}\n\n```{.r .cell-code  code-fold=\"true\"}\nmedian_table <- households_w_income %>%\n  as_survey(weights=HHWT) %>% \n  summarise(weighted_median = survey_median(HHINCOME)) %>% \n  mutate(variable = \"median_hh_income\",\n         ipums_estimate = weighted_median,\n         se = weighted_median_se)\n\ncount_table <- oak %>%\n  as_survey(weights=PERWT) %>% \n  survey_count() %>% \n  mutate(variable = \"total_pop\",\n         ipums_estimate = n,\n         se = n_se)\n\naggregate_data <- oakland_stats %>%\n  select(c(variable, estimate)) %>%\n  rename(ACS_aggregate_estimate = estimate)\n\nbind_rows(count_table, median_table) %>% \n  select(c(variable, ipums_estimate)) %>% inner_join(aggregate_data, by='variable')\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|variable         | ipums_estimate| ACS_aggregate_estimate|\n|:----------------|--------------:|----------------------:|\n|total_pop        |         430052|                 426323|\n|median_hh_income |          89000|                  93146|\n\n</div>\n:::\n:::\n\n\n\nThese are clearly different. What gives? Unfortunately, **summary\nstatistics calculated using IPUMS data typically cannot match aggregate\nACS figures!**\n\nOne major reason for this gap is additional sampling error. Recall that\nthe American Community Survey is a sample. A given one-year ACS is\ntypically a 1% sample of the U.S. population, with associated sampling\nerror. When the census makes microdata available, they create a sample\nof that sample -- we do not get the full 1%. This second sampling\nprocess introduces further sampling error in the microdata that is not\nreflected in figures sourced from aggregate ACS data, which are\ncalculated using the full ACS sample [[See\nACS](https://usa.ipums.org/usa/chapter2/chapter2.shtml#ACS) in\n@ruggles_ipums_2024].\n\n## Step 5: Calculating standard errors {#sec-step-5}\n\nNow we have estimates derived from the ACS microdata, but how do we\ncommunicate uncertainty around those estimates? For the sake of this\nexample, let's explore the problem of standard errors via a slightly\ndifferent motivating question -- **what was the average household income\nin Oakland in 2022**?\n\nI'm switching to the average because we are about to calculate some\nstandard errors from-scratch, and that's much more clear-cut for linear\nstatistics like the average than for non-linear statistics like the\nmedian. We will return to the median at the end of this section, but\nrely on a package implementation and not do anything from-scratch.\n\nAnyways, let's calculate the average household income in Oakland using\nthe sample household weights. The [formula for the weighted\naverage](https://en.wikipedia.org/wiki/Weighted_arithmetic_mean#Mathematical_definition)\nis: $$\n\\bar{X}_w = \\frac{\\sum_{i=1}^{n}w_ix_i}{\\sum_{i=1}^{n}w_i}\n$$ We'll put that into code and get the following estimate of the\naverage household income in Oakland:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nweighted_mean <- (\n  sum(households_w_income$HHINCOME * households_w_income$HHWT)\n  / sum(households_w_income$HHWT)\n  )\nweighted_mean\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 140889.1\n```\n\n\n:::\n:::\n\n\n\nNow, let's get a standard error for that estimate. Correctly utilizing\nthe sample weights in the variance calculation is [a little\ntricky](https://en.wikipedia.org/wiki/Weighted_arithmetic_mean#Variance)\nand there are a few different approaches to the problem. I'll cover two\ntechniques for variance estimation: the Taylor series method (also\nreferred to as \"linearized variance\" or Taylor series linearization) and\nreplication weights. Each of these are implemented in the `survey`\npackage [@lumley_analysis_2004].\n\n### The Taylor series method\n\nMost packages (e.g.\n[SAS](https://documentation.sas.com/doc/en/statug/latest/statug_surveymeans_details06.htm#statug.surveymeans.variancedetails),\n[Stata](https://www.stata.com/manuals/svyvarianceestimation.pdf), and\nR's [`survey`\npackage](https://r-survey.r-forge.r-project.org/survey/#:~:text=Variances%20by%20Taylor%20linearization))\ndefault to using the Taylor series method for variance estimation of\nweighted statistics. This is because weighted statistics, like the\naverage above, are not linear with respect to the weights, making\nanalytical calculation of the variance difficult or else undefined\n[@lohr_sampling_2021]. The Taylor series method entails calculating the\nfirst-order linear approximation of the statistic (e.g. the linear\napproximation of the weighted mean formula above), then finding the\nvariance of that approximation. The details of those calculations are\nbeyond the scope of this post, but [@lohr_sampling_2021, chap 9.1]\ncovers the process in detail, and Wikipedia has a nice\n[walkthrough](https://en.wikipedia.org/wiki/Weighted_arithmetic_mean#Variance_of_the_weighted_mean_(%CF%80-estimator_for_ratio-mean))\nas well. We will just work with the formula for linearized variance that\nis arrived at in the Wikipedia post:\n\n$$\n\\hat{\\text{Var}}(\\bar{X}_w) = \\frac{\\sum w_i^2 (x_i - \\bar{X}_w)^2}{\\left(\\sum w_i\\right)^2} \n$$ The standard error is just the square root of that variance.\\\n$$\n\\hat{\\text{SE}}(\\bar{X}_w) = \\sqrt{\\hat{\\text{Var}}(\\bar{X}_w)}\n$$ We'll code that up in R:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnumerator <- sum(households_w_income$HHWT^2 * (households_w_income$HHINCOME - weighted_mean)^2)\ndenominator <- sum(households_w_income$HHWT)^2\nvariance <- numerator / denominator\nse <- sqrt(variance)\nse\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4141.197\n```\n\n\n:::\n:::\n\n\n\nThat result is a usable estimate for the standard error of the average\nhousehold income in Oakland. Of course in practice we would always use a\npackage, in this case `survey`, for calculating that. Here I'll\ncalculate the average household income in Oakland using `survey`, and it\nwill default to returning a standard error via the Taylor series method.\n\n\n\n::: {#tbl-wrong-se .cell tbl-cap='Taylor Series Method Standard Error'}\n\n```{.r .cell-code}\nhouseholds_w_income %>%\n  as_survey(weights=HHWT) %>%\n  summarise(weighted_mean = survey_mean(HHINCOME))\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| weighted_mean| weighted_mean_se|\n|-------------:|----------------:|\n|      140889.1|         4142.384|\n\n</div>\n:::\n:::\n\n\n\nThis standard error is off from ours by about \\$1, which could be due to\nother corrections applied in the package's variance calculation. We\nneedn't be too worried about this discrepancy, though. While the Taylor\nseries method is popular in survey analysis, [IPUMS\nrecommends](https://usa.ipums.org/usa/repwt.shtml) that we use the\nreplication weights method when working with ACS microdata.\n\n### Replication weights\n\nIn theory, the [standard\nerror](https://en.wikipedia.org/wiki/Standard_error) of an estimate\nmeasures the variation of a statistic across multiple samples from a\ngiven population. Our sample standard error above, calculated using just\nthe one sample, is an estimate of that theoretical standard error.\n\nReplicate methods operate under the assumption that one sample can be\nconceived of as a miniature version of the population. Instead of taking\nmultiple samples from the population to construct a variance estimator,\none may simply resample from the full, original sample to mimic the\ntheoretical basis of standard errors [@lohr_sampling_2021, chap 9.1].\n\nIn practice, we don't explicitly do any resampling, instead relying on\n\"replicate weights\" that the census pre-computes. Here we can see what\nreplicate weights (in this case, household replicate weights) look like\nin our data. Each of `REPWT`, 1 through 80, is a set of alternative\nhousehold weights, slightly different from the \"production weight,\"\n`HHWT`.\n\n\n\n::: {#tbl-rep-weights .cell tbl-cap='ACS Replicate Household Weights'}\n\n```{.r .cell-code}\nhouseholds_w_income %>%\n  mutate(` ` = \"...\") %>% \n  select(c(HHINCOME, HHWT, REPWT1, REPWT2, ` `, REPWT79, REPWT80)) %>% \n  head()\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| HHINCOME| HHWT| REPWT1| REPWT2|    | REPWT79| REPWT80|\n|--------:|----:|------:|------:|:---|-------:|-------:|\n|   129200|   50|     80|     47|... |      49|      50|\n|   107000|   73|     20|     25|... |      72|      74|\n|   138000|   74|     86|     85|... |      21|      73|\n|    44800|   41|     12|     43|... |      39|      44|\n|   380000|  311|     99|     86|... |     323|     105|\n|   189150|   65|     22|     20|... |     108|     122|\n\n</div>\n:::\n:::\n\n\n\nWe can use replicate weights in a variety of alternative variance\nestimates. In the IPUMS documentation on ACS replicate weights, they\noutline a [variance estimation\nprocedure](https://usa.ipums.org/usa/repwt.shtml#:~:text=can%20change%20substantially.-,HOW%20DO%20I%20OBTAIN%20REPLICATE%20STANDARD%20ERRORS%20FROM%20IPUMS%2DUSA%20DATA%3F,-There%20are%203)\nthat matches the \"successive difference replication (SDR) variance\"\nmethod. We obtain SDR variance by calculating our statistic of interest\nwith the production weights (e.g. `HHWT`) as follows:\\\n$$\n\\bar{X}_w = \\frac{\\sum_{i=1}^{n}\\text{HHWT}_ix_i}{\\sum_{i=1}^{n}\\text{HHWT}_i}\n$$\n\nThen, for each of the 80 household replicate weights (e.g. `REPWT1`), we\ncalculate the weighted average income using the replicate weight\\\n$$\n\\bar{X}_r = \\frac{\\sum_{i=1}^{n}\\text{REPWT}_{ri}x_i}{\\sum_{i=1}^{n}\\text{REPWT}_{ri}}\n$$ and sum the squared deviations between that \"replicate-weighted\"\nestimate, $\\bar{X}_r$, and the production weighted estimate.\nSpecifically: $$\n\\begin{align*}\n\\hat{\\text{Var}}(\\bar{X}_w) &= \\frac{4}{80} \\sum_{r=1}^{80} (\\bar{X}_r - \\bar{X}_w)^2\n\\end{align*}\n$$\n\n[@fay_aspects_1995] initially proposed that variance estimator and\n[their\npaper](https://www.census.gov/content/dam/Census/library/working-papers/1995/demo/faytrain95.pdf)\ngives the full derivation. The standard error is once again just the\nsquare root of that variance.\n\n$$\n\\hat{\\text{SE}}(\\bar{X}_w) = \\sqrt{\\hat{\\text{Var}}(\\bar{X}_w)}\n$$ In code, calculated via a for-loop, we'd get the following:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate X_r\nX_r <- vector()\nfor (r in 1:80){\n  X_r[r] <- households_w_income %>%\n    as_survey(weights=glue(\"REPWT\", r)) %>%\n    summarise(weighted_mean = survey_mean(HHINCOME)) %>% \n    .$weighted_mean\n}\n# Calculate X\nX <- households_w_income %>%\n    as_survey(weights=HHWT) %>%\n    summarise(weighted_mean = survey_mean(HHINCOME)) %>% \n    .$weighted_mean\n\n# Sum over r\nsqrt( (4/80) * sum( (X_r - X)^2 ) )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3432.032\n```\n\n\n:::\n:::\n\n\n\nTo be clear, we don't do that manually in practice -- `survey` supports\nspecifying survey designs with replicate weights. The following is an\nimplementation of the same standard error calculation procedure. Note\nthe identical output.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhouseholds_w_income %>% \n  as_survey_rep(\n  weight = HHWT ,\n  repweights = matches(\"REPWT[0-9]+\"),\n  type = \"successive-difference\",\n  mse = TRUE) %>%\n  summarise(weighted_mean = survey_mean(HHINCOME))\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| weighted_mean| weighted_mean_se|\n|-------------:|----------------:|\n|      140889.1|         3432.032|\n\n</div>\n:::\n:::\n\n\n\nThere are a few different replicate methods for variance estimation,\nseveral of which are equivalent. For example, the successive-difference\nstandard error above is equivalent to using [Fay's Balanced Repeated\nReplication\nmethod](https://documentation.sas.com/doc/en/statug/15.2/statug_surveyphreg_details29.htm)\nwith the Faye coefficient set to $\\epsilon=.5$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhouseholds_w_income %>% \n  as_survey_rep(\n  weight = HHWT ,\n  repweights = matches(\"REPWT[0-9]+\"),\n  type = \"Fay\",\n  rho=.5,\n  mse = TRUE) %>%\n  summarise(weighted_mean = survey_mean(HHINCOME))\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| weighted_mean| weighted_mean_se|\n|-------------:|----------------:|\n|      140889.1|         3432.032|\n\n</div>\n:::\n:::\n\n\n\nIPUMS' documentation [recommends](https://usa.ipums.org/usa/repwt.shtml)\nthe following specification, where the replicate method is set to \"ACS.\"\nThis has the same output as the previous two specifications, and is\npresumably implementing SDR variance.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhouseholds_w_income %>% \nas_survey_rep(\n  weight = HHWT,\n  repweights = matches(\"REPWT[0-9]+\"),\n  type = \"ACS\",\n  mse = TRUE) %>%\n  summarise(weighted_mean = survey_mean(HHINCOME))\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| weighted_mean| weighted_mean_se|\n|-------------:|----------------:|\n|      140889.1|         3432.032|\n\n</div>\n:::\n:::\n\n\n\nNow that we have the methods established, we'll return to the median,\nrather than the mean, household income. Some of the above formulas would\nbe different for the median, but the code and package implementations\nare the same. Here we will calculate the median household income in\nOakland, and the standard error for that statistic using three methods:\n\n1.  ACS aggregate data retrieval\n2.  Estimation via IPUMS microdata with Taylor series variance/standard\n    error\n3.  Estimation via IPUMS microdata with replicate weight\n    variance/standard error\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndf1 <- oakland_stats %>% \n  mutate(`Standard Error` = moe/1.645, Method = 'Aggregate Data') %>%\n  filter(variable == 'median_hh_income') %>% \n  select(c(Method, estimate, `Standard Error`)) %>% \n  rename(`Weighted Median HH Income` = estimate)\ndf2 <- households_w_income %>% \nas_survey(\n  weights = HHWT) %>%\n  summarise(`Weighted Median HH Income` = survey_median(HHINCOME)) %>% \n  rename(`Standard Error` = `Weighted Median HH Income_se`) %>% \n  mutate(Method = 'Microdata w/ Taylor Series Method') %>% \n  select(c(Method, `Weighted Median HH Income`, `Standard Error`))\ndf3 <- households_w_income %>% \nas_survey_rep(\n  weight = HHWT,\n  repweights = matches(\"REPWT[0-9]+\"),\n  type = \"ACS\",\n  mse = TRUE) %>%\n  summarise(`Weighted Median HH Income` = survey_median(HHINCOME)) %>% \n  rename(`Standard Error` = `Weighted Median HH Income_se`) %>% \n  mutate(Method = 'Microdata w/ Replicate Weights') %>% \n  select(c(Method, `Weighted Median HH Income`, `Standard Error`))\n\ncombined_df <- bind_rows(df1, df2, df3)\ncombined_df\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|Method                            | Weighted Median HH Income| Standard Error|\n|:---------------------------------|-------------------------:|--------------:|\n|Aggregate Data                    |                     93146|       3788.450|\n|Microdata w/ Taylor Series Method |                     89000|       4354.199|\n|Microdata w/ Replicate Weights    |                     89000|       3793.112|\n\n</div>\n:::\n:::\n\n\n\nIn IPUMS testing of ACS/PRCS data, replicate weights usually increase\nstandard errors, though in our case the replicate weights produced a\nslightly smaller standard error than the Taylor series method.\n\n## A Production Workflow\n\nPutting all of this post together, here is production code that I would\nuse to build a dataset for analyzing household-level variables in\nOakland, leaning on the IPUMS and geocorr helper functions I defined\nabove, and specifying replicate weights and \"ACS\" variance.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get the IPUMS data for California\ndata <- ipums_data(\"us2022a\", variables, \"Incomes by PUMA\")\n# Find the geographic correspondence for places -> pumas\ncsv_data <- geocorr_2022(\"Ca06\", \"puma22\", \"place\", \"pop20\")\n# isolate pumas that correspond to oakland and have oak population\noak_pumas <- csv_data %>%\n  select(-c(state, stab, place, PUMA22name)) %>%\n  filter(PlaceName == 'Oakland city, CA', afact > 0) %>% \n  pull(puma22) %>% \n  as.integer()\n# Filter the IPUMS data to those pumas\noak_hh_w_inc <- data %>%\n  filter(PUMA %in% oak_pumas) %>% \n  distinct(SAMPLE, SERIAL, .keep_all = TRUE) %>% \n  filter(HHINCOME != 9999999, HHINCOME > 0) %>% \n  haven::zap_labels()\n# convert the IPUMS data to a survey dataset\noak_hh_inc_svy <- households_w_income %>% \n  as_survey_rep(\n    weight = HHWT,\n    repweights = matches(\"REPWT[0-9]+\"),\n    type = \"ACS\",\n    mse = TRUE)\n```\n:::\n\n\n\nAnd here is the median household income.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\noak_hh_inc_svy %>%\n  summarise(median_hh_inc = survey_median(HHINCOME))\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| median_hh_inc| median_hh_inc_se|\n|-------------:|----------------:|\n|         89000|         3793.112|\n\n</div>\n:::\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}