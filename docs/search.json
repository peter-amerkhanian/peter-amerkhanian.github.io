[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "The following are some of my formal, public-facing projects:\nA Tale of Two Cities: Alameda and Alameda Point\nPeter Amerkhanian (2023), Report for The City of Alameda Department of Finance.\nSummary: I integrate various internal data sources to 1.) Create a historical narrative of re-development in the Alameda Point region and 2.) Project the benefits/costs of development alternatives in the region.\nTools: geopandas, sklearn. Methods: linear regression.\n\nA Reexamination of Proposition 13 Using Parcel Level Data\nPeter Amerkhanian, Max Zhang, and James Hawkins (2023), Berkeley Institute for Young Americans Report.\nSummary: We utilize 12 million property records in California and a fixed-effects model to estimate the discount effect of Prop 13 across property types.\nTools: tidyverse, marginaleffects. Methods: fixed-effects.\n\nSupporting Whole Families: SparkPoint® Community Schools\nPeter Amerkhanian and Ena Yasuhara Li (2021), Report for United Way Bay Area.\nSummary: I utilize client outcome data to evaluate United Way Bay Area’s Community Schools program. I support quantitative analysis with qualitative data collected from interviews with clients and program staff.\nTools: pandas. Methods: descriptive statistics."
  },
  {
    "objectID": "projects.html#professional-reports-publications",
    "href": "projects.html#professional-reports-publications",
    "title": "Projects",
    "section": "",
    "text": "The following are some of my formal, public-facing projects:\nA Tale of Two Cities: Alameda and Alameda Point\nPeter Amerkhanian (2023), Report for The City of Alameda Department of Finance.\nSummary: I integrate various internal data sources to 1.) Create a historical narrative of re-development in the Alameda Point region and 2.) Project the benefits/costs of development alternatives in the region.\nTools: geopandas, sklearn. Methods: linear regression.\n\nA Reexamination of Proposition 13 Using Parcel Level Data\nPeter Amerkhanian, Max Zhang, and James Hawkins (2023), Berkeley Institute for Young Americans Report.\nSummary: We utilize 12 million property records in California and a fixed-effects model to estimate the discount effect of Prop 13 across property types.\nTools: tidyverse, marginaleffects. Methods: fixed-effects.\n\nSupporting Whole Families: SparkPoint® Community Schools\nPeter Amerkhanian and Ena Yasuhara Li (2021), Report for United Way Bay Area.\nSummary: I utilize client outcome data to evaluate United Way Bay Area’s Community Schools program. I support quantitative analysis with qualitative data collected from interviews with clients and program staff.\nTools: pandas. Methods: descriptive statistics."
  },
  {
    "objectID": "projects.html#academic-miscelanea",
    "href": "projects.html#academic-miscelanea",
    "title": "Projects",
    "section": "Academic Miscelanea",
    "text": "Academic Miscelanea\nThe following are some of my course papers from graduate school:\nSimulating School Desegregation in San Francisco\nPeter Amerkhanian (2022), Public Policy 275 Final Paper (A+).\nSummary: I use a synthetic dataset of San Francisco public high school students and spatial optimization methods to simulate the effects of various busing strategies for racial desegregation outcomes. Repo. \nTools: geopandas, networkx. Methods: dijkstra’s algorithm, entropy/dissimilarity statistics.\n\nMeasuring Differences in California Politician Agendas in Press Releases\nPeter Amerkhanian (2021), Information 254 Final Paper (A).\nSummary: I use a novel dataset of press releases issued by governors and mayors in California to 1.) Develop a regression model to identify press release authorship, 2.) Cluster press releases by topic, and 3.) Estimate the political similarities between mayors and governors.\nTools: gensim, sklearn. Methods: logistic regression, latent dirichlet allocation."
  },
  {
    "objectID": "posts/rent-vs-buy/index.html",
    "href": "posts/rent-vs-buy/index.html",
    "title": "Buy vs. Rent, A Financial Modeling Workflow in Python",
    "section": "",
    "text": "This post goes through the following exercises:\n\nUse numpy-financial to build a loan amortization calculator for a home mortgage\n\nUse said table as well as simulated home and stock equity returns over time to compare year-to-year wealth resulting from the following strategies:\n\nbuying a residential living space\n\nrenting one instead and investing the dollar amount that would have been your down-payment\n\n\n\n\nAt one point in time, numpy, the popular Python numerical analysis library, included 10 specialized functions for financial analysis. Given their specific nature, they were eventually removed from numpy, I think in 2019 (learn about why that is here) and are now available in the separate library, numpy-financial. The library still seems focused on the same 10 core functions, which handle tasks like calculating loan payment amounts given some inputs, and applied financial economics tasks like calculating time value of money. Cool… Anyways, I’ll use it to create an amortization schedule for a mortgage.\n\n\n\nI built this notebook in a Google Colab instance, which seems to include most major Python libraries (more info).\nYou’ll probably have to download numpy-financial (it’s not included in Anaconda as far as I know), which you can accomplish within any notebook-like environment using the following command:\n\n!pip install numpy-financial\n\nYou’ll want to load the usual suspects - pandas, numpy, seaborn, matplotlib. I also run from datetime import datetime since we will be working with ranges of dates, and I run sns.set_style() to get my seaborn plots looking a bit more aesthetically pleasing - read more on themes here.\n\nimport pandas as pd\nimport numpy as np\nimport numpy_financial as npf\nfrom datetime import datetime\n\nimport seaborn as sns\n# set seaborn style\nsns.set_style(\"white\")\n\nimport matplotlib.pyplot as plt\n# Set Matplotlib font size\nplt.rcParams.update({'font.size': 14})"
  },
  {
    "objectID": "posts/rent-vs-buy/index.html#summary",
    "href": "posts/rent-vs-buy/index.html#summary",
    "title": "Buy vs. Rent, A Financial Modeling Workflow in Python",
    "section": "",
    "text": "This post goes through the following exercises:\n\nUse numpy-financial to build a loan amortization calculator for a home mortgage\n\nUse said table as well as simulated home and stock equity returns over time to compare year-to-year wealth resulting from the following strategies:\n\nbuying a residential living space\n\nrenting one instead and investing the dollar amount that would have been your down-payment\n\n\n\n\nAt one point in time, numpy, the popular Python numerical analysis library, included 10 specialized functions for financial analysis. Given their specific nature, they were eventually removed from numpy, I think in 2019 (learn about why that is here) and are now available in the separate library, numpy-financial. The library still seems focused on the same 10 core functions, which handle tasks like calculating loan payment amounts given some inputs, and applied financial economics tasks like calculating time value of money. Cool… Anyways, I’ll use it to create an amortization schedule for a mortgage.\n\n\n\nI built this notebook in a Google Colab instance, which seems to include most major Python libraries (more info).\nYou’ll probably have to download numpy-financial (it’s not included in Anaconda as far as I know), which you can accomplish within any notebook-like environment using the following command:\n\n!pip install numpy-financial\n\nYou’ll want to load the usual suspects - pandas, numpy, seaborn, matplotlib. I also run from datetime import datetime since we will be working with ranges of dates, and I run sns.set_style() to get my seaborn plots looking a bit more aesthetically pleasing - read more on themes here.\n\nimport pandas as pd\nimport numpy as np\nimport numpy_financial as npf\nfrom datetime import datetime\n\nimport seaborn as sns\n# set seaborn style\nsns.set_style(\"white\")\n\nimport matplotlib.pyplot as plt\n# Set Matplotlib font size\nplt.rcParams.update({'font.size': 14})"
  },
  {
    "objectID": "posts/rent-vs-buy/index.html#problem-setup",
    "href": "posts/rent-vs-buy/index.html#problem-setup",
    "title": "Buy vs. Rent, A Financial Modeling Workflow in Python",
    "section": "Problem Setup",
    "text": "Problem Setup\n\nDefinining Constants\nI’ll run this as a comparison between buying an apartment that costs $700,000 with a 20% downpayment, versus renting a home for $2,600 a month. This is meant to approximate buying versus renting a two-bed one-bath apartment.\nBuying fees are defined at 4%, the homeowners association fees are defined as $700 monthly.\n\n# Buying Constants\ninterest_rate = 0.065\ncost = 700000\nhoa = 700\ndown_payment = cost * .2\nprincipal = cost - down_payment\nbuying_fees = principal*.04\n\n# Renting Constants\nrent = 2600\n\nnpf.pmt() can be used to generate a monthly mortgage payment given those buying constants:\n\nnpf.pmt(interest_rate/12, 12*30, principal)\n\n-3539.580931560606\n\n\nalternatively, we can use npf.ppt() to see how much of the payment goes towards the principal, and use npf.ipmt() to see how much goes towards interest (see below for applications of those functions).\n\n\nDefining Random Variables\nI’ll make the simplifying assumption that both “annual home appreciation” and “annual stock appreciation” are generated from normal distributions. This is a kind of strong assumption, but one that seems to be routinely made at least with regards to stock market returns, even if there might be better distribution choices out there (more here).\nHere’s a look at how we’ll draw from a normal distribution. Given an average annual return, \\(\\mu = 0.0572\\) (\\(\\mu\\), or, mu, is a common variable name for average) and a standard deviation \\(\\sigma = 0.1042\\) (\\(\\sigma\\), or, sigma, is the common variable name for standard deviation), we can draw one sample from a normal distribution as follows:\n\n# Set a random seed for stability of results\nnp.random.seed(30)\n\nmean = .0572\nstandard_deviation = .1042\nsamples = 1\n\n# Draw the sample\nnp.random.normal(mean, standard_deviation, samples)\n\narray([-0.07451429])\n\n\nWe now simulate market returns for every month by supplying mean and standard deviation values for both home and stock market appreciation and drawing 360 samples (360 months in 30 years). For simplicity, we’ll just use world-wide aggregate values from “The Rate of Return on Everything, 1870-2015”.\n\nmu_stock = .1081\nsigma_stock = .2267\n\nmu_home = .0572\nsigma_home = .1042\n\nGiven that stock and home appreciation is probably correlated, I’ll have ti sample from a bivariate normal distribution using numpy.random.Generator.multivariate_normal - documentation here, rather than the univariate distribution draw shown above. I am going to assume a correlation coefficient, \\(\\rho_{stock,home}\\) of 0.5 - a fairly clear correlation.\nIn order to use that numpy function, I’ll need to translate my correlation statistic into a covariance statistic, and I’ll use the following formula (source):\n\\[ \\begin{align*}\ncov_{stock,home} &= \\rho_{stock,home} \\times \\sigma_{stock} \\sigma_{home} \\\\\\\ncov_{stock,home} &= 0.5 \\times .2267 \\times .1042 \\end{align*}\n\\]\nI calculate covariance and confirm that the covariance and correlations match up below:\n\ncov = 0.5 * sigma_stock * sigma_home\nprint(\"Covariance:\", cov)\nprint(\"Back to correlation:\", cov / (sigma_stock * sigma_home))\n\nCovariance: 0.01181107\nBack to correlation: 0.5\n\n\nNow that I have the covariance, I’ll be able to sample from a bivariate normal distribution of the form shown below (source).\n\\[\n\\begin{pmatrix} Stock \\\\\\\\ Home\\end{pmatrix} \\sim \\mathcal{N} \\left[ \\begin{pmatrix} \\mu_{s} \\\\\\ \\mu_{h}\\end{pmatrix}, \\begin{pmatrix} \\sigma_{s}^2 & cov_{s,h} \\\\\\ cov_{s,h} & \\sigma_{h}^2\\end{pmatrix} \\right]\n\\]\nNote, \\(s\\) is shorthand for stock and \\(h\\) is shorthand for home.\nNow I’ll code that in Python and confirm that the means and standard deviations of our samples match what we expect:\n\ncov_matrix = np.array([[sigma_stock**2, cov],\n              [cov, sigma_home**2]])\n\nreturns_df = pd.DataFrame(np.random\n                          .default_rng(30)\n                          .multivariate_normal([mu_stock, mu_home],\n                                               cov_matrix,\n                                               360),\n                          columns=[\"Stock_Appreciation\", \"Home_Appreciation\"])\nprint(\"Means:\", returns_df.mean(axis=0).values)\nprint(\"Std. Devs:\", returns_df.std(axis=0).values)\n\nreturns_df = (returns_df / 12)\n\nMeans: [0.10764063 0.05970695]\nStd. Devs: [0.22544095 0.10543034]\n\n\nPlotting the simulated values, we can see that stock market returns are typically higher than home value appreciation.\n\nreturns_df.cumsum().plot(figsize=(9,5))\nplt.xlabel(\"Months\")\nplt.ylabel(\"Money Multiplier\")\nplt.title(\"Simulated Home/Stock Returns\")\nsns.despine();\n\n\n\n\n\n\n\n\n\nhome_performance = returns_df.cumsum()['Home_Appreciation'] + 1\nstock_performance = returns_df.cumsum()['Stock_Appreciation'] + 1\n\nNow we can define two spread-sheet-like dataframes: - one that shows a mortgage amortization schedule for if you were to buy the $600,000 home, along with the home’s appreciation over time. - one that shows a table of rent payments and the stock market growth of what would have been your down payment (you can invest the down payment since you didn’t end up purchasing a house)."
  },
  {
    "objectID": "posts/rent-vs-buy/index.html#scenarios",
    "href": "posts/rent-vs-buy/index.html#scenarios",
    "title": "Buy vs. Rent, A Financial Modeling Workflow in Python",
    "section": "Scenarios",
    "text": "Scenarios\n\nOwnership Table\n\n# Buying Table\ndf_own = pd.DataFrame()\ndf_own[\"Period\"] =  pd.Series(range(12*30)) + 1\ndf_own[\"Date\"] = pd.date_range(start=datetime.today(),\n                           periods=12*30,\n                           freq='MS',\n                           name=\"Date\")\ndf_own[\"Principal_Paid\"] = npf.ppmt(interest_rate/12,\n                                    df_own[\"Period\"],\n                                    12*30,\n                                    principal)\ndf_own[\"Interest_Paid\"] = npf.ipmt(interest_rate/12,\n                                   df_own[\"Period\"],\n                                   12*30,\n                                   principal)\ndf_own[\"HOA_Paid\"] = hoa\ndf_own[\"HOA_Paid\"] = df_own[\"HOA_Paid\"].cumsum()\ndf_own[\"Balance_Remaining\"] = principal + df_own[\"Principal_Paid\"].cumsum()\ndf_own[\"Home_Value\"] = round(cost * home_performance, 2)\ndf_own[\"PropTax_Paid\"] = (df_own[\"Period\"]\n                          .apply(lambda x:\n                                 (cost * 1.02**((x-1)/12) * 0.01)\n                                 if (x-1) in list(range(0, 12*1000, 12))\n                                 else 0)\n                          .cumsum())\ndf_own[\"Sale_Fees\"] = df_own[\"Home_Value\"] * .07\ndf_own[\"Own_Profit\"] = (df_own[\"Home_Value\"] -\n                              df_own[\"HOA_Paid\"] -\n                              df_own[\"Balance_Remaining\"] -\n                              (buying_fees + df_own[\"Sale_Fees\"]) -\n                              df_own[\"PropTax_Paid\"])\ndf_own = round(df_own, 2)\n\nNote this code, which is a bit of a monster:\n\ndf_own[\"PropTax_Paid\"] = (df_own[\"Period\"]\n                          .apply(lambda x:\n                                 (cost * 1.02**((x-1)/12) * 0.01)\n                                 if (x-1) in list(range(0, 12*1000, 12))\n                                 else 0)\n                          .cumsum())\n\nWhat is happening here is a calculation of property assessed value and property tax according to California’s property assessment/tax regime (more here). We’ll look at this in two pieces, first, assessed value. In California, once you purchase a property, its assessed value is set at the purchase price, then increases annually by the lower of 2% or the rate of inflation according to the California Consumer Price Index (CCPI). You could write out an equation for this as follows:\n\\[\n\\begin{align*}\nAnnualFactor_y =\n\\begin{cases}\n        1 + CCPI_y, & \\text{if } CCPI_y &lt; 0.02 \\\\\\\n        1.02, & \\text{otherwise}\n\\end{cases}\n\\end{align*}\n\\]\n\\(AnnualFactor\\) is the amount that the assessed value of a home will appreciate (expressed as a multiplier) in a given year, \\(y\\). We define \\(y^*\\) as the year of initial purchase and \\(PurchasePrice\\) as the amount that the home was purchased for. Given that, \\(AssessedValue\\) is defined as follows:\n\\[ \\begin{align*}\nAssessedValue_y =\n    \\begin{cases}\n        PurchasePrice, & \\text{if } y = y^* \\\\\n        AssessedValue_{y-1} \\times AnnualFactor_y, & \\text{otherwise }\n    \\end{cases}\n\\end{align*}\n\\]\nIn our code, we will simplify this calculation by excluding the CCPI and just always using 1.02 as our annual factor. Therefore, we get:\n\\[\n  AssessedValue_y = PurchasePrice \\times 1.02^y\n\\]\nand once we factor in taxes (1%), we get:\n\\[\n  PropertyTax_y = 0.01(PurchasePrice \\times 1.02^y)\n\\]\nand finally we look at the the cumulative total property tax you’ve paid in a given year \\(y\\), which is df_own[\"PropTax_Paid\"]:\n\\[\n\\begin{equation*}\n  PropertyTaxPaid_y = \\sum_{y=1}^{30} 0.01(PurchasePrice \\times 1.02^y)\n\\end{equation*}\n\\]\nThere’s some elements added to the code to work between years and months, but that equation is the gist of it.\nWe end up with the following table for property ownership:\n\ndf_own\n\n\n\n\n\n\n\n\n\nPeriod\nDate\nPrincipal_Paid\nInterest_Paid\nHOA_Paid\nBalance_Remaining\nHome_Value\nPropTax_Paid\nSale_Fees\nOwn_Profit\n\n\n\n\n0\n1\n2024-04-01 16:50:37.077628\n-506.25\n-3033.33\n700\n559493.75\n701405.73\n7000.000000\n49098.40\n62713.58\n\n\n1\n2\n2024-05-01 16:50:37.077628\n-508.99\n-3030.59\n1400\n558984.76\n707143.89\n7000.000000\n49500.07\n67859.06\n\n\n2\n3\n2024-06-01 16:50:37.077628\n-511.75\n-3027.83\n2100\n558473.02\n723148.91\n7000.000000\n50620.42\n82555.47\n\n\n3\n4\n2024-07-01 16:50:37.077628\n-514.52\n-3025.06\n2800\n557958.50\n736621.91\n7000.000000\n51563.53\n94899.88\n\n\n4\n5\n2024-08-01 16:50:37.077628\n-517.31\n-3022.28\n3500\n557441.19\n744559.68\n7000.000000\n52119.18\n102099.31\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n355\n356\n2053-11-01 16:50:37.077628\n-3445.26\n-94.33\n249200\n13968.65\n1937424.41\n283976.554436\n135619.71\n1232259.49\n\n\n356\n357\n2053-12-01 16:50:37.077628\n-3463.92\n-75.66\n249900\n10504.74\n1940547.12\n283976.554436\n135838.30\n1237927.53\n\n\n357\n358\n2054-01-01 16:50:37.077628\n-3482.68\n-56.90\n250600\n7022.06\n1944357.20\n283976.554436\n136105.00\n1244253.59\n\n\n358\n359\n2054-02-01 16:50:37.077628\n-3501.54\n-38.04\n251300\n3520.51\n1949518.54\n283976.554436\n136466.30\n1251855.18\n\n\n359\n360\n2054-03-01 16:50:37.077628\n-3520.51\n-19.07\n252000\n-0.00\n1953845.89\n283976.554436\n136769.21\n1258700.12\n\n\n\n\n360 rows × 10 columns\n\n\n\n\n\n\nRental Table\nThis one is a but more simple, only examining the total rent you’ve paid in a given month and simulated stock returns at that point.\n\n# Rental Table\ndf_rent = pd.DataFrame()\ndf_rent[\"Period\"] =  pd.Series(range(12*30)) + 1\ndf_rent[\"Date\"] = pd.date_range(start=datetime.today(),\n                           periods=12*30,\n                           freq='MS',\n                           name=\"Date\")\ndf_rent[\"DownPayment_Invested\"] =  stock_performance * down_payment\ndf_rent[\"Rent_Paid\"] = rent\ndf_rent[\"Rent_Paid\"] = df_rent[\"Rent_Paid\"].cumsum()\ndf_rent[\"Rent_Profit\"] = df_rent[\"DownPayment_Invested\"] - df_rent[\"Rent_Paid\"]\ndf_rent = round(df_rent, 2)\n\n\ndf_rent\n\n\n\n\n\n\n\n\n\nPeriod\nDate\nDownPayment_Invested\nRent_Paid\nRent_Profit\n\n\n\n\n0\n1\n2024-04-01 16:50:37.260140\n136919.68\n2600\n134319.68\n\n\n1\n2\n2024-05-01 16:50:37.260140\n140789.47\n5200\n135589.47\n\n\n2\n3\n2024-06-01 16:50:37.260140\n142175.79\n7800\n134375.79\n\n\n3\n4\n2024-07-01 16:50:37.260140\n145552.44\n10400\n135152.44\n\n\n4\n5\n2024-08-01 16:50:37.260140\n146217.26\n13000\n133217.26\n\n\n...\n...\n...\n...\n...\n...\n\n\n355\n356\n2053-11-01 16:50:37.260140\n594635.22\n925600\n-330964.78\n\n\n356\n357\n2053-12-01 16:50:37.260140\n591339.68\n928200\n-336860.32\n\n\n357\n358\n2054-01-01 16:50:37.260140\n589319.15\n930800\n-341480.85\n\n\n358\n359\n2054-02-01 16:50:37.260140\n588221.21\n933400\n-345178.79\n\n\n359\n360\n2054-03-01 16:50:37.260140\n592090.64\n936000\n-343909.36\n\n\n\n\n360 rows × 5 columns"
  },
  {
    "objectID": "posts/rent-vs-buy/index.html#results",
    "href": "posts/rent-vs-buy/index.html#results",
    "title": "Buy vs. Rent, A Financial Modeling Workflow in Python",
    "section": "Results",
    "text": "Results\nAt this point, I’ll merge the ownership and rental tables and plot out what happened in this simulation\n\nmerged = pd.merge(df_own, df_rent, on=\"Period\")\nmerged = merged.melt(value_vars = [\"Rent_Profit\", \"Own_Profit\"], id_vars='Period')\n\n\nplt.figure(figsize=(14, 6))\nplt.title(\"Wealth Outcomes for Owning vs. Renting a 2b1br Apt\")\nsns.lineplot(data=merged, x=\"Period\", y=\"value\", hue=\"variable\")\nfor x in range(0, 350, 12):\n    if x == 0:\n        plt.axvline(x, color=\"grey\", linestyle=\":\", alpha=1, label=\"Year\")\n    else:\n        plt.axvline(x, color=\"grey\", linestyle=\":\", alpha=0.7)\n    plt.text(x+1, -100000, str(int(x/12)), alpha=0.8)\nplt.axhline(0, color=\"red\", linestyle=\"--\", alpha=0.5, label=\"Zero\")\nplt.legend()\nsns.despine()\n\n\n\n\n\n\n\n\nWe can quickly see that ownership will clearly build more wealth in the medium and long run:\n\nyears = 5\nprint(f\"Owner after {years} years:\", df_own.loc[12*years-1, \"Own_Profit\"])\nprint(f\"Renter after {years} years:\", df_rent.loc[12*years-1, \"Rent_Profit\"])\n\nOwner after 5 years: 215177.81\nRenter after 5 years: 40500.11\n\n\nHowever, we can see that, in the unlikely case that the home is sold within the first year or so, it’s the renter that has more wealth, likely due to the owner contending with buying/selling fees:\n\nyears = 1\nprint(f\"Owner after {years} years:\", df_own.loc[12*years-1, \"Own_Profit\"])\nprint(f\"Renter after {years} years:\", df_rent.loc[12*years-1, \"Rent_Profit\"])\n\nOwner after 1 years: 114027.92\nRenter after 1 years: 115865.99\n\n\nA possible takeaway here is that, as long as you can be confident you’ll be able to hold onto the house for more than a year, it’s probably better to purchase it. Uncertainty estimates would be useful here, and could be obtained by running the simulation under a wide variety of randomly generated market conditions."
  },
  {
    "objectID": "posts/overfitting/index.html",
    "href": "posts/overfitting/index.html",
    "title": "Train-Test Split",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\n\npd.set_option(\"display.float_format\", \"{:.2f}\".format)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.tree import DecisionTreeRegressor\n\n\ndef model_data_lm(degree, df):\n    X = df[[\"Education\"]].values\n    y = df[\"Income\"].values\n    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n    model.fit(X, y)\n    rmse = mean_squared_error(y, model.predict(X)) ** 0.5\n    r2 = r2_score(y, model.predict(X))\n    X_test = np.linspace(X.min() - 1.5 * X.std(), X.max() + 1.5 * X.std(), 100).reshape(\n        -1, 1\n    )\n    y_pred = model.predict(X_test)\n\n    return X_test, y_pred, rmse, r2, model"
  },
  {
    "objectID": "posts/overfitting/index.html#some-notes-on-modeling-data-and-resampling-methods",
    "href": "posts/overfitting/index.html#some-notes-on-modeling-data-and-resampling-methods",
    "title": "Train-Test Split",
    "section": "Some Notes on Modeling Data and Resampling Methods",
    "text": "Some Notes on Modeling Data and Resampling Methods\nThe following is an exercise with synthetic data intended to memorialize some thoughts on models and resampling methods. I won’t reference a lot of specific books here, but generally this post is influenced by James et al. (2013) and Hastie, Tibshirani, and Friedman (2016).\nTo begin, say we are examining the relationship between income and years of education.\n\ndf = pd.DataFrame()\nnp.random.seed(46)\nn = 200\neducation_dgp = lambda n: np.random.lognormal(1.2, 0.6, n)\neducation = education_dgp(n)\ndf[\"Education\"] = education\n\nLet’s say that income is a function of years of education:\n\nnp.random.seed(49)\ndf[\"Income\"] = 33000 + 6000 * education\n\nThe relationship is governed by the deterministic function, which is a data generating process (DGP): \\[\nincome = 33000 + 6000 \\times education\n\\]\n\n\nCode\ng = sns.JointGrid(data=df, x=\"Education\", y=\"Income\", marginal_ticks=False, height=5)\ng.plot_joint(sns.scatterplot, alpha=0.6, marker=\"o\")\ng.plot_marginals(sns.kdeplot, fill=True, clip=(0, np.inf))\ng.fig.suptitle(f\"Income ~ Education\", y=1.05)\ng.ax_joint.grid(alpha=0.1)\n# Formatting the sides\ng.ax_joint.yaxis.set_major_formatter(\"${x:,.0f}\")\ng.ax_marg_x.tick_params(left=False, bottom=False)\ng.ax_marg_y.tick_params(left=False, bottom=False)\ng.ax_marg_y.set(xticklabels=[]);\n\n\n\n\n\n\n\n\n\nModeling this relationship is trivial. Given that it is a deterministic, linear function, we can use data to solve a linear system of equations and find the exact parameters of the relationship:\n\n\nCode\nfig, ax = plt.subplots(1, 2, sharey=True)\nsns.scatterplot(x=\"Education\", y=\"Income\", data=df, alpha=0.6, marker=\"o\", ax=ax[0])\nfig.suptitle(\"Income ~ Education\")\nax[0].grid(alpha=0.1)\nax[0].yaxis.set_major_formatter(\"${x:,.0f}\")\nsns.despine(ax=ax[0])\nax[0].set(\n    xlim=(0, df[\"Education\"].max() + df[\"Education\"].std()),\n    ylim=(0, df[\"Income\"].max() + df[\"Income\"].std()),\n)\nax[1].set(\n    xlim=(0, df[\"Education\"].max() + df[\"Education\"].std()),\n    ylim=(0, df[\"Income\"].max() + df[\"Income\"].std()),\n)\nX_test, y_pred, _, r2, _ = model_data_lm(1, df)\nax[1].grid(alpha=0.1)\nsns.scatterplot(x=\"Education\", y=\"Income\", data=df, alpha=0.3, marker=\"o\", ax=ax[1])\nax[1].plot(\n    X_test,\n    y_pred,\n    color=\"darkred\",\n    alpha=0.7,\n    label=r\"Model $\\rightarrow R^2=$\" + str(round(r2, 2)),\n)\nax[1].legend()\nsns.despine(ax=ax[1]);\n\n\n\n\n\n\n\n\n\n\\(R^2\\) is the percentage of variance in the outcome, income, that our model can describe (in this case 100%, a perfect fit). We’ve created a perfect model of the DGP."
  },
  {
    "objectID": "posts/overfitting/index.html#modeling-a-non-deterministic-process",
    "href": "posts/overfitting/index.html#modeling-a-non-deterministic-process",
    "title": "Train-Test Split",
    "section": "Modeling a Non-Deterministic Process",
    "text": "Modeling a Non-Deterministic Process\nIn policy analysis settings, it’s rare that we would encounter deterministic processes that we have any interest in modeling. Instead, we typically encounter processes with some stochastic element. For example, the following is a new DGP, where income is a linear function of years of education, but with an added random variable, representing noise.\n\nnoise_std = 9000\nDGP = lambda x: 33000 + 6000 * x + np.random.normal(0, noise_std, size=len(x))\n\n\\[\nincome = 33000 + 6000 \\times education + \\mathcal{N}(0, 9000)\n\\]\n\ndf[\"Income\"] = DGP(education)\n\n\n\nCode\ng = sns.JointGrid(data=df, x=\"Education\", y=\"Income\", marginal_ticks=False, height=5)\ng.plot_joint(sns.scatterplot, alpha=0.6, marker=\"o\")\ng.plot_marginals(sns.kdeplot, fill=True, clip=(0, np.inf))\ng.fig.suptitle(f\"Income ~ Education + $\\mathcal{{N}}(0, {noise_std})$\", y=1.05)\ng.ax_joint.grid(alpha=0.1)\n\n# Formatting the sides\ng.ax_joint.yaxis.set_major_formatter(\"${x:,.0f}\")\ng.ax_marg_x.tick_params(left=False, bottom=False)\ng.ax_marg_y.tick_params(left=False, bottom=False)\ng.ax_marg_y.set(xticklabels=[]);\n\n\n\n\n\n\n\n\n\nModeling this process is no longer trivial. Rather than solve a simple system of equations algebraically, we must use an estimation method to find a “best” model. Estimation involves choices, and finding the best model for this data is more complex than it may seem at first glance.\n\nOverfitting\nWe will model this DGP using a linear regression fit via the ordinary least squares algorithm. We face a number of choices in using linear regression – for example, we can freely use polynomial transformations to make our model flexible to non-linear relationships. Here we’ll examine the behavior of a polynomial regression by setting up an infinite series as follows: \\[\ny_p=\\sum_{p=0}^{\\infty} \\beta_p x^p\n\\] As this series expands, it represents a polynomial regression of degree \\(p\\). When we expand, we can examine how this model performs as the degree increases.\n\n\nCode\ny_max = 3\nx_max = 3\nfig, ax = plt.subplots(x_max, y_max, sharey=True, sharex=True, figsize=(10, 6))\ndegree = 0\nfor i in range(x_max):\n    for j in range(y_max):\n        X_test, y_pred, _, r2, _ = model_data_lm(degree, df)\n        ax[i, j].set_ylim(\n            df[\"Income\"].min() - df[\"Income\"].std(),\n            df[\"Income\"].max() + df[\"Income\"].std(),\n        )\n        ax[i, j].grid(alpha=0.1)\n        sns.despine(ax=ax[i, j])\n        ax[i, j].plot(df[\"Education\"], df[\"Income\"], \".\", alpha=0.4, color=\"tab:blue\")\n        ax[i, j].plot(X_test, y_pred, color=\"darkred\", alpha=0.7)\n        ax[i, j].set(yticklabels=[], xticklabels=[])\n        ax[i, j].tick_params(left=False, bottom=False)\n        ax[i, j].set_title(\n            r\"$y_{{p={}}}=\\sum_{{p=0}}^{} \\beta_p x^p \\rightarrow$ \".format(\n                degree, degree\n            )\n            + r\"$R^2=$\"\n            + f\"{round(r2, 4)}\",\n            size=12,\n        )\n        degree += 1\n\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nRegardless of what you think of the shape of these curves, it’s clear that as the series expands and \\(p\\) increases, we see improving model accuracy, \\(R^2\\). We can expand this series until we find a seemingly “optimal” model.\n\nAn “Optimal” Model\nWe expand the series, or, in more typical language, we conduct a “grid-search,” to find the optimal model as defined by the maximum \\(R^2\\) score:\n\nr2s = []\nmax_root = 25\nroots = range(max_root)\nfor degree in roots:\n    _, _, _, r2, _ = model_data_lm(degree, df)\n    r2s.append(r2)\nr2s = np.array(r2s)\n\n\n\nCode\nfig, ax = plt.subplots(1, 2)\nax[0].plot(roots[1:], r2s[1:], \".-\", alpha=0.8)\nax[0].set_title(\"$R^2$ ~ $d$\")\nax[0].plot(\n    np.argmax(r2s),\n    np.max(r2s),\n    \"o\",\n    color=\"tab:red\",\n    alpha=0.6,\n    label=f\"$\\max \\; (R^2)=${round(np.max(r2s), 3)}\\n\"\n    + r\"$ \\underset{d}{\\arg\\max} \\; (R^2)=$\"\n    + f\"{np.argmax(r2s)}\",\n)\nax[0].set(xlabel=\"Polynomial Degree\", ylabel=\"$R^2$\")\nax[0].legend()\nax[0].grid(alpha=0.1)\nsns.despine(ax=ax[0])\n\ndegree = np.argmax(r2s)\nX_test, y_pred, _, r2, optimal_model = model_data_lm(degree, df)\nax[1].set_ylabel(r\"$\\downarrow$\", size=25)\nax[1].set_ylim(\n    df[\"Income\"].min() - df[\"Income\"].std(), df[\"Income\"].max() + df[\"Income\"].std()\n)\nax[1].grid(alpha=0.1)\nsns.despine(ax=ax[1])\nax[1].plot(df[\"Education\"], df[\"Income\"], \".\", alpha=0.4, color=\"tab:blue\")\nax[1].plot(X_test, y_pred, color=\"darkred\", alpha=0.7)\nax[1].set_xlabel('\"Optimal\" Model')\nax[1].tick_params(left=False, bottom=False)\nax[1].set_title(\n    r\"$y^*_{{p={}}}=\\sum_{{p=0}}^{{{}}} \\beta_p x^p$ \".format(degree, degree)\n)\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nWe see that the accuracy-maximizing model, \\(y^*\\), is a very high degree polynomial. Our simple, empirical investigation leads us to conclude that this is the “best” model.\nHowever, it’s clear that the model is overly complex and likely problematic given these visuals, where there are extreme swings in predictions. It would greatly benefit us to have a clear quantity that represents why this model may be problematic.\n\n\n\nOut of sample performance, or, what happens when we collect new data?\nOne simple way to further evaluate this model is to collect new data (from outside of our original sample) and then evaluate whether the model can describe variation in that new data. Given that we are working with synthetic data, it’s straightforward to simulate new observations from the underlying DGP, and thus “collect” new data. In the following plot, the right-hand pane is a fresh sample of individuals with observed education and income values:\n\neducation = education_dgp(1000)\ncorrect_income = DGP(education)\n\n\n\nCode\nom_residuals = np.abs(optimal_model.predict(education.reshape(-1, 1)) - correct_income)\nnew_data_df = pd.DataFrame()\nnew_data_df[\"Education\"] = education\nnew_data_df[\"Residuals\"] = om_residuals\nnew_data_df[\"Income\"] = correct_income\nnew_data_df[\"Data\"] = \"New\"\nnew_data_df = pd.concat(\n    [\n        df.assign(\n            Residuals=np.abs(\n                optimal_model.predict(df[[\"Education\"]].values) - df[\"Income\"]\n            )\n        ).assign(Data=\"Original\"),\n        new_data_df,\n    ],\n    axis=0,\n)\nax = sns.relplot(\n    height=4,\n    data=new_data_df,\n    x=\"Education\",\n    y=\"Income\",\n    col=\"Data\",\n    kind=\"scatter\",\n    legend=False,\n)\n\nax.axes[0][0].yaxis.set_major_formatter(\"${x:,.0f}\")\n# ax.axes[0][0].plot(X_test, y_pred, color='darkred', alpha=.7)\nax.axes[0][0].set_ylim(\n    df[\"Income\"].min() - df[\"Income\"].std(), df[\"Income\"].max() + df[\"Income\"].std()\n)\nax.axes[0][0].set_ylim(\n    df[\"Income\"].min() - df[\"Income\"].std(), df[\"Income\"].max() + df[\"Income\"].std()\n)\nax.tight_layout();\n\n\n\n\n\n\n\n\n\nHere we can see that, when exposed to a new sample from the same DGP, the model produces extreme error.\n\n\nCode\nnorm = (0, 1000000)\nnorm = plt.Normalize(*norm)\nsm = plt.cm.ScalarMappable(cmap=\"coolwarm\", norm=norm)\n\nax = sns.relplot(\n    height=4,\n    data=new_data_df,\n    x=\"Education\",\n    y=\"Income\",\n    col=\"Data\",\n    hue=\"Residuals\",\n    kind=\"scatter\",\n    palette=\"coolwarm\",\n    hue_norm=norm,\n    legend=False,\n)\nax.axes[0][0].yaxis.set_major_formatter(\"${x:,.0f}\")\nax.axes[0][1].figure.colorbar(sm, ax=ax.axes[0][1], format=\"${x:,.0f}\").set_label(\n    r\"$\\hat{y}-y$\"\n)\nax.axes[0][1].plot(X_test, y_pred, color=\"darkred\", alpha=0.7)\nax.axes[0][0].plot(X_test, y_pred, color=\"darkred\", alpha=0.7)\nax.axes[0][0].set_ylim(\n    df[\"Income\"].min() - df[\"Income\"].std(), df[\"Income\"].max() + df[\"Income\"].std()\n)\nax.axes[0][0].set_ylim(\n    df[\"Income\"].min() - df[\"Income\"].std(), df[\"Income\"].max() + df[\"Income\"].std()\n)\nax.tight_layout();\n\n\n\n\n\n\n\n\n\nWe quantify this error as follows:\n\n\nCode\nrmse = lambda y, y_hat: np.sqrt(np.mean((y_hat - y) ** 2))\noutput_1 = pd.DataFrame(\n    {\n        \"r2\": [\n            r2_score(correct_income, optimal_model.predict(education.reshape(-1, 1)))\n        ],\n        \"rmse\": [rmse(optimal_model.predict(education.reshape(-1, 1)), correct_income)],\n    },\n    index=[\"p=14\"],\n)\nround(output_1, 2)\n\n\n\n\n\n\n\n\n\n\nr2\nrmse\n\n\n\n\np=14\n-1975424250597210112.00\n26686561853746.36\n\n\n\n\n\n\n\n\nThis is clearly a bad model. What’s worrying is that it seemed like the best model when we simply examined model accuracy on one dataset. This raises the possibility that we might be duped into selecting models like this in practice.\nOut of curiosity, let’s see how an extremely simply model, a linear regression with \\(p=1\\) compares on this new data.\n\n\nCode\nX_test, y_pred, _, r2, sub_optimal_model = model_data_lm(1, df)\nnew_data_df.loc[new_data_df[\"Data\"] == \"New\", \"Degree 1 Residuals\"] = np.abs(\n    sub_optimal_model.predict(education.reshape(-1, 1)) - correct_income\n)\nnew_data_df.loc[new_data_df[\"Data\"] == \"Original\", \"Degree 1 Residuals\"] = np.abs(\n    sub_optimal_model.predict(df[[\"Education\"]].values) - df[\"Income\"]\n)\n\nsm = plt.cm.ScalarMappable(cmap=\"coolwarm\", norm=norm)\nax = sns.relplot(\n    height=4,\n    data=new_data_df,\n    x=\"Education\",\n    y=\"Income\",\n    col=\"Data\",\n    hue=\"Degree 1 Residuals\",\n    kind=\"scatter\",\n    palette=\"coolwarm\",\n    hue_norm=norm,\n    legend=False,\n)\n\nax.axes[0][0].yaxis.set_major_formatter(\"${x:,.0f}\")\nax.axes[0][1].figure.colorbar(sm, ax=ax.axes[0][1], format=\"${x:,.0f}\")\n\nax.axes[0][1].plot(X_test, y_pred, color=\"darkred\", alpha=0.7)\nax.axes[0][0].plot(X_test, y_pred, color=\"darkred\", alpha=0.7)\nax.axes[0][0].set_ylim(\n    df[\"Income\"].min() - df[\"Income\"].std(), df[\"Income\"].max() + df[\"Income\"].std()\n)\nax.axes[0][0].set_ylim(\n    df[\"Income\"].min() - df[\"Income\"].std(), df[\"Income\"].max() + df[\"Income\"].std()\n)\nax.tight_layout();\n\n\n\n\n\n\n\n\n\nThis is a very good fit, even though our previous empirical fitting process told us that the high \\(p\\) model was optimal.\n\n\nCode\noutput_2 = pd.DataFrame(\n    {\n        \"r2\": [\n            r2_score(\n                correct_income, sub_optimal_model.predict(education.reshape(-1, 1))\n            )\n        ],\n        \"rmse\": [\n            rmse(sub_optimal_model.predict(education.reshape(-1, 1)), correct_income)\n        ],\n    },\n    index=[\"p=1\"],\n)\nround(output_2, 2)\n\n\n\n\n\n\n\n\n\n\nr2\nrmse\n\n\n\n\np=1\n0.77\n9139.36\n\n\n\n\n\n\n\n\nWe have the benefit of already knowing that \\(p=1\\) matches the functional form of the true DGP, but even if we didn’t know that in advance, out-of-sample performance seems like a better way of evaluating whether or not our model gets at the underlying DGP of the observations."
  },
  {
    "objectID": "posts/minima-3d/index.html",
    "href": "posts/minima-3d/index.html",
    "title": "A Minima Problem in 3D",
    "section": "",
    "text": "Code\nlibrary(plotly)\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/minima-3d/index.html#footnotes",
    "href": "posts/minima-3d/index.html#footnotes",
    "title": "A Minima Problem in 3D",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is beyond the scope of this post, but based on the inequality \\(0 \\leq x \\leq y \\rightarrow  x^2 \\leq y^2\\).↩︎"
  },
  {
    "objectID": "posts/linear-approximation-3d/index.html",
    "href": "posts/linear-approximation-3d/index.html",
    "title": "Linear Approximation in 3D",
    "section": "",
    "text": "Code\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import axes3d\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nThe following is a calculus problem that I think provides a good overview of linear approximation, a method for approximating a general function using a linear function. Common applications of linear approximation exist in optics, oscillation, and electric resistivity problems (“Linear Approximation” 2023). The following image from (Strang and Herman 2016, chap. 4.4) provides some good intuition of what this approximation looks like:"
  },
  {
    "objectID": "posts/linear-approximation-3d/index.html#the-question",
    "href": "posts/linear-approximation-3d/index.html#the-question",
    "title": "Linear Approximation in 3D",
    "section": "The Question",
    "text": "The Question\nUse a tangent plane to approximate the value of the following function at the point \\((2.1, -5.1)\\).\n\\[f(x,y) = \\sqrt{42-4x^2-y^2}\\]\nHere’s some code setup for the question:\n\nf = lambda x, y: np.sqrt(42-4*x**2 - y**2)\nrange_x = np.linspace(-8, 8, 500)\nrange_y = range_x.copy()\nX, Y = np.meshgrid(range_x, range_y)\nZ = f(X, Y)\n\n\nWhy do we need to approximate this?\nWe can’t just plug in and compute this output because the point (2.1, -5.1) is outside of the domain of this function – thus \\(f(2.1, -5.1)\\) does not exist.\n\nf(2.1, -5.1)\n\nnan\n\n\n\\[f(2.1, -5.1) \\approx \\sqrt{-1.65} = DNE\\]\n\n\nCode\ndef style_plot(ax, z=False):\n    ax.grid(alpha=.5)\n    ax.set(xlabel=\"$x$\", ylabel=\"$y$\")\n    if z:\n        ax.set(zlabel=\"$z$\")\n\nfig, ax = plt.subplots(figsize=(6, 4))\nax.contourf(X, Y, Z)\nax.scatter(x=2.2,\n           y=-5.1,\n           s=15,\n           color='tab:orange',\n           label=r\"$f(2.1, -5.1) = DNE$\")\nstyle_plot(ax)\nax.legend();\n\n\n\n\n\n\n\n\n\n\n\nDeveloping a linear approximation of the function\nTo approximate this quantity, we find a nearby point where the function is defined, and construct a tangent plane to extrapolate our quantity of interest. For a nearby point, I select \\(P_0 = (x_0=2, y_0=-5)\\), where the function still produces a real number output:\n\\[f(x_0, y_0) = f(2, -5) =\\sqrt{1} = 1\\]\n\n\nCode\nfig = plt.figure(figsize=(8, 3))\nax1 = plt.subplot(122)\nax2 = plt.subplot(121, projection='3d')\n# Countour plot\nax1.contourf(X, Y, Z)\nax1.scatter(2, -5, color='red', s=15, label=r\"$f(2, -5)$\")\nax1.grid(alpha=.5)\nax1.legend()\nstyle_plot(ax1)\n# 3d surface\nax2.plot_wireframe(X, Y, Z)\nstyle_plot(ax2, z=True)\nax2.set_box_aspect(None, zoom=0.9)\nax2.set_xlim(-5, 5)\nax2.set_ylim(-8, 8)\nax2.set_zlim(-5, 5)\nax2.scatter(2, -5, f(2, -5), color='red', s=15)\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nWe then derive a linear approximation of \\(f\\) at this point using the following equation:\n\\[\n\\begin{align}\n   L(x, y) = f\\left( {{x_0},{y_0}} \\right) + {f_x}\\left( {{x_0},{y_0}} \\right)\\left( {x - {x_0}} \\right) + {f_y}\\left( {{x_0},{y_0}} \\right)\\left( {y - {y_0}} \\right)\n\\end{align}\n\\tag{1}\\]\n\n[…] This equation […] represents the tangent plane to the surface defined by \\(z=f(x,y)\\) at the point \\((x_0,y_0)\\). The idea behind using a linear approximation is that, if there is a point \\((x_0,_0)\\) at which the precise value of \\(f(x,y)\\) is known, then for vales of \\((x,y)\\) reasonably close to \\((x_0,y_0)\\), the linear approximation (i.e., tangent plane) yields a value that is also reasonably close to the exact value of \\(f(x,y)\\).\n– (Strang and Herman 2016, chap. 4.4)\n\nRecall that \\(f_x\\) and \\(f_y\\) refer to components of the gradient vector, \\(\\nabla f\\). We calculate that gradient as follows: \\[  \n\\begin{align*}\n\\nabla f &= \\left&lt; f_x, f_y \\right&gt; \\\\\n\\nabla f &= \\left&lt; \\frac{df}{dx}, \\frac{df}{dy} \\right&gt; \\\\\n    \\nabla f &= \\left&lt; \\frac{-4x}{\\sqrt{42-4x^2-y^2}}, \\frac{-y}{\\sqrt{42-4x^2-y^2}} \\right&gt;\n\\end{align*}\n\\] Now we can plug in our point, \\(P_0\\):\n\\[\n\\begin{align*}\n\\nabla f(P_0) &= \\left&lt; {f_x}\\left( {{x_0},{y_0}} \\right), {f_y}\\left( {{x_0},{y_0}} \\right) \\right&gt; \\\\\n    \\nabla f(P_0) &= \\left&lt; \\frac{-4(2)}{1}, \\frac{-(-5)}{1} \\right&gt; = \\left&lt; -8, 5 \\right&gt;\n\\end{align*}\n\\] We now have all elements of the tangent plane/linear approximation, Equation 1, and we can simply plug-in and compute: \\[\n\\begin{align*}\n         L(x,y) &= f(2, -5) + -8 \\left( {x - 2} \\right) + 5\\left( y - (-5) \\right) \\\\\n         L(x,y) &= 1 + -8x + 16 + 5y + 25 \\\\\n         L(x,y) &= -8x + 5y + 42\n\\end{align*}\n\\]\n\nL = lambda x, y: -8*x + 5*y + 42\n\n\n\nCode\nfig = plt.figure(figsize=(8, 4))\nax1 = plt.subplot(121,)\nax2 = plt.subplot(122, projection='3d')\n# Countour plot\nax1.contour(X, Y, Z, label=r\"$f(x)$\", alpha=.6)\nax1.contour(X, Y, L(X, Y), levels= 25, alpha=.8, cmap=\"coolwarm\", label=r\"$L(x)$\")\nax1.scatter(2, -5, s=15, color='red', label=r\"$f(2, -5) = 1$\")\nax1.scatter(2.2, -5.1, s=15, color='tab:orange', label=f\"$L(2.1, -5.1) = {round(L(2.1, -5.1), 2)}$\")\nax1.grid(alpha=.5)\nax1.legend(loc=\"upper left\", framealpha=1)\nstyle_plot(ax1)\n# ax1.view_init(-130, -90, 0)\n# 3d surface\nZ = np.ma.masked_where(Z &lt;= 0, Z)\nX = np.ma.masked_where(Z &lt;= 0, X)\nY = np.ma.masked_where(Z &lt;= 0, Y)\nax2.contour3D(X, Y, Z, corner_mask=True)\nax2.plot_surface(X, Y, L(X, Y), alpha=1)\n# ax2.contour3D(X, Y, Z, 20, cmap='gray')\nstyle_plot(ax2, z=True)\nax2.set_xlim(-5, 5)\nax2.set_ylim(-8, 8)\nax2.set_zlim(-5, 5)\nax2.scatter(2, -5, f(2, -5), color='red', s=15)\nax2.scatter(2.2, -5.1, s=15, color='tab:orange', label=f\"$L(2.1, -5.1) = {round(L(2.1, -5.1), 2)}$\")\n\nax2.view_init(0, -133, 0)\nfig.suptitle(r\"$f(x, y)$ & $L(x, y)$\")\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nWith that linear approximation established, we can now estimate our original quantity of interest, \\(f(2.1, -5.1)\\). \\[f(2.1, -5.1) \\approx L(2.1, -5.1) = -8(2.1) + 5(-5.1) + 42 \\approx \\boxed{-0.3} \\]\n\nL(2.1, -5.1)\n\n-0.29999999999999716"
  },
  {
    "objectID": "posts/iterated-expectations/index.html",
    "href": "posts/iterated-expectations/index.html",
    "title": "Iterated Expectations",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nI recently came across a list of 10 theorems/proofs that you “need to know” if you do econometrics. These were compiled by Jeffrey Wooldridge, an economist and textbook author whose introductory textbook has been fundamental to my interest in econometrics. As an exercise, I’m working through these 10 items, compiling resources, textbook passages, and data exercises that I think can make them easier to understand. The first item I’m trying to write my notes on is the Law of Iterated Expectations, but I’ll be prefacing/augmenting the notes with some discussion of basic probability for completeness.\nMy core reference is Introduction to Probability, Second Edition By Joseph K. Blitzstein, Jessica Hwang.\nTo start, I’ll simulate some data.\n# Set a random seed for reproducability\nnp.random.seed(42)\n# Define the number of people in the dataset\nnum_people = 100\n# Generate random ages - X ~ Uniform(min, max)\nages = np.random.randint(71, 79, num_people)\n# Create DataFrame\ndata = {'Person_ID': range(1, num_people + 1), 'Age': ages}\npeople_df = pd.DataFrame(data).set_index(\"Person_ID\")\n\npeople_df.head()\n\n\n\n\n\n\n\n\n\nAge\n\n\nPerson_ID\n\n\n\n\n\n1\n77\n\n\n2\n74\n\n\n3\n75\n\n\n4\n77\n\n\n5\n73\nLet’s say that these data represent life spans, thus \\(\\text{age}_i\\) is an individual’s lifespan, e.g. \\(\\text{age}_2=\\)\npeople_df.loc[2]\n\nAge    74\nName: 2, dtype: int32"
  },
  {
    "objectID": "posts/iterated-expectations/index.html#expectation",
    "href": "posts/iterated-expectations/index.html#expectation",
    "title": "Iterated Expectations",
    "section": "Expectation",
    "text": "Expectation\nThe mean of a random variable, like age above, is also referred to as its “expected value,” denoted \\(E(\\text{age})\\).\n\npeople_df['Age'].mean()\n\n74.6\n\n\nThe mean above is specifically called an arithmetic mean, defined as follows:\n\\[ \\bar{x} = \\frac{1}{n} \\sum_i^n x_i\\]\n\n(1/len(people_df)) * people_df['Age'].sum()\n\n74.60000000000001\n\n\nBut the arithmetic mean is just a special case of the more general weighted mean:\n\\[\\begin{align*}\n\\text{weighted-mean}(x) &= \\sum_i^n x_i p_i \\\\\n\\end{align*}\\]\nWhere the weights, \\(p_1, p_2, ...,p_n\\) are non-negative numbers that sum to 1. We can see that the arithmetic mean is the specific case of the weighted mean where all weights are equal\n\\[\\begin{align*}\n\\text{If } [p_1=p_2=...=p_n] &\\text{ And } [\\sum_i^n p_i =1]\\\\\n\\text{weighted-mean}(x) &= \\sum_i^n x_i \\frac{1}{n}\\\\\n\\text{weighted-mean}(x) &= \\frac{1}{n} \\sum_i^n x_i = \\bar{x}\\\\\n\\end{align*}\\]\nWe use the more general weighted mean when we define expectation.\n\nthe expected value of \\(X\\) is a weighted average of the possible values that \\(X\\) can take on, weighted by their probabilities\n– Blitzstein and Hwang (2019)\n\nMore formally, given a random variable, \\(X\\), with distinct possible values, \\(x_1, x_2, ... x_n\\), the expected value \\(E(X)\\) is defined as:\n\\[\\begin{align*}\nE(X) = & x_1P(X = x_1) + \\\\\n&x_2P(X = x_2) +\\\\\n&... + x_nP(X = x_n) \\\\\n= &\\sum_{i=1}^n x_iP(X = x_i)\n\\end{align*}\\]\nNow we’ll demonstrate this formula on our data. It’s useful here to move from our individual-level dataset, where each row is a person, to the following, where each row is a lifespan, which the probability that an individual has that lifespan.\n\nprob_table = people_df['Age'].value_counts(normalize=True)\nprob_table = prob_table.sort_index()\nprob_table\n\n71    0.08\n72    0.13\n73    0.11\n74    0.19\n75    0.12\n76    0.13\n77    0.13\n78    0.11\nName: Age, dtype: float64\n\n\nAdapting the formula above to our data, we must solve the following:\n\\[\\begin{align*}\nE(\\text{Age}) = &\\text{Age}_1P(\\text{Age}=\\text{Age}_1) + \\\\\n&\\text{Age}_2P(\\text{Age}=\\text{Age}_2) + \\\\\n&... + \\text{Age}_nP(\\text{Age}=\\text{Age}_n) \\\\\n= &\\sum_{i=1}^n \\text{Age}_iP(\\text{Age}=\\text{Age}_i)\n\\end{align*}\\]\nWhich we can do transparently using a for-loop:\n\nsummation = 0\nfor i in range(len(prob_table)):\n  summation += prob_table.index[i] * prob_table.values[i]\nsummation\n\n74.60000000000001\n\n\nAs a quick aside – this can also be expressed as the dot product of two vectors, where the dot product is defined as follows:\n\\[\n\\begin{align*}\n\\vec{\\text{Age}}\\cdot P(\\vec{\\text{Age}}) = &\\text{Age}_1P(\\text{Age}=\\text{Age}_1) + \\\\\n&\\text{Age}_2P(\\text{Age}=\\text{Age}_2) + \\\\\n&... + \\text{Age}_3P(\\text{Age}=\\text{Age}_3)\n\\end{align*}\n\\]\n\nprob_table.index.values @ prob_table.values\n\n74.6\n\n\nThough we will stick to the summation notation paired with python for-loops for consistency"
  },
  {
    "objectID": "posts/iterated-expectations/index.html#conditional-expectation",
    "href": "posts/iterated-expectations/index.html#conditional-expectation",
    "title": "Iterated Expectations",
    "section": "Conditional Expectation",
    "text": "Conditional Expectation\nWe often have more than one variable available to us in an analysis. Below I simulate the variable gender:\n\nnp.random.seed(45)\npeople_df['Gender'] = np.random.choice(['Female', 'Male'], len(people_df))\npeople_df.head()\n\n\n\n\n\n\n\n\n\nAge\nGender\n\n\nPerson_ID\n\n\n\n\n\n\n1\n77\nMale\n\n\n2\n74\nFemale\n\n\n3\n75\nMale\n\n\n4\n77\nFemale\n\n\n5\n73\nFemale\n\n\n\n\n\n\n\n\nEach row in our dataset represents an individual person, and we now have access to both their gender and their life-span. It follows that we may be interested in how life-span varies across gender. In code, this entails a groupby operation, grouping on gender before calculting the mean age:\n\npeople_df.groupby('Gender')['Age'].mean()\n\nGender\nFemale    74.672727\nMale      74.511111\nName: Age, dtype: float64\n\n\nThe code in this case resembles the formal notation of a conditional expectation: \\(E(\\text{Age} \\mid \\text{Gender}=\\text{Gender}_j)\\), where each \\(\\text{Gender}=\\text{Gender}_j\\) is a distinct event.\nIf we are interested specifically in the mean life-span given the event that gender is equal to male (a roundabout way of saying the average life-span for males in the data), we could calculate the following\n\\(E(\\text{Age} \\mid \\text{Gender}=\\text{Male})\\)\n\npeople_df.groupby('Gender')['Age'].mean()['Male']\n\n74.5111111111111\n\n\nThese groupby operations in pandas obscure some of the conceptual stuff happening inside the conditional expectation, which we’ll delve deeper into now.\nSo what exactly is the conditional expectation, \\(E(X \\mid Y=y)\\)?\nBefore answering this, it will be useful to refresh the related concept of conditional probability:\n\nIf \\(X=x\\) and \\(Y=y\\) are events with \\(P(Y=y)&gt;0\\), then the conditional probability of \\(X=x\\) given \\(Y=y\\) is denoted by \\(P(X=x \\mid Y=y)\\), defined as\n\\[ P(X=x \\mid Y=y) = \\frac{P(X=x , Y=y)}{P(Y=y)} \\]\n– Blitzstein and Hwang (2019)\n\nThis formula specifically describes the probability of the event, \\(X=x\\), given the evidence, an observed event \\(Y=y\\).\nWe want to shift to describing a mean conditional on that evidence, and we include that information via the weights in the expectation.\n\nRecall that the expectation \\(E(X)\\) is a weighted average of the possible values of \\(X\\), where the weights are the PMF values \\(P(X = x)\\). After learning that an event \\(Y=y\\) occurred, we want to use weights that have been updated to reflect this new information.\n– Blitzstein and Hwang (2019)\n\nThe key point here is that just the weights that each \\(x_i\\) gets multiplied by will change, going from the probability \\(P(X=x)\\) to the conditional probability \\(P(X=x \\mid Y=y)\\).\nArmed with conditional probability formula above, we can define how to compute the conditional expected value \\[\n\\begin{align*}\nE(X \\mid Y=y) &= \\sum_{x} x P(X=x \\mid Y=y) \\\\\n&= \\sum_{x} x \\frac{P(X=x , Y=y)}{P(Y=y)}\n\\end{align*}\n\\]\nReturning to our example with data, we substitute terms to find the following: \\[\n\\begin{align*}\nE(\\text{Age} \\mid \\text{Gender}=\\text{Male}) &= \\sum_{i=1}^n \\text{Age}_iP(\\text{Age}=\\text{Age}_i \\mid \\text{Gender}=\\text{Male}) \\\\\n&= \\sum_{i=1}^n \\text{Age}_i \\frac{P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Male})}{P(\\text{Gender}=\\text{Male})}\n\\end{align*}\n\\]\nWe can explicitly compute this with a for-loop in python, as we did for \\(E(X)\\), but this time we will need to do a little up front work and define components we need for calculating the weights, \\(\\frac{P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Male})}{P(\\text{Gender}=\\text{Male})}\\)\n\nComponents\n\nThe joint probability distribution: \\(P(\\text{Age}=\\text{Age}_i, \\text{Gender} = \\text{Male})\\)\nThe probability of the event, \\(P(\\text{Gender}=\\text{Male})\\)\n\nWhere 1.) is the following:\n\nP_Age_Gender = pd.crosstab(people_df['Age'],\n                           people_df['Gender'],\n                           normalize='all')\nP_Age_Gender['Male']\n\nAge\n71    0.03\n72    0.07\n73    0.05\n74    0.08\n75    0.07\n76    0.05\n77    0.06\n78    0.04\nName: Male, dtype: float64\n\n\nand 2.) is:\n\nP_Gender = people_df['Gender'].value_counts(normalize=True)\nP_Gender.loc['Male']\n\n0.45\n\n\nWith those two pieces, we’ll convert the following into a for-loop: \\[\n\\sum_{i=1}^n \\text{Age}_i \\frac{P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Male})}{P(\\text{Gender}=\\text{Male})}\n\\]\n\nE_age_male = 0\nn = len(P_Age_Gender['Male'])\nfor i in range(n):\n  weight = P_Age_Gender['Male'].values[i] / P_Gender.loc['Male']\n  E_age_male += P_Age_Gender['Male'].index[i] * weight\nE_age_male\n\n74.51111111111112\n\n\nWe confirm that this is equal to the result of the more direct groupby:\n\npeople_df.groupby('Gender')['Age'].mean()['Male']\n\n74.5111111111111"
  },
  {
    "objectID": "posts/iterated-expectations/index.html#the-law-of-iterated-expectations",
    "href": "posts/iterated-expectations/index.html#the-law-of-iterated-expectations",
    "title": "Iterated Expectations",
    "section": "The Law of Iterated Expectations",
    "text": "The Law of Iterated Expectations\nThe law of iterated expectations, also referred to as the law of total expectation, the tower property, Adam’s law, or, my favorite, LIE, states the following: \\[E(X) = E(E(X \\mid Y))\\] Which is to say, the weighted average of \\(X\\) is equal to the weighted average of the weighted averages of \\(X\\) conditional on each value of \\(Y\\). This isn’t a particularly useful sentence, so let’s return to our example data. We plug in our values as follows: \\[E(\\text{Age}) = E(E(\\text{Age} \\mid \\text{Gender}))\\] Now it is useful to break this into some components that we’ve seen before. We previously found \\[\nE(\\text{Age} \\mid \\text{Gender}=\\text{Male}) =  \\sum_{i=1}^n \\text{Age}_i \\frac{P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Male})}{P(\\text{Gender}=\\text{Male})}\n\\]\nOver all \\(\\text{Gender}_j\\), we have the more generalizable expression: \\[\nE(\\text{Age} \\mid \\text{Gender}=\\text{Gender}_j)\n\\]\nWhich can tell us about any gender, not just \\(\\text{Gender}=\\text{Male}\\). This is equivalent to the expression:\n\\[\n\\begin{align*}\nE(\\text{Age} \\mid \\text{Gender} = \\text{Gender}_j) = \\sum_{i=1}^n \\text{Age}_i \\frac{P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Gender}_j) }{P(\\text{Gender}=\\text{Gender}_j)}\n\\end{align*}\n\\]\nGiven this, let’s return to the informal definition of the LIE, but break it into parts. The weighted average of \\(X\\) is equal to:\n1. The weighted average of 2. the weighted averages of \\(X\\) conditional on each value of \\(Y\\)“.\nThe expression above, \\(E(\\text{Age} \\mid \\text{Gender}=\\text{Gender}_j)\\) is equivalent to 2.) “the weighted averages of \\(X\\) conditional on each value of \\(Y\\).” So what we need to do now is find the weighted average of that expression. We’ll set up in the next few lines\n\\[\\begin{align*}\nE(\\text{Age}) &=E( \\underbrace{E(\\text{Age} \\mid \\text{Gender}=\\text{Gender}\\_j)}_{\\text{weighted averages conditional on each gender}} ) \\\\\n&=E(\\sum_{i} \\text{Age}_i \\frac{P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Gender}_j) }{P(\\text{Gender}=\\text{Gender}_j)}) \\\\\n\\end{align*}\\]\nWith that set up, we’ll now write out the last weighted average explicitly. Note that the variation in \\(\\text{Age}_i\\) has been accounted for – we are now averaging over gender, \\(\\text{Gender}_j\\).\n\\[\\begin{align*}\n&=\\sum_j (\\sum_{i} \\text{Age}_i \\frac{P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Gender}_j) }{P(\\text{Gender}=\\text{Gender}_j)}) P(\\text{Gender}=\\text{Gender}_j) \\\\\n&=\\sum_j \\sum_{i} \\text{Age}_i P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Gender}_j) \\\\\n\\end{align*}\\]\nSince \\(j\\) only appears in one of these two terms, we can rewrite this as follows:\n\\[\\begin{align*}\n&=  \\sum_{i} \\text{Age}_i \\sum_j P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Gender}_j)\n\\end{align*}\\]\nHere I’ll pause, because the next steps can be clarified with code. \\(P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Gender}_j)\\) is the joint probability distribution of age and gender, and it helps to take a look at exactly what it is in pandas:\n\nP_Age_Gender\n\n\n\n\n\n\n\n\nGender\nFemale\nMale\n\n\nAge\n\n\n\n\n\n\n71\n0.05\n0.03\n\n\n72\n0.06\n0.07\n\n\n73\n0.06\n0.05\n\n\n74\n0.11\n0.08\n\n\n75\n0.05\n0.07\n\n\n76\n0.08\n0.05\n\n\n77\n0.07\n0.06\n\n\n78\n0.07\n0.04\n\n\n\n\n\n\n\n\nLet’s compute the summation of \\(P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Gender}_j)\\) over \\(\\text{Gender}_j\\) and see what we get.\n\nP_Age_Gender[\"Male\"] + P_Age_Gender[\"Female\"]\n\nAge\n71    0.08\n72    0.13\n73    0.11\n74    0.19\n75    0.12\n76    0.13\n77    0.13\n78    0.11\ndtype: float64\n\n\nInterestingly, that is the exact same thing we get if we simply compute the probability of each age, \\(P(\\text{Age}=\\text{Age}_i)\\)\n\npeople_df['Age'].value_counts(normalize=True).sort_index()\n\n71    0.08\n72    0.13\n73    0.11\n74    0.19\n75    0.12\n76    0.13\n77    0.13\n78    0.11\nName: Age, dtype: float64\n\n\nSo when you sum \\(P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Gender}_j)\\) only over \\(\\text{Gender}_j\\), you’re just left with \\(P(\\text{Age}=\\text{Age}_i)\\). This result stems from the definition of the Marginal PMF:\n\nFor the discrete random variables \\(X\\) and \\(Y\\), the marginal PMF of \\(X\\) is:\n\\[P(X=x) = \\sum_y P(X=x, Y=y)\\] – Blitzstein and Hwang (2019)\n\nand with this definition in mind we can finish the proof for the LIE: \\[\n\\begin{align*}\nE(\\text{Age})  &= \\sum_{i} \\text{Age}_i \\sum_j P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Gender}_j) \\\\\n&= \\sum_{i} \\text{Age}_i  P(\\text{Age}=\\text{Age}_i) \\\\\n&= E(\\text{Age})\n\\end{align*}\n\\]\nWe can directly show the last bit, \\(E(\\text{Age}) = \\sum_j \\sum_{i} \\text{Age}_i P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Gender}_j)\\) using the joint probability distribution object from before:\n\nP_Age_Gender.head()\n\n\n\n\n\n\n\n\nGender\nFemale\nMale\n\n\nAge\n\n\n\n\n\n\n71\n0.05\n0.03\n\n\n72\n0.06\n0.07\n\n\n73\n0.06\n0.05\n\n\n74\n0.11\n0.08\n\n\n75\n0.05\n0.07\n\n\n\n\n\n\n\n\n\n(P_Age_Gender\n .sum(axis=1) # sum over j\n .reset_index() # bring out Age_i\n .product(axis=1) # Age_i * P(Age=Age_i)\n .sum() # sum over i\n )\n\n74.6\n\n\n\npeople_df['Age'].mean()\n\n74.6"
  },
  {
    "objectID": "posts/dask-data-io/index.html",
    "href": "posts/dask-data-io/index.html",
    "title": "Scraping with bs4, parquet, and dask",
    "section": "",
    "text": "The following example involves a situation where I want to analyze historical data on Bay Area Rapid Transit (BART) ridership at the station/hour level. BART kindly makes such ridership information publicly available on their open data portal. This post will examine the following workflow:\nI should note that my adoption of dask and parquet storage is due to the influence of two of my day-job coworkers, B. Hasan and Konrad Franco, after we leveraged them for an internal project with a similar workflow.\nAnyways, to start, here is the homepage of the portal\nimport pandas as pd\nimport numpy as np\nimport dask.dataframe as dd\n\n# Timers\nfrom dask.diagnostics import ProgressBar\nfrom tqdm import tqdm\n\n# I/O Utilities\nimport requests\nfrom bs4 import BeautifulSoup\nimport re\nimport os\nfrom datetime import datetime"
  },
  {
    "objectID": "posts/dask-data-io/index.html#data-retrieval",
    "href": "posts/dask-data-io/index.html#data-retrieval",
    "title": "Scraping with bs4, parquet, and dask",
    "section": "Data Retrieval",
    "text": "Data Retrieval\nTo start, I’ll set up a scraping script using requests and BeautifulSoup to build access to the BART hourly ridership data.\n\n# URL of the webpage to scrape\nurl = 'https://afcweb.bart.gov/ridership/origin-destination/'\n# Send an HTTP GET request to the webpage\nresponse = requests.get(url)\n# Check if the request was successful\nif response.status_code == 200:\n    # Parse the content of the webpage with Beautiful Soup\n    soup = BeautifulSoup(response.text, 'html.parser')\nelse:\n    print(\"Failed to retrieve the webpage\")\n\nThe page is laid out as follows:\n\n\n\nOrigin-destination pairing data\n\n\nMy target here is the set of .csv.gz (compressed .csv files) that contain hourly ridership totals between each station pairing. These are all links, thus are in &lt;a&gt; &lt;/a&gt; tags, and have an href that ends in .csv.gz. The following captures links to that specification:\n\nlinks = soup.find_all(\n    # final all &lt;a&gt;&lt;/a&gt; content\n    'a',\n    # filter to only those links with href ending in .csv.gz\n    href=lambda x: x and x.endswith(\".csv.gz\")\n    )\n# example output\nlinks[0]\n\n&lt;a href=\"date-hour-soo-dest-2018.csv.gz\"&gt; date-hour-soo-dest-2018.csv.gz&lt;/a&gt;\n\n\nI’m specifically interested in the file that this piece of html links to, which is contained in the href tag. I can capture the href for each of these pieces of html as follows\n\nfiles = [l.get('href') for l in links]\n# example output\nfiles[0]\n\n'date-hour-soo-dest-2018.csv.gz'\n\n\nI’ve now captured the filename, which is a relative url. To download this, I’ll need to convert this to a full url, by concatenating the base url:\n\nurl\n\n'https://afcweb.bart.gov/ridership/origin-destination/'\n\n\nto each of the files’ relative urls. This leaves us with direct links that each prompt the download of one year’s worth of hourly trip totals between the station pairings in the system:\n\nfile_urls = [url + f for f in files]\nfile_urls\n\n['https://afcweb.bart.gov/ridership/origin-destination/date-hour-soo-dest-2018.csv.gz',\n 'https://afcweb.bart.gov/ridership/origin-destination/date-hour-soo-dest-2019.csv.gz',\n 'https://afcweb.bart.gov/ridership/origin-destination/date-hour-soo-dest-2020.csv.gz',\n 'https://afcweb.bart.gov/ridership/origin-destination/date-hour-soo-dest-2021.csv.gz',\n 'https://afcweb.bart.gov/ridership/origin-destination/date-hour-soo-dest-2022.csv.gz',\n 'https://afcweb.bart.gov/ridership/origin-destination/date-hour-soo-dest-2023.csv.gz',\n 'https://afcweb.bart.gov/ridership/origin-destination/date-hour-soo-dest-2024.csv.gz']\n\n\nThis is our target data, so before proceeding to download all of it for local storage, we’ll profile the sizes of each file and the total download:\n\ncounter = 1\ntotal = 0\nfor f in file_urls:\n    response = requests.head(f)\n    # Retrieve the file size for each file\n    file_size = int(response.headers.get('Content-Length', 0))\n    # Keep track of the total file size\n    total += file_size\n    print(f\"File {counter} size: {file_size} bytes ({round(file_size*10e-7, 2)} mega-bytes)\")\n    counter += 1\nprint(f\"Total size of data: {total*10e-7} mega-bytes\")\n\nFile 1 size: 38627139 bytes (38.63 mega-bytes)\nFile 2 size: 38177159 bytes (38.18 mega-bytes)\nFile 3 size: 21415653 bytes (21.42 mega-bytes)\nFile 4 size: 24350926 bytes (24.35 mega-bytes)\nFile 5 size: 30546036 bytes (30.55 mega-bytes)\nFile 6 size: 32224174 bytes (32.22 mega-bytes)\nFile 7 size: 8808985 bytes (8.81 mega-bytes)\nTotal size of data: 194.150072 mega-bytes\n\n\nWe’ll proceed to download all of this into a folder, data. Here I take advantage of tqdm’s progress bar so that I can track the potentially large job’s progress. I also add logic for two conditions:\n\nmake sure that the ingest doesn’t re-read files that I already store locally.\nUNLESS, it the data is from the current year, in which case it likely contains more data than the present file for that year.\n\n\ncurrent_year: str = str(datetime.today().year)\n# Create the \"data\" folder if it doesn't exist\nif not os.path.exists('data'):\n    os.makedirs('data')\n# Download and save the files\nfor url in tqdm(file_urls):\n    filename = os.path.join('data', os.path.basename(url))\n    current_year_data: bool = re.search(\"\\d{4}\", url)[0] == current_year\n    file_exists: bool = os.path.exists(filename)\n    if file_exists and not current_year_data:\n        pass\n    else:\n        response = requests.get(url)\n        if response.status_code == 200:\n            with open(filename, 'wb') as file:\n                file.write(response.content)\n        else:\n            print(f\"Failed to download: {url}\")\n\n100%|██████████| 7/7 [00:03&lt;00:00,  2.26it/s]\n\n\nSince we are storing a large amount of data in the project directory, we will also set up a .gitignore to make sure that it doesn’t end up being tracked in version control.\n\n# Create a .gitignore file\ngitignore_content = \"data/\\n\"  # Content to exclude the \"data\" folder\nwith open('.gitignore', 'w') as gitignore_file:\n    gitignore_file.write(gitignore_content)\n\nI now have the raw data stored locally in the following paths.\n\ndata_paths = [\"data/\" + f for f in files]\ndata_paths\n\n['data/date-hour-soo-dest-2018.csv.gz',\n 'data/date-hour-soo-dest-2019.csv.gz',\n 'data/date-hour-soo-dest-2020.csv.gz',\n 'data/date-hour-soo-dest-2021.csv.gz',\n 'data/date-hour-soo-dest-2022.csv.gz',\n 'data/date-hour-soo-dest-2023.csv.gz',\n 'data/date-hour-soo-dest-2024.csv.gz']"
  },
  {
    "objectID": "posts/dask-data-io/index.html#efficient-storage",
    "href": "posts/dask-data-io/index.html#efficient-storage",
    "title": "Scraping with bs4, parquet, and dask",
    "section": "Efficient Storage",
    "text": "Efficient Storage\nSince these are in a usable form now, I decided against implementing any other processing steps to the data and preserve a raw form in storage, but I’ll proceed to batch convert this directory and set of files into a directory of parquet files for more efficient data I/O.\nTo do that, I’ll lazily read in the full directory of .csv files using dask:\n\ndata = dd.read_csv(data_paths, blocksize=None, compression='gzip')\ndata.columns = ['Date', 'Hour', 'Start', 'End', 'Riders']\ndata\n\n\nDask DataFrame Structure:\n\n\n\n\n\nDate\nHour\nStart\nEnd\nRiders\n\n\nnpartitions=7\n\n\n\n\n\n\n\n\n\n\nstring\nint64\nstring\nstring\nint64\n\n\n\n...\n...\n...\n...\n...\n\n\n...\n...\n...\n...\n...\n...\n\n\n\n...\n...\n...\n...\n...\n\n\n\n...\n...\n...\n...\n...\n\n\n\n\nDask Name: operation, 2 expressions\n\n\n\n…\nNow I’ll establish a subdirectory in data that’s just for the parquet directory\n\nif not os.path.exists('data/parquet_data'):\n    os.makedirs('data/parquet_data')\n\nand I’ll write out to that folder using dask, with a progress bar to monitor how long this takes.\n\npbar = ProgressBar()\npbar.register()\n\nFor data of this size, the one-time conversion takes under two minutes\n\ndata.to_parquet('data/parquet_data', write_index=False)\n\n[########################################] | 100% Completed | 79.82 s"
  },
  {
    "objectID": "posts/dask-data-io/index.html#using-the-data",
    "href": "posts/dask-data-io/index.html#using-the-data",
    "title": "Scraping with bs4, parquet, and dask",
    "section": "Using the Data",
    "text": "Using the Data\nWith the data in parquet, it’s now very easy to read in and analyze all of this ridership data. For example, here I use dask to read in about 52 million rows of data in under 30 seconds, in a single line of code.\n\ndf = dd.read_parquet('data/parquet_data').compute()\n\n[########################################] | 100% Completed | 20.13 s\n\n\n\ndf.shape\n\n(52063941, 5)\n\n\nThis data is now all in memory as a pandas dataframe, ready for typical use.\n\ntype(df)\n\npandas.core.frame.DataFrame"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Peter Amerkhanian",
    "section": "",
    "text": "Data Scientist @ CA Department of Social Services\nUC Berkeley MPP ’23, BA ’16\n\n\nI’m Data Scientist/Policy Analyst, currently working on computational and data analysis problems at The California Department of Social Services.\n\nI’m passionate about public service.\nI’m skilled in policy analysis, applied statistics, and various computational methods."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "The following are a collection of my informal notes on topics in programming, math, statistics, and policy analysis. If you find errors or else find them helpful, I’d love to hear from you.\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nTopics\n\n\nTime\n\n\n\n\n\n\nApr 12, 2024\n\n\nScraping with bs4, parquet, and dask\n\n\nPython, Data Management\n\n\n4 min\n\n\n\n\nMar 25, 2024\n\n\nCovariance Matrices\n\n\nPython, Linear Algebra\n\n\n6 min\n\n\n\n\nMar 17, 2024\n\n\nProduction Maximization with Lagrange Mutlipliers\n\n\nR, Calculus\n\n\n5 min\n\n\n\n\nMar 9, 2024\n\n\nA Minima Problem in 3D\n\n\nR, Calculus\n\n\n7 min\n\n\n\n\nMar 2, 2024\n\n\nLinear Approximation in 3D\n\n\nPython, Calculus\n\n\n3 min\n\n\n\n\nDec 8, 2023\n\n\nTrain-Test Split\n\n\nPython, Statistics\n\n\n6 min\n\n\n\n\nDec 1, 2023\n\n\nIterated Expectations\n\n\nPython, Probability\n\n\n7 min\n\n\n\n\nNov 3, 2023\n\n\nNewton’s Method From The Ground Up\n\n\nPython, Calculus\n\n\n10 min\n\n\n\n\nJul 4, 2023\n\n\nSimple Constrained Optimization in 2D\n\n\nPython, Calculus\n\n\n5 min\n\n\n\n\nAug 6, 2022\n\n\nBuy vs. Rent, A Financial Modeling Workflow in Python\n\n\nPython, Applications\n\n\n7 min\n\n\n\n\nJul 20, 2022\n\n\nReliable PDF Scraping with tabula-py\n\n\nPython, Data Management\n\n\n6 min\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/covariance-matrices/index.html",
    "href": "posts/covariance-matrices/index.html",
    "title": "Covariance Matrices",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nimport ucimlrepo\nimport sympy as sp\nimport matplotlib.pyplot as plt\nThe following is largely based on exercises in (Cohen 2022), an excellent overview of applied, numerical linear algebra in python. This exercise is an overview of how to compute the covariance matrix of a dataset, and contains enough fundamental linear algebra and statistics topics – matrix multiplication, the definitions of variance and covariance, etc. – that I wanted to commit it to a blog for my future reference."
  },
  {
    "objectID": "posts/covariance-matrices/index.html#question-and-data",
    "href": "posts/covariance-matrices/index.html#question-and-data",
    "title": "Covariance Matrices",
    "section": "Question and Data",
    "text": "Question and Data\nThe exercise dataset is “Communities and Crime,” from the UC Irvine Machine Learning Repository, which includes data on\n\nCommunities within the United States. The data combines socio-economic data from the 1990 US Census, law enforcement data from the 1990 US LEMAS survey, and crime data from the 1995 FBI UCR.\n\nIn the exercise, we are tasked with created a covariance matrix of the data.\n\n\nCode\n# Fetch the dataset \ncommunities_and_crime: ucimlrepo.dotdict = ucimlrepo.fetch_ucirepo(id=183) \ndf: pd.DataFrame = pd.concat([communities_and_crime.data.features,\n                              communities_and_crime.data.features], axis=1)\n# Isolate numerical features and clear duplicate columns\ndf_num: pd.DataFrame = df.copy().select_dtypes(include='number')\ndf_num = df_num.loc[:,~df_num.columns.duplicated()]\ndf_num = df_num.iloc[:, 2:]\n\n\nTo make the operations as clear as possible, I’m going to overview the process using only the first five rows before computing the full covariance matrix.\n\nexample = df_num.head()\nexample\n\n\n\n\n\n\n\n\n\npopulation\nhouseholdsize\nracepctblack\nracePctWhite\nracePctAsian\nracePctHisp\nagePct12t21\nagePct12t29\nagePct16t24\nagePct65up\n...\nNumStreet\nPctForeignBorn\nPctBornSameState\nPctSameHouse85\nPctSameCity85\nPctSameState85\nLandArea\nPopDens\nPctUsePubTrans\nLemasPctOfficDrugUn\n\n\n\n\n0\n0.19\n0.33\n0.02\n0.90\n0.12\n0.17\n0.34\n0.47\n0.29\n0.32\n...\n0.0\n0.12\n0.42\n0.50\n0.51\n0.64\n0.12\n0.26\n0.20\n0.32\n\n\n1\n0.00\n0.16\n0.12\n0.74\n0.45\n0.07\n0.26\n0.59\n0.35\n0.27\n...\n0.0\n0.21\n0.50\n0.34\n0.60\n0.52\n0.02\n0.12\n0.45\n0.00\n\n\n2\n0.00\n0.42\n0.49\n0.56\n0.17\n0.04\n0.39\n0.47\n0.28\n0.32\n...\n0.0\n0.14\n0.49\n0.54\n0.67\n0.56\n0.01\n0.21\n0.02\n0.00\n\n\n3\n0.04\n0.77\n1.00\n0.08\n0.12\n0.10\n0.51\n0.50\n0.34\n0.21\n...\n0.0\n0.19\n0.30\n0.73\n0.64\n0.65\n0.02\n0.39\n0.28\n0.00\n\n\n4\n0.01\n0.55\n0.02\n0.95\n0.09\n0.05\n0.38\n0.38\n0.23\n0.36\n...\n0.0\n0.11\n0.72\n0.64\n0.61\n0.53\n0.04\n0.09\n0.02\n0.00\n\n\n\n\n5 rows × 99 columns\n\n\n\n\n\nBivariate Covariance via Summation\nSay we have two columns, householdsize and population, and we’d like to know to what degree they are associated.\n\nx = example['population']\ny = example['householdsize']\nexample[['householdsize', 'population']]\n\n\n\n\n\n\n\n\n\nhouseholdsize\npopulation\n\n\n\n\n0\n0.33\n0.19\n\n\n1\n0.16\n0.00\n\n\n2\n0.42\n0.00\n\n\n3\n0.77\n0.04\n\n\n4\n0.55\n0.01\n\n\n\n\n\n\n\n\nI can compute their covariance statistic via the following equation, where \\(x\\) is, say householdsize, and \\(y\\) is population\n\\[\nc_{x, y} = (n-1)^{-1} \\sum_{i=1} ^n (x_i - \\bar{x})(y_i - \\bar{y})\n\\] (Cohen 2022, chap. 7)\nI create a function as follows to flexibly compute this formula for two arrays of data.\n\ndef bivariate_cov(x: np.array, y: np.array) -&gt; float:\n    # Define Constants\n    x_bar: float = x.mean()\n    y_bar: float = y.mean()\n    n = len(x)\n    # Summation Script\n    summation = 0\n    for i in range(n):\n        # mean-center vector, then normalize\n        summation += (x.loc[i] - x_bar) * (y.loc[i] - y_bar)\n    return summation / (n-1)\n\nFor these two variables, we find the covariance as follows:\n\nprint(\"cov(x, y):\", bivariate_cov(x, y))\n\ncov(x, y): -0.0020099999999999996\n\n\nI’ll check my answer with a built in function, numpy.cov():\n\nnp.cov(x, y)\n\narray([[ 0.00657, -0.00201],\n       [-0.00201,  0.05293]])\n\n\nThis is clearly different! Rather than a scalar, this function returned a matrix. However, the upper and lower triangles of the matrix are the same as the covariance value we computed. Indeed, what numpy returned for covariance is the following matrix: \\[\n\\left[\n\\begin{matrix}\n    var(x) & cov(x, y) \\\\\n    cov(y, x) & var(y) \\\\\n\\end{matrix}\n\\right]\n\\]\nWe can compute the variances of \\(x\\) and \\(y\\) to confirm this (note that variance is, in a sense, a vector’s covariance with itself)\n\nprint(\"var(x):\", bivariate_cov(x, x))\nprint(\"var(y):\", bivariate_cov(y, y))\n\nvar(x): 0.006570000000000002\nvar(y): 0.05293000000000001\n\n\n\n\nThe Vector Formula for Bivariate Covariance\nTo get our output to match numpy’s we’ll start by generalizing the covariance equation to input vectors\n\\[\n\\begin{align*}\nc_{a, b} &= (n-1)^{-1} \\sum_{i=1} ^n (x_i - \\bar{x})(y_i - \\bar{y}) \\\\\n&= (n-1)^{-1}(\\mathbf {x} - \\bar{\\mathbf {x}})^\\intercal (\\mathbf {y} - \\bar{\\mathbf {y}}) \\\\\n&= (n-1)^{-1} \\tilde{\\mathbf {x}}^\\intercal \\tilde{\\mathbf {y}}\n\\end{align*}\n\\] (Cohen 2022, chap. 7)\nNote that we’ve made a few notational changes here. We’ll now refer to the vectors using typical bold-face notation to differentiate them from scalars. Also note that \\(\\tilde{\\mathbf {x}}\\) is \\(\\mathbf {x}\\) mean-centered.\nTo compute this formula, I’ll denote the vectors and their means below:\n\nprint(\"x:\", x.values, \"mean(x)=\", x.mean())\nprint(\"y:\", y.values,  \"mean(y)=\", y.mean())\n\nx: [0.19 0.   0.   0.04 0.01] mean(x)= 0.048\ny: [0.33 0.16 0.42 0.77 0.55] mean(y)= 0.446\n\n\nFirst we’ll define those two mean centered vectors as follows: \\[\n\\tilde{\\mathbf {x}} = (\\left[\\begin{matrix}0.19\\\\0\\\\0\\\\0.04\\\\0.01\\end{matrix}\\right] - 0.048), \\quad \\tilde{\\mathbf {y}} = (\\left[\\begin{matrix}0.33\\\\0.16\\\\0.42\\\\0.77\\\\0.55\\end{matrix}\\right] - 0.446) \\\\\n\\] then we’ll plug those into the vector covariance formula\n\\[\n\\begin{align*}\nc_{x,y} &= \\frac{1}{4} (\\left[\\begin{matrix}0.142\\\\-0.048\\\\-0.048\\\\-0.008\\\\-0.038\\end{matrix}\\right]^\\intercal \\left[\\begin{matrix}-0.116\\\\-0.286\\\\-0.026\\\\0.324\\\\0.104\\end{matrix}\\right]) \\\\\n&= \\frac{1}{4} (\\left[\\begin{matrix}0.142 & -0.048 & -0.048 & -0.008 & -0.038\\end{matrix}\\right] \\left[\\begin{matrix}-0.116\\\\-0.286\\\\-0.026\\\\0.324\\\\0.104\\end{matrix}\\right]) \\\\\n&\\approx \\frac{1}{4} (-0.00804) \\\\\n&\\approx -0.00201\n\\end{align*}\n\\]\nWe’ll also write a new python function to compute the vector formula for covariance\n\ndef bivariate_cov_vec(x: np.array, y: np.array) -&gt; float:\n    x_bar: float = x.mean()\n    y_bar: float = y.mean()\n    n = len(x)\n    return ((x - x_bar) @ (y - y_bar)) / (n - 1)\nbivariate_cov_vec(x, y)\n\n-0.0020099999999999996\n\n\n\n\nThe Matrix Formulation\nNow that we’ve defined the vector formula for covariance, we can generalize one step further to the most flexible equation for covariance, the matrix formulation. This formula can take in an arbitrary number of variables (column vectors in this case) and compute the covariance between each of them. Our equation is as follows:\n\\[\nC = X^\\intercal X \\frac{1}{n-1}\n\\] (Cohen 2022, chap. 7)\n(Where \\(X\\) is matrix with mean centered column vectors).\nIn a moment we’ll overview how this relates to the vector formulation, but first we’ll address the transpose, \\(X^\\intercal\\)\n\nC = sp.symbols(\"C\")\nX = example[['population', 'householdsize']]\n\n\nA Note on Matrix Multiplication\nIf we examine the dimensions of our matrix, we can see that these do not satisfy the conditions for matrix multiplication (# columns in first matrix == # rows in second matrix).\n\nX.shape, X.shape\n\n((5, 2), (5, 2))\n\n\n\nmatrix multiplication is valid when the number of columns in the left matrix equals the number of rows in the right matrix.\n– (Cohen 2022, chap. 5)\n\n\ntry:\n    X @ X\nexcept Exception as e:\n    print(\"ERROR -&gt;\\n\", e)\n\nERROR -&gt;\n matrices are not aligned\n\n\nTo remedy this, we transpose the first matrix. A matrix transposed then multiplied with itself is always a valid matrix multiplication. Indeed, the product is always a square-symmetric matrix, with implications for various linear algebra methods (Cohen 2022, chap. 5).\n\nX.T.shape, X.shape\n\n((2, 5), (5, 2))\n\n\n\ntry:\n    X.T @ X\n    print(\"Successfully Multiplied!\")\nexcept Exception as e:\n    print(\"ERROR -&gt;\", e)\n\nSuccessfully Multiplied!\n\n\nNow we’ll return to the formula: \\[\nC = X^\\intercal X \\frac{1}{n-1}\n\\]\nWe plug in our data as follows. I’ll be doing it in sympy so that I can take a peak at the matrix multiplication underneath the hood.\n\nout = (\n    sp.UnevaluatedExpr(sp.Matrix(X).T) * sp.UnevaluatedExpr(sp.Matrix(X))\n    * 1 / (sp.UnevaluatedExpr(len(x)) - sp.UnevaluatedExpr(1))\n    )\nsp.Eq(C, out)\n\n\\(\\displaystyle C = \\left[\\begin{matrix}0.19 & 0 & 0 & 0.04 & 0.01\\\\0.33 & 0.16 & 0.42 & 0.77 & 0.55\\end{matrix}\\right] \\left[\\begin{matrix}0.19 & 0.33\\\\0 & 0.16\\\\0 & 0.42\\\\0.04 & 0.77\\\\0.01 & 0.55\\end{matrix}\\right] \\left(- 1 + 5\\right)^{-1}\\)\n\n\nAlready it should become apparent that this multiplication will be a sequential application of the vector formula for covariance. We can make that abundantly clear by visualizing the multiplication result:\n\nout = sp.Matrix([[sp.UnevaluatedExpr(sp.Matrix(x).T) * sp.UnevaluatedExpr(sp.Matrix(x))\n                  * 1 / (sp.UnevaluatedExpr(len(x)) - sp.UnevaluatedExpr(1)),\n                  sp.UnevaluatedExpr(sp.Matrix(x).T) * sp.UnevaluatedExpr(sp.Matrix(y))\n                  * 1 / (sp.UnevaluatedExpr(len(x)) - sp.UnevaluatedExpr(1))],\n                  [sp.UnevaluatedExpr(sp.Matrix(y).T) * sp.UnevaluatedExpr(sp.Matrix(x))\n                   * 1 / (sp.UnevaluatedExpr(len(x)) - sp.UnevaluatedExpr(1)),\n                   sp.UnevaluatedExpr(sp.Matrix(y).T) * sp.UnevaluatedExpr(sp.Matrix(y))\n                   * 1 / (sp.UnevaluatedExpr(len(x)) - sp.UnevaluatedExpr(1))]\n                   ])\nout\n\n\\(\\displaystyle \\left[\\begin{matrix}\\left[\\begin{matrix}0.19 & 0 & 0 & 0.04 & 0.01\\end{matrix}\\right] \\left[\\begin{matrix}0.19\\\\0\\\\0\\\\0.04\\\\0.01\\end{matrix}\\right] \\left(- 1 + 5\\right)^{-1} & \\left[\\begin{matrix}0.19 & 0 & 0 & 0.04 & 0.01\\end{matrix}\\right] \\left[\\begin{matrix}0.33\\\\0.16\\\\0.42\\\\0.77\\\\0.55\\end{matrix}\\right] \\left(- 1 + 5\\right)^{-1}\\\\\\left[\\begin{matrix}0.33 & 0.16 & 0.42 & 0.77 & 0.55\\end{matrix}\\right] \\left[\\begin{matrix}0.19\\\\0\\\\0\\\\0.04\\\\0.01\\end{matrix}\\right] \\left(- 1 + 5\\right)^{-1} & \\left[\\begin{matrix}0.33 & 0.16 & 0.42 & 0.77 & 0.55\\end{matrix}\\right] \\left[\\begin{matrix}0.33\\\\0.16\\\\0.42\\\\0.77\\\\0.55\\end{matrix}\\right] \\left(- 1 + 5\\right)^{-1}\\end{matrix}\\right]\\)\n\n\n\\[\n\\begin{align*}\n&= \\frac{1}{n-1}\n\\left[\n\\begin{matrix}\n    \\tilde{\\mathbf{x}} \\cdot \\tilde{\\mathbf{x}} & \\tilde{\\mathbf{x}} \\cdot \\tilde{\\mathbf{y}} \\\\\n    \\tilde{\\mathbf{y}} \\cdot \\tilde{\\mathbf{x}} & \\tilde{\\mathbf{y}} \\cdot \\tilde{\\mathbf{y}} \\\\\n\\end{matrix}\n\\right] \\\\\n&=\n\\left[\n\\begin{matrix}\n    var(\\tilde{\\mathbf{x}}) & cov(\\tilde{\\mathbf{x}}, \\tilde{\\mathbf{y}}) \\\\\n    cov(\\tilde{\\mathbf{y}}, \\tilde{\\mathbf{x}}) & var(\\tilde{\\mathbf{y}}) \\\\\n\\end{matrix}\n\\right]\n\\end{align*}\n\\]\n(Note that the Commutative Property holds for dot products, thus \\(\\tilde{\\mathbf{x}} \\cdot \\tilde{\\mathbf{y}} = \\tilde{\\mathbf{y}} \\cdot \\tilde{\\mathbf{x}} = cov(\\tilde{\\mathbf{x}}, \\tilde{\\mathbf{y}})\\)).\nFor a sanity check, we can compare our results with those in numpy and pandas:\n\nA = X - X.mean()\n(A .T @ A ) / (len(A )-1)\n\n\n\n\n\n\n\n\n\npopulation\nhouseholdsize\n\n\n\n\npopulation\n0.00657\n-0.00201\n\n\nhouseholdsize\n-0.00201\n0.05293\n\n\n\n\n\n\n\n\nnumpy:\n\nnp.cov(x, y)\n\narray([[ 0.00657, -0.00201],\n       [-0.00201,  0.05293]])\n\n\npandas:\n\nX.cov()\n\n\n\n\n\n\n\n\n\npopulation\nhouseholdsize\n\n\n\n\npopulation\n0.00657\n-0.00201\n\n\nhouseholdsize\n-0.00201\n0.05293\n\n\n\n\n\n\n\n\n\n\nVisualizing the Covariance Matrix\nWe’ll now return to our original, full dataset\n\ndf_num.shape\n\n(1994, 99)\n\n\nand we’ll compute the covariance matrix across al 99 features.\n\ncov_matrix = (df_num - df_num.mean()).T @ (df_num - df_num.mean()) / (len(df_num)-1)\n\ncov_matrix.shape\n\n(99, 99)\n\n\nLooking at a \\(99 \\times 99\\) matrix is not particularly useful, so it’s common to use heatmaps to visualize matrices that show association between many variables.\n\nplt.imshow(cov_matrix.values)\nplt.colorbar();"
  },
  {
    "objectID": "posts/covariance-matrices/index.html#the-correlation-matrix",
    "href": "posts/covariance-matrices/index.html#the-correlation-matrix",
    "title": "Covariance Matrices",
    "section": "The Correlation Matrix",
    "text": "The Correlation Matrix\nWhile this looks cool, it’s more of less useless as a tool for doing any sort of analysis because the covariances between variables are highly influenced by the scale of the original variable. This will be much more useful if we convert it into a correlation matrix, \\(R\\), wherein the measures of association, \\(\\rho_{i,j}\\) are all normalized, \\(\\rho_{i,j} \\in [-1, 1]\\). This in effect adjusts our measures for scale and allows us to actually see which variables are the most associated.\nTo convert a covariance matrix into a correlation matrix, we take the covariance matrix \\(C\\): \\[\nC = X^\\intercal X \\frac{1}{n-1}\n\\]\n\nC = (A.T @ A ) / (len(A )-1)\nC\n\n\n\n\n\n\n\n\n\npopulation\nhouseholdsize\n\n\n\n\npopulation\n0.00657\n-0.00201\n\n\nhouseholdsize\n-0.00201\n0.05293\n\n\n\n\n\n\n\n\nand we first calculate its diagonals, which are the variances of each column vector/feature. We convert these variances \\(\\sigma^2\\) into standard deviation, \\(\\sqrt{\\sigma^2}\\), and take the reciprocal, \\(\\frac{1}{\\sqrt{\\sigma^2}}\\), as follows to obtain the diagonal matrix \\(S\\): \\[\nS = (\\text{diag}(C))^{-\\frac{1}{2}}\n\\]\nwherein each diagonal element is \\(\\frac{1}{\\sigma_{j, (j+1)}}\\).\n\nprint(\"C:\\n\", C.values)\nprint()\nprint(\"diag(C):\\n\", np.diag(np.diag(C)))\nprint()\nprint(\"diag(C)^(-.5):\\n\", np.diag(np.diag(C)**(-.5)))\n\nS = np.diag(np.diag(C)**(-.5))\n\nC:\n [[ 0.00657 -0.00201]\n [-0.00201  0.05293]]\n\ndiag(C):\n [[0.00657 0.     ]\n [0.      0.05293]]\n\ndiag(C)^(-.5):\n [[12.33722017  0.        ]\n [ 0.          4.34659377]]\n\n\nWith that in hand, we can compute the correlation matrix \\(R\\): \\[\nR = SCS\n\\]\n\nS @ C @ S\n\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\n1.000000\n-0.107786\n\n\n1\n-0.107786\n1.000000\n\n\n\n\n\n\n\n\nWe double check this with the pandas implementation of corr()\n\nX.corr()\n\n\n\n\n\n\n\n\n\npopulation\nhouseholdsize\n\n\n\n\npopulation\n1.000000\n-0.107786\n\n\nhouseholdsize\n-0.107786\n1.000000\n\n\n\n\n\n\n\n\nWe’ll now return to our full dataset and compute the correlation matrix\n\nS_large = np.diag(np.diag(cov_matrix)**(-.5))\ncorr_matrix = S_large @ cov_matrix @ S_large\ncorr_matrix.iloc[:5, :5]\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\n0\n1.000000\n-0.046148\n0.231178\n-0.300845\n0.181603\n\n\n1\n-0.046148\n1.000000\n-0.067109\n-0.235907\n0.201996\n\n\n2\n0.231178\n-0.067109\n1.000000\n-0.794389\n-0.106738\n\n\n3\n-0.300845\n-0.235907\n-0.794389\n1.000000\n-0.270266\n\n\n4\n0.181603\n0.201996\n-0.106738\n-0.270266\n1.000000\n\n\n\n\n\n\n\n\nWhen we visualize this matrix, we gain richer information, because we can see, relative to all of the inter-variable relationships in the data, which are the strongest.\n\nplt.imshow(corr_matrix.values, vmin=-1, vmax=1)\nplt.colorbar();"
  },
  {
    "objectID": "posts/lagrange-cobb-douglas/index.html",
    "href": "posts/lagrange-cobb-douglas/index.html",
    "title": "Production Maximization with Lagrange Mutlipliers",
    "section": "",
    "text": "Code\nlibrary(plotly)\nlibrary(dplyr)\n\nscene &lt;- list(\n  camera = list(eye = list(\n    x = -2.2, y = 1.1, z = 1.2\n  )),\n  xaxis = list(title = \"L\"),\n  yaxis = list(title = \"K\"),\n  zaxis = list(title = \"$\")\n)\n\n\n\nThe Optimization Problem\n\nA company has determined that its production level is given by the Cobb-Douglas function \\(f(x,y)=2.5x^{0.45}y^{0.55}\\) where \\(x\\) represents the total number of labor hours in 1 year and \\(y\\) represents the total capital input for the company. Suppose 1 unit of labor costs $40 and 1 unit of capital costs $50. Use the method of Lagrange multipliers to find the maximum value of \\(f(x,y)=2.5x^{0.45}y^{0.55}\\) subject to a budgetary constraint of $500,000 per year.\n– (Strang and Herman 2016, chap. 4.8)\n\n\\[\n\\begin{align}\nf(x, y) \\rightarrow P(L, K) \\\\\nP(L, K) &= 2.5L^{0.45}K^{0.55} \\\\\ng(L, K) &= 40L + 50K - 500,000\n\\end{align}\n\\tag{1}\\]\nWe set the equation up in R so that we can inspect a plot and better understand the optimization problem.\n\nP_l_k &lt;- function(L, K) {\n  2.5 * L ^ (0.45) * K ^ (0.55)\n}\ng_l_k &lt;- function(L, K) {\n  40 * L + 50 * K - 500000\n}\nn &lt;- 100\nL &lt;- seq(0, 6000, length.out = n)\nK &lt;- seq(0, 6000, length.out = n)\nP &lt;- outer(L, K, P_l_k)\ng &lt;- outer(L, K, g_l_k)\n\n\n\nCode\nplot_ly(\n  x = L,\n  y = K,\n  z = P,\n  type = \"surface\",\n  name = \"P(L,K)\"\n) %&gt;%\n  colorbar(title = \"P(L,K)\") %&gt;%\n  add_trace(\n    x = L,\n    y = K,\n    z = g,\n    type = \"surface\",\n    colorscale = \"coolwarm\",\n    name = \"g(L,K)\",\n    colorbar = list(title = \"g(L,K)\")\n  ) %&gt;% layout(scene = scene)\n\n\n\n\n\n\nWe see that the production function \\(P\\) and the cost function \\(g\\) are surfaces that intersect. We are looking for the highest possible point in \\(P\\) that does not exceed the constraint \\(g\\), which will be somewhere around their intersection. Note that generally, all values below the intersection are possible, though not profit-maximizing, points. The points higher than the intersection are more profit-maximizing, but are not possible with this budget constraint.\n\n\nMaximizing using the Method of Lagrange Multipliers\nWe adapt the Lagrange multiplier problem-solving strategy from (Strang and Herman 2016, chap. 4.8) to our function input, and set up the following system of equations, which we will solve for \\(L_0\\) and \\(K_0\\):\n\\[\n\\begin{align*}\n\\nabla P(L_0, K_0) &= \\lambda \\nabla g(L_0, K_0) \\\\\ng(L_0, K_0) &= 0\n\\end{align*}\n\\tag{2}\\]\nAt this point, we will need to do some calculations to find each function in Equation 1’s gradient.\n\\[\n\\begin{align*}\n\\nabla P(L_0, K_0) &= \\left&lt;  \\frac{1.125K^{0.55}}{L^{0.55}} , \\frac{1.375L^{0.45}}{K^{0.45}}\\right&gt; \\\\\n\\nabla g(L_0, K_0) &= \\left&lt; 40, 50 \\right&gt; \\\\\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\n&\\begin{cases}\n\\left&lt;  \\frac{1.125K^{0.55}}{L^{0.55}} , \\frac{1.375L^{0.45}}{K^{0.45}}\\right&gt; &= \\lambda \\left&lt; 40, 50 \\right&gt; \\\\\n40L + 50K - 500,000 &= 0\n\\end{cases} \\\\\n&\\begin{cases}\n\\frac{1.125K^{0.55}}{L^{0.55}} &= 40 \\lambda\\\\\n\\frac{1.375L^{0.45}}{K^{0.45}} &= 50 \\lambda \\\\\n40L + 50K - 500,000 &= 0\n\\end{cases} \\\\\n&\\begin{cases}\n\\frac{1.125K^{0.55}}{40L^{0.55}} &= \\lambda\\\\\n\\frac{1.375L^{0.45}}{50K^{0.45}} &= \\lambda \\\\\n40L + 50K - 500,000 &= 0\n\\end{cases} \\\\\n&\\begin{cases}\n\\frac{1.125K^{0.55}}{40L^{0.55}} &= \\frac{1.375L^{0.45}}{50K^{0.45}} \\\\\n40L + 50K - 500,000 &= 0\n\\end{cases} \\\\\n&\\begin{cases}\n5.5L &= 5.625K \\\\\n40L + 50K - 500,000 &= 0\n\\end{cases} \\\\\n&\\begin{cases}\n5.5L- 5.625K &= 0 \\\\\n40L + 50K &= 500,000\n\\end{cases}\n\\end{align*}\n\\]\nWe now have a clear linear system of equations that we can solve via some substitution and algebraic manipulation:\n\\[\n\\begin{align*}\nL &= \\frac{5.625K}{5.5} \\\\\n40 (\\frac{5.625K}{5.5}) + 50K &= 500,000 \\\\\nK &= \\frac{500,000}{(40 (\\frac{5.625}{5.5}) + 50)} = \\boxed{K = \\$ 5,500} \\\\\n40 L + 50(5,500) &= 500,000 \\\\\nL &= \\frac{500,000 - 50(5,500)}{40} = \\boxed{L = 5,625 \\, \\text{labor hours}}\n\\end{align*}\n\\]\nWe’ll now plug those values for capital and labor into our production function and see how much output this maximizing parameter combination produces (we’ll round given we are solving for whole output):\n\nP_l_k(5625, 5500) %&gt;% round()\n\n[1] 13890\n\n\nWhen we return to the plot of the product function and budget constraint, we can see that this point clearly is the highest possible output under the constraints.\n\n\nCode\nplot_ly(\n  x = L,\n  y = K,\n  z = P,\n  type = \"surface\",\n  name = \"P(L,K)\"\n) %&gt;%\n  colorbar(title = \"P(L,K)\") %&gt;%\n  add_trace(\n    x = L,\n    y = K,\n    z = g,\n    type = \"surface\",\n    colorscale = \"coolwarm\",\n    name = \"g(L,K)\",\n    colorbar = list(title = \"g(L,K)\")\n  ) %&gt;%\n  add_trace(\n    x = 5625,\n    y = 5500,\n    z = P_l_k(5625, 5500) %&gt;% round(),\n    type = \"scatter3d\",\n    mode = \"markers\",\n    marker = list(size = 5, color = \"black\")\n  ) %&gt;%\n  layout(scene = scene, legend=list(x=.5, y=0))\n\n\n\n\n\n\nHowever, in \\(R^3\\), contour plots offer a much clearer way of visualizing our solution.\n\n\nCode\nplot_ly(\n  x = L,\n  y = K,\n  z = P,\n  type = \"contour\",\n  name = \"P(L,K)\"\n) %&gt;%\n  colorbar(title = \"P(L,K)\") %&gt;%\n  add_trace(\n    x =  L,\n    y = 10000 - 4 * K / 5,\n    type = 'scatter',\n    mode = 'lines',\n    name = \"g(L, K)\",\n    color = \"red\"\n  ) %&gt;%\n  add_trace(\n    x = 5625,\n    y = 5500,\n    type = \"scatter\",\n    mode = \"markers\",\n    marker = list(\n      size = 10,\n      color = \"black\",\n      name = \"P(L*,K*)\"\n    )\n  ) %&gt;%\n  layout(xaxis = list(range = c(0, max(L))),\n         yaxis = list(range = c(0, max(K))))\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nStrang, Gilbert, and Edwin Herman. 2016. Calculus Volume 3. OpenStax."
  },
  {
    "objectID": "posts/newtons-method/index.html",
    "href": "posts/newtons-method/index.html",
    "title": "Newton’s Method From The Ground Up",
    "section": "",
    "text": "Code\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport sympy\nimport imageio\nfrom typing import Callable\nimport seaborn as sns"
  },
  {
    "objectID": "posts/newtons-method/index.html#newtons-method-for-finding-roots-what-how-and-why",
    "href": "posts/newtons-method/index.html#newtons-method-for-finding-roots-what-how-and-why",
    "title": "Newton’s Method From The Ground Up",
    "section": "Newton’s Method for Finding Roots: What, How, and Why",
    "text": "Newton’s Method for Finding Roots: What, How, and Why\nIn the context of a Differential Calculus course, Newton’s Method, also referred to as The Newton-Raphson Method, seems to typically come up near the end of the semester, offering a brief look into the world of “numerical methods” and how we might solve complex problems in the real world. I think that it’s a cool topic and I wanted to write an extended blog post about it. The main purpose of this is to ensure that I always have personal reference materials for Newton’s Method, but perhaps it can be helpful to other readers.\nI draw on two key sources for thinking about Newton’s Method:\n\n“Newton’s Method.” 2023. In Wikipedia\nStrang, Gilbert, and Edwin Herman. 2016. Calculus Volume 1. OpenStax College.\n\n\nWhat is it\n\nIn many areas of pure and applied mathematics, we are interested in finding solutions to an equation of the form \\(f(x)=0\\)\n\n(Strang and Herman 2016)\nNewton’s Method is a numerical method that helps us solve \\(f(x)=0\\).\nThere are many cases where we need to solve equations like that, but the application area I work in involves statistical modeling, so I jump to the case where we want to “fit” a line as close as possible to some set of data points, thus creating a model of a data-generating-process. The fitting process rests on getting the line as close to the data points as possible, thus minimizing error. If we can formulate error as a function, then we can minimize it – we differentiate it and set the derivative to 0, and solve \\(f'(x)=0\\). Therein lies the opportunity to apply Newton’s Method to a real problem.\nHowever, we’ll take a step back from statistics and return to the domain of an introductory calculus course. Newton’s Method is useful for finding the root – the x-intercept – of a function. We’ll explore the method by walking through an example\n\n\nUsing Newton’s Method to Solve a Simple Problem\nSay we are given the function \\(4\\sin(x)\\) and we want to find the x-intercept of the function with the domain \\(-5\\leq x \\leq -1\\).\nWe’ll first set this function up in sympy, a python library for symbolic computation.\n\nx, y = sympy.symbols('x y')\ny = 4*sympy.sin(x)\ny\n\n\\(\\displaystyle 4 \\sin{\\left(x \\right)}\\)\n\n\nThis is a problem with a known answer (you can google x-intercepts of \\(\\sin(x)\\) for a table), so it’s not particularly useful to use Newton’s Method here, but for our purposes it will be helpful that we can check our answer with the “right” one. \\[\n\\begin{align*}\n4 \\sin(x) &= 0 \\quad \\text{where} \\quad -5\\leq x \\leq -1 \\\\\n\\sin(x) &= 0 \\\\\nx &= \\sin^{-1}(0) \\\\\nx &= -\\pi\n\\end{align*}\n\\]\n\n\nCode\ndef plot_f(\n        f: Callable[[float], float],\n        x_low: int,\n        x_high: int,\n        ax: matplotlib.axes.Axes) -&gt; None:\n    \"\"\"\n    Plots a given function f within a specified range on a provided axes.\n\n    Parameters:\n        f (Callable[[float], float]): The function to be plotted.\n        x_low (int): The lower bound of the x-axis.\n        x_high (int): The upper bound of the x-axis.\n        ax (matplotlib.axes.Axes): The matplotlib axes object on which the function will be plotted.\n\n    Returns:\n        None\n    \"\"\"\n    x_vec = np.linspace(x_low, x_high, 100)\n    ax.plot(x_vec, f(x_vec))\n\n\ndef base_plot(\n        y: sympy.core.mul.Mul,\n        x: sympy.core.mul.Mul,\n        x_low: int = -5,\n        x_high: int = 5) -&gt; None:\n    \"\"\"\n    Creates a base plot for a mathematical expression and its graph.\n\n    Parameters:\n        y (sympy.core.mul.Mul): The mathematical expression to be plotted.\n        x (sympy.core.mul.Mul): The symbol representing the independent variable in the expression.\n        x_low (int): The lower bound of the x-axis (default is -5).\n        x_high (int): The upper bound of the x-axis (default is 5).\n\n    Returns:\n        tuple: A tuple containing the matplotlib figure and axes used for plotting.\n\n    Note:\n        The mathematical expression is first converted to a Python function using sympy.lambdify.\n    The function is then plotted on the specified axes along with gridlines and labels.\n    \"\"\"\n    f = sympy.lambdify(x, y)\n    fig, ax = plt.subplots()\n    ax.grid(alpha=.5)\n    ax.axhline(0, color=\"black\", alpha=.5)\n    plot_f(f, x_low, x_high, ax)\n    ax.set(title=f\"$f(x)={sympy.latex(y)}$\", xlabel=\"$x$\", ylabel=\"$f(x)$\")\n    return fig, ax\n\n\ndef plot_truth(ax: matplotlib.axes.Axes) -&gt; None:\n    \"\"\"\n    Plots the true root of a function as a marker on the graph.\n\n    Parameters:\n        ax (matplotlib.axes.Axes): The matplotlib axes on which the marker will be plotted.\n\n    Returns:\n        None\n    \"\"\"\n    ax.plot(-np.pi,\n            0,\n            \"*\",\n            markersize=15,\n            color=\"darkblue\",\n            label=\"True root, $-\\pi$\")\n\n\ndef plot_guess(\n        ax: matplotlib.axes.Axes,\n        guess: int,\n        label: str) -&gt; None:\n    \"\"\"\n    Plots a guess or estimate as a marker on the graph.\n\n    Parameters:\n        ax (matplotlib.axes.Axes): The matplotlib axes on which the marker will be plotted.\n        guess (int): The estimated value to be marked on the graph.\n        label (str): Label for the marker.\n\n    Returns:\n        None\n    \"\"\"\n    ax.plot(guess,\n            0,\n            \"o\",\n            label=label)\n\n\ndef plot_guess_coords(\n        ax: matplotlib.axes.Axes,\n        guess: int,\n        label: str,\n        y: sympy.core.mul.Mul = y,\n        x: sympy.core.mul.Mul = x):\n    \"\"\"\n    Plots a guess or estimate with specific coordinates as a marker on the graph.\n\n    Parameters:\n        ax (matplotlib.axes.Axes): The matplotlib axes on which the marker will be plotted.\n        guess (int): The estimated x-coordinate where the marker will be placed.\n        label (str): Label for the marker.\n        y (sympy.core.mul.Mul): The mathematical expression corresponding to the y-coordinate.\n        x (sympy.core.mul.Mul): The symbol representing the independent variable (x).\n\n    Returns:\n        None\n    \"\"\"\n    ax.plot(guess,\n            y.subs(x, guess).evalf(),\n            \"s\",\n            label=label, color=\"black\")\n\n\ndef euclidean_dist_to_truth(x): return np.sqrt((-np.pi - float(x))**2)\n\n\nNow we’ll begin the process of using Newton’s Method to arrive at that same answer of \\(x=-\\pi\\). #### Step 1: Make a first guess and evaluate its (x, y) coordinates For a first guess, it’s typical to start close to 0. In our case, we’ll try -2.\n\nx_0 = -2\n\nLet’s get a sense of how good of a guess this is by plotting it\n\\[\n\\begin{align*}\nf(x) &= 4 \\sin \\left( x \\right) \\\\\\\nf(x_0) &= 4 \\sin \\left( -2 \\right) \\\\\\\nf(x_0) &\\approx −3.6371897 \\\\\\\n(x_0, y_0) &= (x_0, f(x_0)) \\approx (-2, −3.6371897)\n\\end{align*}\n\\]\n\nfig, ax = base_plot(y, x, x_low=-5, x_high=-1)\nplot_truth(ax)\nplot_guess(ax, x_0, \"$x_0$ (initial guess)\")\nplot_guess_coords(ax, x_0, \"$(x_0, f(x_0))$\")\nax.legend();\n\n\n\n\n\n\n\n\nWe can assess the quality of that guess by calculating the distance from the guess to the right answer (\\(-\\pi\\)):\n\nprint(f\"error for guess x_0:\", euclidean_dist_to_truth(x_0))\n\nerror for guess x_0: 1.1415926535897931\n\n\n\nStep 2: Find the equation of the tangent line at those coordinates\nWe will proceed to improve upon that initial guess by computing the linear approximation of the function at that point, then retrieve its root to make a next-guess. Our first guess wasn’t based on any relevant information other than the domain of our search (between -5 and 0). Our next guess is going to be better-informed, as it is an estimate based on an approximation of the function.\n(Note that the linear approximation at this point is typically referred to as a tangent line, and I’ll use those two phrases interchangeably.)\nWe compute the tangent by first differentiating the function and plugging in our previous guess. This yields the slope of the tangent line: \\[\n\\begin{align*}\nf(x) &= 4 \\sin \\left( x \\right) \\\\\\\nf'(x) &= 4 \\cos \\left( x \\right) \\\\\\\nf'(x_0) &= 4 \\cos \\left( -2 \\right) \\\\\\\nf'(x_0) &\\approx −1.6645873\n\\end{align*}\n\\]\nI’ll note that sympy is capable of doing all of these routine calculations as well:\n\nprint(\"   f(x) = \", y)\nprint(\"  f'(x) = \", y.diff())\nprint(\"f'(x_0) = \", y.diff().subs(x, -2))\nprint(\"[note: sympy converted x_0=-2 to x_0=2 because cos(-x)=cos(x)]\")\nprint(\"f'(x_0) = \", y.diff().subs(x, x_0).evalf())\n\n   f(x) =  4*sin(x)\n  f'(x) =  4*cos(x)\nf'(x_0) =  4*cos(2)\n[note: sympy converted x_0=-2 to x_0=2 because cos(-x)=cos(x)]\nf'(x_0) =  -1.66458734618857\n\n\nNow, given each of these terms:\n\n\n\nTerm (math)\nValue\n\n\n\n\n\\(x_0\\) (X, Last Guess)\n\\(= -2\\)\n\n\n\\(f(x_0)\\) (Y at \\(x_0\\))\n\\(\\approx −3.63719\\)\n\n\n\\(f'(x_0)\\) (Slope of tangent at \\(x_0\\))\n\\(\\approx −1.66459\\)\n\n\n\nWe can proceed to find the full equation of the tangent line by writing out the point-slope form of a linear equation with slope \\(m=f'(x_0)\\). \\[\n\\begin{align*}\n(y - y_0) &= m(x - x_0) \\\\\n(y - f(x_0)) &= f'(x_0)(x - x_0) \\\\\ny &= f'(x_0)(x - x_0) + f(x_0)\n\\end{align*}\n\\] Plugging in our values, we get:\n\\[\n\\begin{align*}\ny &\\approx −1.66459x + (1.66459)(-2) - 3.63719 \\\\\ny &\\approx −1.66459x - 6.966364 \\\\\n\\end{align*}\n\\]\nWe’ll save that into a python function and plot it to make sure it does look like the tangent.\n\ndef f_1(x): return -1.6645873*x - 6.9663643\n\n\nfig, ax = base_plot(y, x, x_low=-5, x_high=-1)\nplot_truth(ax)\n\nplot_f(f_1, x_low=-5, x_high=-1, ax=ax)\n\nplot_guess(ax, x_0, \"$x_0$ (initial guess)\")\nplot_guess_coords(ax, x_0, \"$(x_0, f(x_0))$\")\n\nax.legend();\n\n\n\n\n\n\n\n\n\n\nStep 3: Find the x-intercept of the tangent line\nGiven that the tangent line is the best linear approximation of the original function, we can use its x-intercept as an approximation of the x-intercept of the original function. Thus, the root of the tangent line is the new “best-guess” of the original function’s root.\n\\[\n\\begin{align*}\n0 &\\approx −1.6645873x_1 - 6.9663643 \\\\\n\\frac{6.9663643}{−1.6645873} &\\approx x_1 \\\\\nx_1 &\\approx -4.1850\n\\end{align*}\n\\]\n\nx_1 = -4.1850\n\n\nfig, ax = base_plot(y, x, x_low=-5, x_high=-1)\nplot_truth(ax)\nplot_guess(ax, x_0, \"$x_0$ (initial guess)\")\nplot_f(f_1, x_low=-5, x_high=-1, ax=ax)\nplot_guess(ax, x_1, \"$x_1$ (next guess)\")\nax.legend();\n\n\n\n\n\n\n\n\nWhile this guess still isn’t particularly great, we can see that we have actually reduced the “error” of our guess:\n\nfor i, x_n in enumerate([x_0, x_1]):\n    print(f\"error for guess x_{i}:\", euclidean_dist_to_truth(x_n))\n\nerror for guess x_0: 1.1415926535897931\nerror for guess x_1: 1.0434073464102065\n\n\nThe big reveal of Newton-Raphson is that this error will continue to shrink as we repeat steps 1-3.\n\n\nStep 4: Repeat\nWe will again find the tangent line at this new point, \\((x_1, f(x_1))\\). We could take the old tangent line equation, \\(y = f'(x_0)(x - x_0) + f(x_0)\\) and simply update all of those \\(x_0\\) to \\(x_1\\), but at this point it will benefit us to move towards a more general equation: \\[\ny = f'(x_n)(x - x_n) + f(x_n)\n\\] This allows us to generate the tangent line at any given guess, \\(x_n\\). The following code leverages sympy to write that equation as a python function.\n\ndef y_n(x_n): return (\n    y.diff().subs(x, x_n) * (x - x_n) +  # f_1(x_1)(x-x_1) +\n    y.subs(x, x_n)  # f(x_1)\n)\n\n\ny_n(x_1)\n\n\\(\\displaystyle - 2.01311525745113 x - 4.96839101072835\\)\n\n\nWe’ll also use sympy to easily solve for the new tangent line’s x-intercept, \\(x_2\\)\n\nx_2 = sympy.solve(y_n(x_1), x)[0]\nx_2\n\n\\(\\displaystyle -2.46801120419652\\)\n\n\n\nfig, ax = base_plot(y, x, x_low=-5, x_high=-1)\nplot_truth(ax)\nplot_f(sympy.lambdify(x, y_n(x_1)), x_low=-5, x_high=-1, ax=ax)\n\nplot_guess_coords(ax, x_1, \"$(x_1, f(x_1))$\")\n\nplot_guess(ax, x_0, \"$x_0$ (initial guess)\")\nplot_guess(ax, x_1, \"$x_1$ (previous guess)\")\nplot_guess(ax, x_2, \"$x_2$ (next guess)\")\n\nax.legend();\n\n\n\n\n\n\n\n\nWe can verify that this new guess again reduces our “error,” which should encourage us to continue this process.\n\nfor i, x_n in enumerate([x_0, x_1, x_2]):\n    print(f\"error for guess x_{i}:\", euclidean_dist_to_truth(x_n))\n\nerror for guess x_0: 1.1415926535897931\nerror for guess x_1: 1.0434073464102065\nerror for guess x_2: 0.6735814493932737\n\n\n\n\nStep 5: Generalize the procedure\n\nA.) Make the equation more direct\nThus far we have used the general equation \\(y = f'(x_n)(x - x_n) + f(x_n)\\), where \\(x_n\\) is our current guess, and we solve for \\(x\\) to define our next guess. Given that we solve the equation for \\(x\\), we can rewrite it as follows:\n\\[\n\\begin{align*}\n0 &= f'(x_n)(x - x_n) + f(x_n) \\\\\n0 &= f'(x_n)(x - x_n) + f(x_n) \\\\\n0 - f(x_n) &= f'(x_n)(x - x_n) \\\\\n-\\frac{f(x_n)}{f'(x_n)} &= x - x_n \\\\\nx &= x_n -\\frac{f(x_n)}{f'(x_n)} \\\\\n\\end{align*}\n\\] This expresses one step of Newton’s Method – solving for the x-intercept of the tangent line at the point \\((x_n, f(x_n))\\).\n\n\nB.) Move from equation to algorithm\nWe now build on this single step and express the general process of Newton’s method. To start, it’s more accurate to label the left hand side \\(x_{n+1}\\) given that it represents the next guess: \\[\nx_{n+1} = x_n -\\frac{f(x_n)}{f'(x_n)}\n\\]\nGiven this equation, we can think of Newton’s Method as essentially searching for good guesses – defining new \\(x_{n+1}\\) – until it’s right. But how do we define “right”? Put in other words, when do we stop?\nIn our case, we are working with a simple function and we know the correct answer – we can just stop once we get close to \\(-\\pi\\) – but in any real application that won’t be the case. In those cases, it is common practice to define “right” as when the guesses stop changing much with each iteration. Stated semi-formally, we wait until \\(|x_{n+1} - x_{n}|\\) gets small.\nFor example: if the last guess was -3.14159 and the new guess is -3.141592, the guess only changed by .0000002, and we might conclude that we’ve gotten as close to the answer as is necessary. In this case, we set a stopping condition – when the next guess is less than or equal to .0000002 away from the previous one, we stop. We can write out the stopping condition as follows:\n\\[\n\\begin{align*}\n|x_{n+1} - x_{n}| &\\leq 2\\times 10^{-7} \\\\\n|(x_n -\\frac{f(x_n)}{f'(x_n)}) - x_{n}| &\\leq 2\\times 10^{-7} \\\\\n|-\\frac{f(x_n)}{f'(x_n)}| &\\leq 2\\times 10^{-7} \\\\\n|\\frac{f(x_n)}{f'(x_n)}| &\\leq 2\\times 10^{-7}\n\\end{align*}\n\\]\nWe can try writing out the recursive algorithm as a piece-wise equation:\n\\[\n\\begin{align*}\n\\text{Let } x_0 := \\text{initial guess, } \\\\\n\\text{For all natural numbers } n \\ge 0, \\\\\n\\text{Define }  x_{n+1} \\text{ as:}\n\\end{align*}\n\\]\n\\[\nx_{n+1} = \\begin{cases}\n        x_n & \\text{if }\\quad |\\frac{f(x_n)}{f'(x_n)}| \\leq 2\\times 10^{-7} \\\\\n        x_n -\\frac{f(x_n)}{f'(x_n)} & \\text{Otherwise}\n        \\end{cases}\n\\]\nHowever, now that we are moving into the realm of algorithms, I think it’s clearer to write this as code:\n\nx_n = -2\n\n# We'll use this to count which guess we are on\ncounter = 1\nwhile True:\n    # Disregard the following utility code\n    print(f\"Guess {counter}:\",\n          str(round(float(x_n), 5)).ljust(8, '0'),\n          \" --- Error:\",\n          euclidean_dist_to_truth(x_n))\n    ################################################\n    # The following is the code for newton's method\n    # 1.) Check for the stopping condition,\n    # |f(x_n)/f'(x_n)| &lt; 2 * 10^-7\n    stop_condition = (\n        np.abs(sympy.lambdify(x, y)(x_n) /\n               sympy.lambdify(x, y.diff())(x_n))\n        &lt; 2e-7\n    )\n    if stop_condition:\n        print(f\"Converged in {counter} steps.\")\n        break\n    # 2.) If stopping condition not met, make a new guess\n    x_n = x_n - (\n        # f(x_n) /\n        sympy.lambdify(x, y)(x_n) /\n        # f'(x_n)\n        sympy.lambdify(x, y.diff())(x_n)\n    )\n    ################################################\n    # Update the counter\n    counter += 1\n\nGuess 1: -2.00000  --- Error: 1.1415926535897931\nGuess 2: -4.18504  --- Error: 1.0434472096717258\nGuess 3: -2.46789  --- Error: 0.6736989790751275\nGuess 4: -3.26619  --- Error: 0.1245936239793135\nGuess 5: -3.14094  --- Error: 0.00064874127215786\nGuess 6: -3.14159  --- Error: 9.101119857746198e-11\nConverged in 6 steps.\n\n\nWe’ve converged at our best-guess after six steps, which we can see animated below.\n\n\nCode\n%%capture\nx_n = -2\nmax_iter = 10\ntolerance = 1e-6\n\nfig, ax = base_plot(y, x, x_low=-5, x_high=-1)\nplot_truth(ax)\nax.set_ylim(-4, 4)\nax.set_xlim(-4.5, -1.5)\n\nimages = []\nbreaking_condition = False\nfor j in range(max_iter):\n    error = euclidean_dist_to_truth(x_n)\n    if error &lt; tolerance:\n        breaking_condition = True\n\n    ax.set_title(\n        f\"Iteration {j+1}\\nGuess: {round(x_n, 6)}\\n Error: {round(error, 8)}\")\n\n    # Plot the current guess\n    plot_guess(ax, x_n, \"$x_n$\")\n    filename = f'newton_iteration_{j}_0.png'\n    fig.savefig(filename)\n    images.append(imageio.imread(filename))\n\n    if breaking_condition:\n        ax.text(x_n, 2, s=f\"CONVERGED IN {j} STEPS\", size=15)\n        filename = f'newton_iteration_{i}_3.png'\n        fig.savefig(filename)\n        images.append(imageio.imread(filename))\n        ax.text(x_n, 2, s=f\"CONVERGED IN {j} STEPS\", size=15)\n        filename = f'newton_iteration_{i}_4.png'\n        fig.savefig(filename)\n        images.append(imageio.imread(filename))\n        break\n    # Plot the coordinates of the current guess\n    plot_guess_coords(ax, x_n, \"$(x_n, f(x_n))$\")\n    filename = f'newton_iteration_{j}_1.png'\n    fig.savefig(filename)\n    images.append(imageio.imread(filename))\n    # Plot the tangent line of that coordinate to inform next guess\n    plot_f(sympy.lambdify(x, y_n(x_n)), x_low=-5, x_high=-1, ax=ax)\n    filename = f'newton_iteration_{j}_2.png'\n    fig.savefig(filename)\n    images.append(imageio.imread(filename))\n\n    # Reset plot\n    ax.clear()\n    fig, ax = base_plot(y, x, x_low=-5, x_high=-1)\n    plot_truth(ax)\n    ax.set_ylim(-4, 4)\n    ax.set_xlim(-4.5, -1.5)\n\n    x_n -= (\n        sympy.lambdify(x, y)(x_n) /\n        sympy.lambdify(x, y.diff())(x_n)\n    )\nimageio.mimsave('newton_iterations.gif', images, duration=1)\n\n# Clean out pngs\nwd = os.getcwd()\nfiles = os.listdir(wd)\nfor item in files:\n    if item.endswith(\".png\"):\n        os.remove(os.path.join(wd, item))\n\n\n\n\n\n\n\nUsing Newton’s Method to Solve a Real Problem\nIn the previous example, we dealt with a function, \\(f(x) = 4\\sin(x)\\) with a well known analytical solution for its x-intercept. Other simple functions can typically be solved with known formulas – e.g. a second degree polynomial’s roots can be found using the quadratic formula. In cases of known analytical solutions or readily available root-finding formulas, there is no reason to use Newton’s Method beyond as a learning exercise.\nHowever, many functions do not have have readily available methods for finding the root. For example, if \\(f(x)\\) is a polynomial of degree 5 or greater, it is known that no formula for finding its roots exist (Strang and Herman 2016). Consider the following polynomial of degree 5: \\[\nf(x) =x^{5} + 8 x^{4} + 4 x^{3} - 2 x - 7\n\\] What if we are asked to solve the following: \\[\nf(x)=0 \\quad \\text{where} -5\\leq x \\leq 0\n\\] There is no formula that solves this. Even plugging the polynomial into sympy and running its solver, sympy.solveset, doesn’t give a clear answer.\n\nx, y = sympy.symbols('x y')\ny = x**5 + 8*x**4 + 4*x**3 - 2*x-7\ny\n\n\\(\\displaystyle x^{5} + 8 x^{4} + 4 x^{3} - 2 x - 7\\)\n\n\n\nprint(sympy.solveset(y, x, sympy.Interval(-5, 0)))\n\n{CRootOf(x**5 + 8*x**4 + 4*x**3 - 2*x - 7, 1)}\n\n\n(where CRootOf is an indexed complex root of the polynomial – not an analytical solution)\nWe might proceed to visually inspect the function on this domain – but it’s even pretty hard to visually spot a root here!\n\nx_vec = np.linspace(-5, 0, 1000000)\nfig, ax = plt.subplots()\nax.grid(alpha=.5)\nax.plot(x_vec, sympy.lambdify(x, y)(x_vec))\nax.set_title(f\"$y={sympy.latex(y)}$\");\n\n\n\n\n\n\n\n\nHere’s a good use-case for Newton’s Method. I set up the algorithm with \\(x_0=0\\) and a stopping condition that \\(|x_{n+1} - x_{n}| \\leq 10^{-12}\\).\nI quickly converge at an answer:\n\nx_n = 0\ntolerance = 1e-12\ncounter = 1\n\nlast = np.inf\nwhile True:\n    # Disregard the following utility code\n    print(\"Guess {0:3}:\".format(counter),\n          str(round(float(x_n), 5)).ljust(8, '0'),\n          \" --- Change:\",\n          round(np.abs(float(x_n) - last), 8))\n    last = float(x_n)\n    ################################################\n    # The following is the code for newton's method\n    # 1.) Check for the stopping condition,\n    # |f(x_n)/f'(x_n)| &lt; 2 * 10^-7\n    stop_condition = (\n        np.abs(sympy.lambdify(x, y)(x_n) /\n               sympy.lambdify(x, y.diff())(x_n))\n        &lt; tolerance\n    )\n    if stop_condition:\n        print(f\"Converged in {counter} steps.\")\n        break\n    # 2.) If stopping condition not met, make a new guess\n    x_n = x_n - (\n        # f(x_n) /\n        sympy.lambdify(x, y)(x_n) /\n        # f'(x_n)\n        sympy.lambdify(x, y.diff())(x_n)\n    )\n    ################################################\n    counter += 1\n\nGuess   1: 0.000000  --- Change: inf\nGuess   2: -3.50000  --- Change: 3.5\nGuess   3: -2.44316  --- Change: 1.05683755\nGuess   4: -1.81481  --- Change: 0.6283498\nGuess   5: -1.41471  --- Change: 0.40010602\nGuess   6: -1.19062  --- Change: 0.22409096\nGuess   7: -1.11070  --- Change: 0.07991323\nGuess   8: -1.10108  --- Change: 0.0096197\nGuess   9: -1.10095  --- Change: 0.00012977\nGuess  10: -1.10095  --- Change: 2e-08\nConverged in 10 steps.\n\n\nWhen we plot our best guess, \\(x_n\\), we see that it is indeed the root of the function.\n\nfig, ax = plt.subplots()\n\nax.axvline(x_n, linestyle=\"--\", color=\"tab:red\",\n           linewidth=2, label=\"Final Guess\")\nax.legend()\n\nx_vec = np.linspace(-1.5, 0, 1000000)\n\nax.grid(alpha=.5)\nax.plot(x_vec, sympy.lambdify(x, y)(x_vec))\nax.set_title(f\"$y={sympy.latex(y)}$\")\nax.axhline(0, color=\"black\", alpha=.5);\n\n\n\n\n\n\n\n\nWe can also compare our answer to what sympy gets from numerically solving for the function’s root:\n\nprint(\"Newton's Method:\", x_n)\n\nNewton's Method: -1.1009529619409872\n\n\n\nprint(\"Sympy's answer:\", sympy.solveset(y, x, sympy.Interval(-5, 0)).evalf())\n\nSympy's answer: {-1.10095296194099}\n\n\nWe see that we have basically the same answer as sympy."
  },
  {
    "objectID": "posts/newtons-method/index.html#newtons-method-as-optimization",
    "href": "posts/newtons-method/index.html#newtons-method-as-optimization",
    "title": "Newton’s Method From The Ground Up",
    "section": "Newton’s Method as Optimization",
    "text": "Newton’s Method as Optimization\nRecall that the derivative of a function is 0 at a critical point – its maximum or minimum. We have been using Newton’s Method to find the root of a function, but in the process we’ve also implicitly been finding the critical point of the anti-derivative, or, integral of that function.\nFor visual intuition of this fact, consider the following plot, in which we visualize the integral of our function, \\(\\int f(x) dx\\), and plot the root of \\(f(x)\\) that we just found using Newton’s Method:\n\nfig, ax = plt.subplots()\n\nax.axvline(x_n, linestyle=\"--\", color=\"tab:red\",\n           linewidth=2, label=\"Final Guess\")\n\nax.grid(alpha=.5)\nax.plot(x_vec, sympy.lambdify(x, y)(x_vec), label=\"$f(x)$\")\nax.plot(x_vec, sympy.lambdify(x, y.integrate())(x_vec), label=\"$\\int f(x) dx$\")\nax.legend()\nax.set_ylim(-5, 7)\nax.set_title(f\"$\\int ({sympy.latex(y)})dx$\");\n\n\n\n\n\n\n\n\nThe root of \\(f(x)\\) seems to also be the maximum of \\(\\int f(x) dx\\).\nWhy is this important? Based on this idea, we can extend Newton’s Method for general use in finding the critical value of a function, which means that we can use it for solving optimization problems. We can setup the optimization approach as follows:\nIf the following equation will converge to the root of \\(f(x)\\) and the critical point of \\(\\int f(x) dx\\): \\[\n\\begin{align*}\nx_{n+1} = x_n -\\frac{f(x_n)}{f'(x_n)}\n\\end{align*}\n\\] Then the following equation will converge to the root of \\(f'(x)\\) and thus the critical point of \\(f(x)\\) (equivalent to \\(\\int f'(x)dx\\)): \\[\n\\begin{align*}\nx_{n+1} = x_n -\\frac{f'(x_n)}{f''(x_n)}\n\\end{align*}\n\\]\n(“Newton’s Method” 2023, “Minimization and maximization problems”)"
  },
  {
    "objectID": "posts/pdf-scraping/index.html",
    "href": "posts/pdf-scraping/index.html",
    "title": "Reliable PDF Scraping with tabula-py",
    "section": "",
    "text": "Summary\n\nUse a combination of tabula‘s read_pdf() function and pandas’ various data manipulation functions in Python to accurately scrape .pdf files\n\n\n\nPrerequisites/Assumptions\n\nWindows 10 with administrator privileges (for setting environmental variables)\nJava SE Development Kit installed on your machine (download)\n\nset Java’s PATH environmental variable to point to the Java directory (see more here under “Get tabula-py working (Windows 10)”)\n\n\nPython version ~3.8 ish (I’m using Python 3.9.12 in Anaconda)\n\nAnaconda included packages - Pandas and NumPy\nLibraries maybe not included in Anaconda: requests, tabula-py\n\n\n\n\nProblem Narrative\nI’m interested in conducting a data analysis that involves the market value of single family homes in San Mateo County, California. This data can be hard to come by, but I’ve found a good county level resource – The San Mateo Association of Realtors’ “Market Data” page.\n\n\n\n\n\n\n\n\nFig 1: San Mateo Realtors Data Download Page\n\n\n\nHowever, to my dismay, I find that when I download one of these reports, I only get a .pdf containing a single table. It seems to be some sort of export of an Excel table, but the Association of Realtors has not made the actual spreadsheet available. Here is an example of one of their .pdf reports – in this case for April 2022:\n\n\n\n\n\n\n\n\nFig 2: Example PDF report :(\n\n\n\nThis is the exact data I want, but there are a few key issues: - The data are in .pdf files - You can only download monthly data files one at a time\n\n\nSolution\nI’ll solve this issue by writing a script to do the following: - Iterate through the urls of each of the monthly reports going back to 2011. For each report: - download its .pdf - parse and save the data from its .pdf\nStart with loading in the necessary libraries:\nimport pandas as pd\nimport numpy as np\nimport requests\nfrom tabula import read_pdf\nGet right into the script, which implements the pseudo-code I outlined above:\nagent = ['Mozilla/5.0 (Windows NT 10.0; Win64; x64)',\n         'AppleWebKit/537.36 (KHTML, like Gecko)',\n         'Chrome/91.0.4472.114 Safari/537.36']\nrows = []\nheaders = {'user-agent': \" \".join(agent)}\nfor year in range(2011, 2021):\n    for month in range(1, 12):\n        base = \"https://www.samcar.org/userfiles/file/salesstats/\"\n        url = base + f\"SF_{year}{str(month).zfill(2) }.pdf\"\n        print(url)\n        r = requests.get(url,\n                         stream=True,\n                         headers=headers)\n        open('holder.pdf', 'wb').write(r.content)\n        df = read_pdf(\"holder.pdf\", pages=\"all\")\n        table = df[0].iloc[-1, :]\n        table[\"date\"] = f\"{year}-{str(month).zfill(2)}\"\n        rows.append(table)\nNote: I’m defining agent in order to preempt being blocked by the site (read more).\nWhat’s remarkable about tabula.read_pdf() is that it just works. I didn’t have to really do any tinkering or iterating to get it going. Once it had access to the downloaded .pdf files, it easily and quickly parsed them.\nNow I run into something unique to this data – some of the .pdf tables had slightly different column names over the years. I implement a fix for that with the following code:\ncleaned_rows = []\nfor row in rows:\n    try:\n        new = row.rename(\n            {\"Sales\": \"Closed Sales\",\n             \"Sold\": \"Closed Sales\",\n             \"Avg Sales Price\": \"Average Sales Price\",\n             \"Avg SalePrice\": \"Average Sales Price\",\n             \"Unnamed: 3\": \"Closed Sales\",\n             \"Unnamed: 5\": \"Average Sales Price\"})[[\"date\",\n                                                    \"Closed Sales\",\n                                                    \"Average Sales Price\"]]\n        cleaned_rows.append(new.to_frame())\n    except KeyError:\n        print(\"******error\")\nWith the data retrieved and parsed, I perform some final cleaning and arrangement steps before exporting to a .csv\nall_years = pd.concat(cleaned_rows, axis=1)\n# Transpose the data and set `date` as the index\nfinal_df = all_years.T.set_index(\"date\")\n# Get the dollar signs and commas out. E.g. $1,658,900 -&gt; 1658900\nfinal_df[\"Average Sales Price\"] = (final_df[\"Average Sales Price\"]\n                                   .str.replace(\"[\\$,]\",\n                                                \"\",\n                                                regex=True)\n                                   .astype(int))\n# Closed Sales is discrete count data, so we convert to `int`\nfinal_df[\"Closed Sales\"] = final_df[\"Closed Sales\"].astype(int)\nfinal_df.to_csv(\"realtors_data_san_mateo.csv\")\nThe final product is a satisfying time series data set of the number of closed single family home sales and the average price of those sales over time.\n\n\n\n\n\n\n\n\nFig 3: Final .csv open in Excel\n\n\n\nThat’s surprisingly it.\n\n\nConclusion\nTabula-py is a very convenient and powerful .pdf parser (ported from Java) and easily handled basically all of the .pdfs I put through it."
  },
  {
    "objectID": "posts/simple-constrained-optimization/index.html",
    "href": "posts/simple-constrained-optimization/index.html",
    "title": "Simple Constrained Optimization in 2D",
    "section": "",
    "text": "Code\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nThe following are my notes on a basic calculus 1 homework question. I liked the question a lot, so decided to write out it all out for my future use."
  },
  {
    "objectID": "posts/simple-constrained-optimization/index.html#a-note-on-the-second-derivative-test",
    "href": "posts/simple-constrained-optimization/index.html#a-note-on-the-second-derivative-test",
    "title": "Simple Constrained Optimization in 2D",
    "section": "A note on the second derivative test",
    "text": "A note on the second derivative test\nWhen we determined \\(V''(x^\\star)&lt;0\\), we determined that coming off of this critical value, the slope of the function is decreasing – we are coming down from a maximum. In the figure below, the critical value is plotted as a black point. It’s clear that if we were to move slightly to the right of the critical value, the slope of the function would decrease, and we see this directly in the plots of the first and second derivatives.\n\n\nCode\nfig, ax = plt.subplots(1, 3)\nax[0].plot(x, V(x))\nax[0].grid(alpha=.5)\nax[0].set_title(r\"$V(x^*) = 8606.62$\")\nax[0].set_ylim(8500,8650)\nax[0].set_xlim(24, 28)\nax[0].plot(x_critical, V(x_critical), \"&gt;\", color=\"grey\", markeredgecolor=\"black\")\n\nax[1].axhline(0, color=\"grey\", linestyle=\"--\")\nax[1].plot(x, V_1(x), c=\"tab:blue\")\nax[1].grid(alpha=.5)\nax[1].set_xlabel(r\"$x$\")\nax[1].set_title(r\"$V'(x^*) = 0$\")\nax[1].set_xlim(20, 30)\nax[1].plot(x_critical, V_1(x_critical), \"&gt;\", color=\"grey\", markeredgecolor=\"black\")\n\nax[2].axhline(0, color=\"grey\", linestyle=\"--\")\nax[2].plot(x[V_2(x) &gt; 0], V_2(x)[V_2(x) &gt; 0], c=\"tab:green\")\nax[2].plot(x[V_2(x) &lt; 0], V_2(x)[V_2(x) &lt; 0], c=\"tab:red\")\nax[2].grid(alpha=.5)\nax[2].set_title(r\"$V''(x^*) \\approx -39$\")\nax[2].plot(x_critical, V_2(x_critical), \"o\", alpha=1, color=\"red\", markeredgecolor=\"black\")\n\nfig.tight_layout();\n\n\n\n\n\n\n\n\n\nExpanding on this point, consider another arbitrary function that has more than one critical point, \\(f(x)=2x^3 - 100x^2\\). In this case the function has one maximum and one minimum, so the second derivative test will be more important for analyzing each point. The function is plotted below, along with its first and second derivative.\n\n\nCode\nf = lambda x: 2*x**3 - 100*x**2\nf_1 = lambda x: 6*x**2 - 200*x\nf_2 = lambda x: 12*x - 200\n\nx = np.linspace(-25, 50, 100)\n\nfig, ax = plt.subplots(1, 3)\nax[0].plot(x, f(x))\nax[0].grid(alpha=.5)\nax[0].plot(100/3, f(100/3), \"&gt;\", color=\"grey\", markeredgecolor=\"black\")\nax[0].plot(0, f(0), \"&gt;\", color=\"grey\", markeredgecolor=\"black\")\n\nax[0].set_title(r\"$f(x)$\")\n\nax[1].axhline(0, color=\"grey\", linestyle=\"--\")\nax[1].plot(x, f_1(x), c=\"tab:blue\")\nax[1].grid(alpha=.5)\nax[1].plot(100/3, f_1(100/3), \"&gt;\", color=\"grey\", markeredgecolor=\"black\")\nax[1].plot(0, f_1(0), \"&gt;\", color=\"grey\", markeredgecolor=\"black\")\n\nax[1].set_title(r\"$f'(x)$\")\n\nax[2].axhline(0, color=\"grey\", linestyle=\"--\")\nax[2].plot(x[f_2(x) &gt; 0], f_2(x)[f_2(x) &gt; 0], c=\"tab:green\")\nax[2].plot(x[f_2(x) &lt; 0], f_2(x)[f_2(x) &lt; 0], c=\"tab:red\")\n\nax[2].grid(alpha=.5)\nax[2].plot(100/3, f_2(100/3), \"o\", color=\"green\", markeredgecolor=\"black\")\nax[2].plot(0, f_2(0), \"o\", color=\"red\", markeredgecolor=\"black\")\n\nax[2].set_title(r\"$f''(x)$\")\n\nfig.suptitle(r\"$f(x) = 2x^3 - 100x^2$\")\n\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nIf we were to want to maximize this function by taking the first derivative and solving for 0, we would find two critical values – a maximum and a minimum. In this case, the second derivative test would be used to conclude which of these points is the maximum and which is the minimum. We substitute each critical value into the second derivative- \\(f''(x^\\star)&lt;0\\) denotes the maximum (the slope of the function is decreasing off of this point), while \\(f''(x^\\star)&gt;0\\) denotes the maximum (the slope of the function is increasing off of this point)."
  }
]