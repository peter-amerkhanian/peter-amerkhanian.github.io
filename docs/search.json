[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "The following are some of my more formal public-facing projects:\nA Tale of Two Cities: Alameda and Alameda Point\nPeter Amerkhanian (2023), Report for The City of Alameda Department of Finance.\nSummary: I integrate internal and external data sources to 1.) Create a historical narrative of re-development in the Alameda Point region and 2.) Project the future benefits/costs of development alternatives in the region.\n\nA Reexamination of Proposition 13 Using Parcel Level Data\nPeter Amerkhanian, Max Zhang, and James Hawkins (2023), Berkeley Institute for Young Americans Report.\nSummary: We utilize 12 million property records in California and a fixed-effects model to estimate the discount effect of Prop 13 across property types."
  },
  {
    "objectID": "projects.html#professional-reports",
    "href": "projects.html#professional-reports",
    "title": "Projects",
    "section": "",
    "text": "The following are some of my more formal public-facing projects:\nA Tale of Two Cities: Alameda and Alameda Point\nPeter Amerkhanian (2023), Report for The City of Alameda Department of Finance.\nSummary: I integrate internal and external data sources to 1.) Create a historical narrative of re-development in the Alameda Point region and 2.) Project the future benefits/costs of development alternatives in the region.\n\nA Reexamination of Proposition 13 Using Parcel Level Data\nPeter Amerkhanian, Max Zhang, and James Hawkins (2023), Berkeley Institute for Young Americans Report.\nSummary: We utilize 12 million property records in California and a fixed-effects model to estimate the discount effect of Prop 13 across property types."
  },
  {
    "objectID": "projects.html#academic-papers",
    "href": "projects.html#academic-papers",
    "title": "Projects",
    "section": "Academic Papers",
    "text": "Academic Papers\nThe following are some of my academic papers from graduate school:\nSimulating School Desegregation in San Francisco\nPeter Amerkhanian (2022), Public Policy 275 Final Paper (A+).\nSummary: I use a synthetic dataset of San Francisco public high school students and spatial optimization methods to simulate the effects of various busing strategies for racial desegregation outcomes. Repo. \n\nMeasuring Differences in California Politician Agendas in Press Releases\nPeter Amerkhanian (2021), Information 254 Final Paper (A).\nSummary: I use a novel dataset of press releases issued by governors and mayors in California to 1. Develop a regression model to identify press release authorship, 2. Cluster press releases by topic, and 3. Estimate the political similarities between mayors and governors."
  },
  {
    "objectID": "posts/rent-vs-buy/index.html",
    "href": "posts/rent-vs-buy/index.html",
    "title": "Buy vs. Rent, A Financial Modeling Workflow in Python",
    "section": "",
    "text": "This post goes through the following exercises:\n\nUse numpy-financial to build a loan amortization calculator for a home mortgage\n\nUse said table as well as simulated home and stock equity returns over time to compare year-to-year wealth resulting from the following strategies:\n\nbuying a residential living space\n\nrenting one instead and investing the dollar amount that would have been your down-payment\n\n\n\n\nAt one point in time, numpy, the popular Python numerical analysis library, included 10 specialized functions for financial analysis. Given their specific nature, they were eventually removed from numpy, I think in 2019 (learn about why that is here) and are now available in the separate library, numpy-financial. The library still seems focused on the same 10 core functions, which handle tasks like calculating loan payment amounts given some inputs, and applied financial economics tasks like calculating time value of money. Cool… Anyways, I’ll use it to create an amortization schedule for a mortgage.\n\n\n\nI built this notebook in a Google Colab instance, which seems to include most major Python libraries (more info).\nYou’ll probably have to download numpy-financial (it’s not included in Anaconda as far as I know), which you can accomplish within any notebook-like environment using the following command:\n\n!pip install numpy-financial\n\nYou’ll want to load the usual suspects - pandas, numpy, seaborn, matplotlib. I also run from datetime import datetime since we will be working with ranges of dates, and I run sns.set_style() to get my seaborn plots looking a bit more aesthetically pleasing - read more on themes here.\n\nimport pandas as pd\nimport numpy as np\nimport numpy_financial as npf\nfrom datetime import datetime\n\nimport seaborn as sns\n# set seaborn style\nsns.set_style(\"white\")\n\nimport matplotlib.pyplot as plt\n# Set Matplotlib font size\nplt.rcParams.update({'font.size': 14})"
  },
  {
    "objectID": "posts/rent-vs-buy/index.html#summary",
    "href": "posts/rent-vs-buy/index.html#summary",
    "title": "Buy vs. Rent, A Financial Modeling Workflow in Python",
    "section": "",
    "text": "This post goes through the following exercises:\n\nUse numpy-financial to build a loan amortization calculator for a home mortgage\n\nUse said table as well as simulated home and stock equity returns over time to compare year-to-year wealth resulting from the following strategies:\n\nbuying a residential living space\n\nrenting one instead and investing the dollar amount that would have been your down-payment\n\n\n\n\nAt one point in time, numpy, the popular Python numerical analysis library, included 10 specialized functions for financial analysis. Given their specific nature, they were eventually removed from numpy, I think in 2019 (learn about why that is here) and are now available in the separate library, numpy-financial. The library still seems focused on the same 10 core functions, which handle tasks like calculating loan payment amounts given some inputs, and applied financial economics tasks like calculating time value of money. Cool… Anyways, I’ll use it to create an amortization schedule for a mortgage.\n\n\n\nI built this notebook in a Google Colab instance, which seems to include most major Python libraries (more info).\nYou’ll probably have to download numpy-financial (it’s not included in Anaconda as far as I know), which you can accomplish within any notebook-like environment using the following command:\n\n!pip install numpy-financial\n\nYou’ll want to load the usual suspects - pandas, numpy, seaborn, matplotlib. I also run from datetime import datetime since we will be working with ranges of dates, and I run sns.set_style() to get my seaborn plots looking a bit more aesthetically pleasing - read more on themes here.\n\nimport pandas as pd\nimport numpy as np\nimport numpy_financial as npf\nfrom datetime import datetime\n\nimport seaborn as sns\n# set seaborn style\nsns.set_style(\"white\")\n\nimport matplotlib.pyplot as plt\n# Set Matplotlib font size\nplt.rcParams.update({'font.size': 14})"
  },
  {
    "objectID": "posts/rent-vs-buy/index.html#problem-setup",
    "href": "posts/rent-vs-buy/index.html#problem-setup",
    "title": "Buy vs. Rent, A Financial Modeling Workflow in Python",
    "section": "Problem Setup",
    "text": "Problem Setup\n\nDefinining Constants\nI’ll run this as a comparison between buying an apartment that costs $700,000 with a 20% downpayment, versus renting a home for $2,600 a month. This is meant to approximate buying versus renting a two-bed one-bath apartment.\nBuying fees are defined at 4%, the homeowners association fees are defined as $700 monthly.\n\n# Buying Constants\ninterest_rate = 0.065\ncost = 700000\nhoa = 700\ndown_payment = cost * .2\nprincipal = cost - down_payment\nbuying_fees = principal*.04\n\n# Renting Constants\nrent = 2600\n\nnpf.pmt() can be used to generate a monthly mortgage payment given those buying constants:\n\nnpf.pmt(interest_rate/12, 12*30, principal)\n\n-3539.580931560606\n\n\nalternatively, we can use npf.ppt() to see how much of the payment goes towards the principal, and use npf.ipmt() to see how much goes towards interest (see below for applications of those functions).\n\n\nDefining Random Variables\nI’ll make the simplifying assumption that both “annual home appreciation” and “annual stock appreciation” are generated from normal distributions. This is a kind of strong assumption, but one that seems to be routinely made at least with regards to stock market returns, even if there might be better distribution choices out there (more here).\nHere’s a look at how we’ll draw from a normal distribution. Given an average annual return, \\(\\mu = 0.0572\\) (\\(\\mu\\), or, mu, is a common variable name for average) and a standard deviation \\(\\sigma = 0.1042\\) (\\(\\sigma\\), or, sigma, is the common variable name for standard deviation), we can draw one sample from a normal distribution as follows:\n\n# Set a random seed for stability of results\nnp.random.seed(30)\n\nmean = .0572\nstandard_deviation = .1042\nsamples = 1\n\n# Draw the sample\nnp.random.normal(mean, standard_deviation, samples)\n\narray([-0.07451429])\n\n\nWe now simulate market returns for every month by supplying mean and standard deviation values for both home and stock market appreciation and drawing 360 samples (360 months in 30 years). For simplicity, we’ll just use world-wide aggregate values from “The Rate of Return on Everything, 1870-2015”.\n\nmu_stock = .1081\nsigma_stock = .2267\n\nmu_home = .0572\nsigma_home = .1042\n\nGiven that stock and home appreciation is probably correlated, I’ll have ti sample from a bivariate normal distribution using numpy.random.Generator.multivariate_normal - documentation here, rather than the univariate distribution draw shown above. I am going to assume a correlation coefficient, \\(\\rho_{stock,home}\\) of 0.5 - a fairly clear correlation.\nIn order to use that numpy function, I’ll need to translate my correlation statistic into a covariance statistic, and I’ll use the following formula (source):\n\\[ \\begin{align*}\ncov_{stock,home} &= \\rho_{stock,home} \\times \\sigma_{stock} \\sigma_{home} \\\\\\\ncov_{stock,home} &= 0.5 \\times .2267 \\times .1042 \\end{align*}\n\\]\nI calculate covariance and confirm that the covariance and correlations match up below:\n\ncov = 0.5 * sigma_stock * sigma_home\nprint(\"Covariance:\", cov)\nprint(\"Back to correlation:\", cov / (sigma_stock * sigma_home))\n\nCovariance: 0.01181107\nBack to correlation: 0.5\n\n\nNow that I have the covariance, I’ll be able to sample from a bivariate normal distribution of the form shown below (source).\n\\[\n\\begin{pmatrix} Stock \\\\\\\\ Home\\end{pmatrix} \\sim \\mathcal{N} \\left[ \\begin{pmatrix} \\mu_{s} \\\\\\ \\mu_{h}\\end{pmatrix}, \\begin{pmatrix} \\sigma_{s}^2 & cov_{s,h} \\\\\\ cov_{s,h} & \\sigma_{h}^2\\end{pmatrix} \\right]\n\\]\nNote, \\(s\\) is shorthand for stock and \\(h\\) is shorthand for home.\nNow I’ll code that in Python and confirm that the means and standard deviations of our samples match what we expect:\n\ncov_matrix = np.array([[sigma_stock**2, cov],\n              [cov, sigma_home**2]])\n\nreturns_df = pd.DataFrame(np.random\n                          .default_rng(30)\n                          .multivariate_normal([mu_stock, mu_home],\n                                               cov_matrix,\n                                               360),\n                          columns=[\"Stock_Appreciation\", \"Home_Appreciation\"])\nprint(\"Means:\", returns_df.mean(axis=0).values)\nprint(\"Std. Devs:\", returns_df.std(axis=0).values)\n\nreturns_df = (returns_df / 12)\n\nMeans: [0.10764063 0.05970695]\nStd. Devs: [0.22544095 0.10543034]\n\n\nPlotting the simulated values, we can see that stock market returns are typically higher than home value appreciation.\n\nreturns_df.cumsum().plot(figsize=(9,5))\nplt.xlabel(\"Months\")\nplt.ylabel(\"Money Multiplier\")\nplt.title(\"Simulated Home/Stock Returns\")\nsns.despine();\n\n\n\n\n\n\n\n\n\nhome_performance = returns_df.cumsum()['Home_Appreciation'] + 1\nstock_performance = returns_df.cumsum()['Stock_Appreciation'] + 1\n\nNow we can define two spread-sheet-like dataframes: - one that shows a mortgage amortization schedule for if you were to buy the $600,000 home, along with the home’s appreciation over time. - one that shows a table of rent payments and the stock market growth of what would have been your down payment (you can invest the down payment since you didn’t end up purchasing a house)."
  },
  {
    "objectID": "posts/rent-vs-buy/index.html#scenarios",
    "href": "posts/rent-vs-buy/index.html#scenarios",
    "title": "Buy vs. Rent, A Financial Modeling Workflow in Python",
    "section": "Scenarios",
    "text": "Scenarios\n\nOwnership Table\n\n# Buying Table\ndf_own = pd.DataFrame()\ndf_own[\"Period\"] =  pd.Series(range(12*30)) + 1\ndf_own[\"Date\"] = pd.date_range(start=datetime.today(),\n                           periods=12*30,\n                           freq='MS',\n                           name=\"Date\")\ndf_own[\"Principal_Paid\"] = npf.ppmt(interest_rate/12,\n                                    df_own[\"Period\"],\n                                    12*30,\n                                    principal)\ndf_own[\"Interest_Paid\"] = npf.ipmt(interest_rate/12,\n                                   df_own[\"Period\"],\n                                   12*30,\n                                   principal)\ndf_own[\"HOA_Paid\"] = hoa\ndf_own[\"HOA_Paid\"] = df_own[\"HOA_Paid\"].cumsum()\ndf_own[\"Balance_Remaining\"] = principal + df_own[\"Principal_Paid\"].cumsum()\ndf_own[\"Home_Value\"] = round(cost * home_performance, 2)\ndf_own[\"PropTax_Paid\"] = (df_own[\"Period\"]\n                          .apply(lambda x:\n                                 (cost * 1.02**((x-1)/12) * 0.01)\n                                 if (x-1) in list(range(0, 12*1000, 12))\n                                 else 0)\n                          .cumsum())\ndf_own[\"Sale_Fees\"] = df_own[\"Home_Value\"] * .07\ndf_own[\"Own_Profit\"] = (df_own[\"Home_Value\"] -\n                              df_own[\"HOA_Paid\"] -\n                              df_own[\"Balance_Remaining\"] -\n                              (buying_fees + df_own[\"Sale_Fees\"]) -\n                              df_own[\"PropTax_Paid\"])\ndf_own = round(df_own, 2)\n\nNote this code, which is a bit of a monster:\n\ndf_own[\"PropTax_Paid\"] = (df_own[\"Period\"]\n                          .apply(lambda x:\n                                 (cost * 1.02**((x-1)/12) * 0.01)\n                                 if (x-1) in list(range(0, 12*1000, 12))\n                                 else 0)\n                          .cumsum())\n\nWhat is happening here is a calculation of property assessed value and property tax according to California’s property assessment/tax regime (more here). We’ll look at this in two pieces, first, assessed value. In California, once you purchase a property, its assessed value is set at the purchase price, then increases annually by the lower of 2% or the rate of inflation according to the California Consumer Price Index (CCPI). You could write out an equation for this as follows:\n\\[\n\\begin{align*}\nAnnualFactor_y =\n\\begin{cases}\n        1 + CCPI_y, & \\text{if } CCPI_y &lt; 0.02 \\\\\\\n        1.02, & \\text{otherwise}\n\\end{cases}\n\\end{align*}\n\\]\n\\(AnnualFactor\\) is the amount that the assessed value of a home will appreciate (expressed as a multiplier) in a given year, \\(y\\). We define \\(y^*\\) as the year of initial purchase and \\(PurchasePrice\\) as the amount that the home was purchased for. Given that, \\(AssessedValue\\) is defined as follows:\n\\[ \\begin{align*}\nAssessedValue_y =\n    \\begin{cases}\n        PurchasePrice, & \\text{if } y = y^* \\\\\n        AssessedValue_{y-1} \\times AnnualFactor_y, & \\text{otherwise }\n    \\end{cases}\n\\end{align*}\n\\]\nIn our code, we will simplify this calculation by excluding the CCPI and just always using 1.02 as our annual factor. Therefore, we get:\n\\[\n  AssessedValue_y = PurchasePrice \\times 1.02^y\n\\]\nand once we factor in taxes (1%), we get:\n\\[\n  PropertyTax_y = 0.01(PurchasePrice \\times 1.02^y)\n\\]\nand finally we look at the the cumulative total property tax you’ve paid in a given year \\(y\\), which is df_own[\"PropTax_Paid\"]:\n\\[\n\\begin{equation*}\n  PropertyTaxPaid_y = \\sum_{y=1}^{30} 0.01(PurchasePrice \\times 1.02^y)\n\\end{equation*}\n\\]\nThere’s some elements added to the code to work between years and months, but that equation is the gist of it.\nWe end up with the following table for property ownership:\n\ndf_own\n\n\n\n\n\n\n\n\n\nPeriod\nDate\nPrincipal_Paid\nInterest_Paid\nHOA_Paid\nBalance_Remaining\nHome_Value\nPropTax_Paid\nSale_Fees\nOwn_Profit\n\n\n\n\n0\n1\n2024-04-01 16:50:37.077628\n-506.25\n-3033.33\n700\n559493.75\n701405.73\n7000.000000\n49098.40\n62713.58\n\n\n1\n2\n2024-05-01 16:50:37.077628\n-508.99\n-3030.59\n1400\n558984.76\n707143.89\n7000.000000\n49500.07\n67859.06\n\n\n2\n3\n2024-06-01 16:50:37.077628\n-511.75\n-3027.83\n2100\n558473.02\n723148.91\n7000.000000\n50620.42\n82555.47\n\n\n3\n4\n2024-07-01 16:50:37.077628\n-514.52\n-3025.06\n2800\n557958.50\n736621.91\n7000.000000\n51563.53\n94899.88\n\n\n4\n5\n2024-08-01 16:50:37.077628\n-517.31\n-3022.28\n3500\n557441.19\n744559.68\n7000.000000\n52119.18\n102099.31\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n355\n356\n2053-11-01 16:50:37.077628\n-3445.26\n-94.33\n249200\n13968.65\n1937424.41\n283976.554436\n135619.71\n1232259.49\n\n\n356\n357\n2053-12-01 16:50:37.077628\n-3463.92\n-75.66\n249900\n10504.74\n1940547.12\n283976.554436\n135838.30\n1237927.53\n\n\n357\n358\n2054-01-01 16:50:37.077628\n-3482.68\n-56.90\n250600\n7022.06\n1944357.20\n283976.554436\n136105.00\n1244253.59\n\n\n358\n359\n2054-02-01 16:50:37.077628\n-3501.54\n-38.04\n251300\n3520.51\n1949518.54\n283976.554436\n136466.30\n1251855.18\n\n\n359\n360\n2054-03-01 16:50:37.077628\n-3520.51\n-19.07\n252000\n-0.00\n1953845.89\n283976.554436\n136769.21\n1258700.12\n\n\n\n\n360 rows × 10 columns\n\n\n\n\n\n\nRental Table\nThis one is a but more simple, only examining the total rent you’ve paid in a given month and simulated stock returns at that point.\n\n# Rental Table\ndf_rent = pd.DataFrame()\ndf_rent[\"Period\"] =  pd.Series(range(12*30)) + 1\ndf_rent[\"Date\"] = pd.date_range(start=datetime.today(),\n                           periods=12*30,\n                           freq='MS',\n                           name=\"Date\")\ndf_rent[\"DownPayment_Invested\"] =  stock_performance * down_payment\ndf_rent[\"Rent_Paid\"] = rent\ndf_rent[\"Rent_Paid\"] = df_rent[\"Rent_Paid\"].cumsum()\ndf_rent[\"Rent_Profit\"] = df_rent[\"DownPayment_Invested\"] - df_rent[\"Rent_Paid\"]\ndf_rent = round(df_rent, 2)\n\n\ndf_rent\n\n\n\n\n\n\n\n\n\nPeriod\nDate\nDownPayment_Invested\nRent_Paid\nRent_Profit\n\n\n\n\n0\n1\n2024-04-01 16:50:37.260140\n136919.68\n2600\n134319.68\n\n\n1\n2\n2024-05-01 16:50:37.260140\n140789.47\n5200\n135589.47\n\n\n2\n3\n2024-06-01 16:50:37.260140\n142175.79\n7800\n134375.79\n\n\n3\n4\n2024-07-01 16:50:37.260140\n145552.44\n10400\n135152.44\n\n\n4\n5\n2024-08-01 16:50:37.260140\n146217.26\n13000\n133217.26\n\n\n...\n...\n...\n...\n...\n...\n\n\n355\n356\n2053-11-01 16:50:37.260140\n594635.22\n925600\n-330964.78\n\n\n356\n357\n2053-12-01 16:50:37.260140\n591339.68\n928200\n-336860.32\n\n\n357\n358\n2054-01-01 16:50:37.260140\n589319.15\n930800\n-341480.85\n\n\n358\n359\n2054-02-01 16:50:37.260140\n588221.21\n933400\n-345178.79\n\n\n359\n360\n2054-03-01 16:50:37.260140\n592090.64\n936000\n-343909.36\n\n\n\n\n360 rows × 5 columns"
  },
  {
    "objectID": "posts/rent-vs-buy/index.html#results",
    "href": "posts/rent-vs-buy/index.html#results",
    "title": "Buy vs. Rent, A Financial Modeling Workflow in Python",
    "section": "Results",
    "text": "Results\nAt this point, I’ll merge the ownership and rental tables and plot out what happened in this simulation\n\nmerged = pd.merge(df_own, df_rent, on=\"Period\")\nmerged = merged.melt(value_vars = [\"Rent_Profit\", \"Own_Profit\"], id_vars='Period')\n\n\nplt.figure(figsize=(14, 6))\nplt.title(\"Wealth Outcomes for Owning vs. Renting a 2b1br Apt\")\nsns.lineplot(data=merged, x=\"Period\", y=\"value\", hue=\"variable\")\nfor x in range(0, 350, 12):\n    if x == 0:\n        plt.axvline(x, color=\"grey\", linestyle=\":\", alpha=1, label=\"Year\")\n    else:\n        plt.axvline(x, color=\"grey\", linestyle=\":\", alpha=0.7)\n    plt.text(x+1, -100000, str(int(x/12)), alpha=0.8)\nplt.axhline(0, color=\"red\", linestyle=\"--\", alpha=0.5, label=\"Zero\")\nplt.legend()\nsns.despine()\n\n\n\n\n\n\n\n\nWe can quickly see that ownership will clearly build more wealth in the medium and long run:\n\nyears = 5\nprint(f\"Owner after {years} years:\", df_own.loc[12*years-1, \"Own_Profit\"])\nprint(f\"Renter after {years} years:\", df_rent.loc[12*years-1, \"Rent_Profit\"])\n\nOwner after 5 years: 215177.81\nRenter after 5 years: 40500.11\n\n\nHowever, we can see that, in the unlikely case that the home is sold within the first year or so, it’s the renter that has more wealth, likely due to the owner contending with buying/selling fees:\n\nyears = 1\nprint(f\"Owner after {years} years:\", df_own.loc[12*years-1, \"Own_Profit\"])\nprint(f\"Renter after {years} years:\", df_rent.loc[12*years-1, \"Rent_Profit\"])\n\nOwner after 1 years: 114027.92\nRenter after 1 years: 115865.99\n\n\nA possible takeaway here is that, as long as you can be confident you’ll be able to hold onto the house for more than a year, it’s probably better to purchase it. Uncertainty estimates would be useful here, and could be obtained by running the simulation under a wide variety of randomly generated market conditions."
  },
  {
    "objectID": "posts/newtons-method/index.html",
    "href": "posts/newtons-method/index.html",
    "title": "Newton’s Method From The Ground Up",
    "section": "",
    "text": "Code\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport sympy\nimport imageio\nfrom typing import Callable\nimport seaborn as sns"
  },
  {
    "objectID": "posts/newtons-method/index.html#newtons-method-for-finding-roots-what-how-and-why",
    "href": "posts/newtons-method/index.html#newtons-method-for-finding-roots-what-how-and-why",
    "title": "Newton’s Method From The Ground Up",
    "section": "Newton’s Method for Finding Roots: What, How, and Why",
    "text": "Newton’s Method for Finding Roots: What, How, and Why\nIn the context of a Differential Calculus course, Newton’s Method, also referred to as The Newton-Raphson Method, seems to typically come up near the end of the semester, offering a brief look into the world of “numerical methods” and how we might solve complex problems in the real world. I think that it’s a cool topic and I wanted to write an extended blog post about it. The main purpose of this is to ensure that I always have personal reference materials for Newton’s Method, but perhaps it can be helpful to other readers.\nI draw on two key sources for thinking about Newton’s Method:\n\n“Newton’s Method.” 2023. In Wikipedia\nStrang, Gilbert, and Edwin Herman. 2016. Calculus Volume 1. OpenStax College.\n\n\nWhat is it\n\nIn many areas of pure and applied mathematics, we are interested in finding solutions to an equation of the form \\(f(x)=0\\)\n\n(Strang and Herman 2016)\nNewton’s Method is a numerical method that helps us solve \\(f(x)=0\\).\nThere are many cases where we need to solve equations like that, but the application area I work in involves statistical modeling, so I jump to the case where we want to “fit” a line as close as possible to some set of data points, thus creating a model of a data-generating-process. The fitting process rests on getting the line as close to the data points as possible, thus minimizing error. If we can formulate error as a function, then we can minimize it – we differentiate it and set the derivative to 0, and solve \\(f'(x)=0\\). Therein lies the opportunity to apply Newton’s Method to a real problem.\nHowever, we’ll take a step back from statistics and return to the domain of an introductory calculus course. Newton’s Method is useful for finding the root – the x-intercept – of a function. We’ll explore the method by walking through an example\n\n\nUsing Newton’s Method to Solve a Simple Problem\nSay we are given the function \\(4\\sin(x)\\) and we want to find the x-intercept of the function with the domain \\(-5\\leq x \\leq -1\\).\nWe’ll first set this function up in sympy, a python library for symbolic computation.\n\nx, y = sympy.symbols('x y')\ny = 4*sympy.sin(x)\ny\n\n\\(\\displaystyle 4 \\sin{\\left(x \\right)}\\)\n\n\nThis is a problem with a known answer (you can google x-intercepts of \\(\\sin(x)\\) for a table), so it’s not particularly useful to use Newton’s Method here, but for our purposes it will be helpful that we can check our answer with the “right” one. \\[\n\\begin{align*}\n4 \\sin(x) &= 0 \\quad \\text{where} \\quad -5\\leq x \\leq -1 \\\\\n\\sin(x) &= 0 \\\\\nx &= \\sin^{-1}(0) \\\\\nx &= -\\pi\n\\end{align*}\n\\]\n\n\nCode\ndef plot_f(\n        f: Callable[[float], float],\n        x_low: int,\n        x_high: int,\n        ax: matplotlib.axes.Axes) -&gt; None:\n    \"\"\"\n    Plots a given function f within a specified range on a provided axes.\n\n    Parameters:\n        f (Callable[[float], float]): The function to be plotted.\n        x_low (int): The lower bound of the x-axis.\n        x_high (int): The upper bound of the x-axis.\n        ax (matplotlib.axes.Axes): The matplotlib axes object on which the function will be plotted.\n\n    Returns:\n        None\n    \"\"\"\n    x_vec = np.linspace(x_low, x_high, 100)\n    ax.plot(x_vec, f(x_vec))\n\n\ndef base_plot(\n        y: sympy.core.mul.Mul,\n        x: sympy.core.mul.Mul,\n        x_low: int = -5,\n        x_high: int = 5) -&gt; None:\n    \"\"\"\n    Creates a base plot for a mathematical expression and its graph.\n\n    Parameters:\n        y (sympy.core.mul.Mul): The mathematical expression to be plotted.\n        x (sympy.core.mul.Mul): The symbol representing the independent variable in the expression.\n        x_low (int): The lower bound of the x-axis (default is -5).\n        x_high (int): The upper bound of the x-axis (default is 5).\n\n    Returns:\n        tuple: A tuple containing the matplotlib figure and axes used for plotting.\n\n    Note:\n        The mathematical expression is first converted to a Python function using sympy.lambdify.\n    The function is then plotted on the specified axes along with gridlines and labels.\n    \"\"\"\n    f = sympy.lambdify(x, y)\n    fig, ax = plt.subplots()\n    ax.grid(alpha=.5)\n    ax.axhline(0, color=\"black\", alpha=.5)\n    plot_f(f, x_low, x_high, ax)\n    ax.set(title=f\"$f(x)={sympy.latex(y)}$\", xlabel=\"$x$\", ylabel=\"$f(x)$\")\n    return fig, ax\n\n\ndef plot_truth(ax: matplotlib.axes.Axes) -&gt; None:\n    \"\"\"\n    Plots the true root of a function as a marker on the graph.\n\n    Parameters:\n        ax (matplotlib.axes.Axes): The matplotlib axes on which the marker will be plotted.\n\n    Returns:\n        None\n    \"\"\"\n    ax.plot(-np.pi,\n            0,\n            \"*\",\n            markersize=15,\n            color=\"darkblue\",\n            label=\"True root, $-\\pi$\")\n\n\ndef plot_guess(\n        ax: matplotlib.axes.Axes,\n        guess: int,\n        label: str) -&gt; None:\n    \"\"\"\n    Plots a guess or estimate as a marker on the graph.\n\n    Parameters:\n        ax (matplotlib.axes.Axes): The matplotlib axes on which the marker will be plotted.\n        guess (int): The estimated value to be marked on the graph.\n        label (str): Label for the marker.\n\n    Returns:\n        None\n    \"\"\"\n    ax.plot(guess,\n            0,\n            \"o\",\n            label=label)\n\n\ndef plot_guess_coords(\n        ax: matplotlib.axes.Axes,\n        guess: int,\n        label: str,\n        y: sympy.core.mul.Mul = y,\n        x: sympy.core.mul.Mul = x):\n    \"\"\"\n    Plots a guess or estimate with specific coordinates as a marker on the graph.\n\n    Parameters:\n        ax (matplotlib.axes.Axes): The matplotlib axes on which the marker will be plotted.\n        guess (int): The estimated x-coordinate where the marker will be placed.\n        label (str): Label for the marker.\n        y (sympy.core.mul.Mul): The mathematical expression corresponding to the y-coordinate.\n        x (sympy.core.mul.Mul): The symbol representing the independent variable (x).\n\n    Returns:\n        None\n    \"\"\"\n    ax.plot(guess,\n            y.subs(x, guess).evalf(),\n            \"s\",\n            label=label, color=\"black\")\n\n\ndef euclidean_dist_to_truth(x): return np.sqrt((-np.pi - float(x))**2)\n\n\nNow we’ll begin the process of using Newton’s Method to arrive at that same answer of \\(x=-\\pi\\). #### Step 1: Make a first guess and evaluate its (x, y) coordinates For a first guess, it’s typical to start close to 0. In our case, we’ll try -2.\n\nx_0 = -2\n\nLet’s get a sense of how good of a guess this is by plotting it\n\\[\n\\begin{align*}\nf(x) &= 4 \\sin \\left( x \\right) \\\\\\\nf(x_0) &= 4 \\sin \\left( -2 \\right) \\\\\\\nf(x_0) &\\approx −3.6371897 \\\\\\\n(x_0, y_0) &= (x_0, f(x_0)) \\approx (-2, −3.6371897)\n\\end{align*}\n\\]\n\nfig, ax = base_plot(y, x, x_low=-5, x_high=-1)\nplot_truth(ax)\nplot_guess(ax, x_0, \"$x_0$ (initial guess)\")\nplot_guess_coords(ax, x_0, \"$(x_0, f(x_0))$\")\nax.legend();\n\n\n\n\n\n\n\n\nWe can assess the quality of that guess by calculating the distance from the guess to the right answer (\\(-\\pi\\)):\n\nprint(f\"error for guess x_0:\", euclidean_dist_to_truth(x_0))\n\nerror for guess x_0: 1.1415926535897931\n\n\n\nStep 2: Find the equation of the tangent line at those coordinates\nWe will proceed to improve upon that initial guess by computing the linear approximation of the function at that point, then retrieve its root to make a next-guess. Our first guess wasn’t based on any relevant information other than the domain of our search (between -5 and 0). Our next guess is going to be better-informed, as it is an estimate based on an approximation of the function.\n(Note that the linear approximation at this point is typically referred to as a tangent line, and I’ll use those two phrases interchangeably.)\nWe compute the tangent by first differentiating the function and plugging in our previous guess. This yields the slope of the tangent line: \\[\n\\begin{align*}\nf(x) &= 4 \\sin \\left( x \\right) \\\\\\\nf'(x) &= 4 \\cos \\left( x \\right) \\\\\\\nf'(x_0) &= 4 \\cos \\left( -2 \\right) \\\\\\\nf'(x_0) &\\approx −1.6645873\n\\end{align*}\n\\]\nI’ll note that sympy is capable of doing all of these routine calculations as well:\n\nprint(\"   f(x) = \", y)\nprint(\"  f'(x) = \", y.diff())\nprint(\"f'(x_0) = \", y.diff().subs(x, -2))\nprint(\"[note: sympy converted x_0=-2 to x_0=2 because cos(-x)=cos(x)]\")\nprint(\"f'(x_0) = \", y.diff().subs(x, x_0).evalf())\n\n   f(x) =  4*sin(x)\n  f'(x) =  4*cos(x)\nf'(x_0) =  4*cos(2)\n[note: sympy converted x_0=-2 to x_0=2 because cos(-x)=cos(x)]\nf'(x_0) =  -1.66458734618857\n\n\nNow, given each of these terms:\n\n\n\nTerm (math)\nValue\n\n\n\n\n\\(x_0\\) (X, Last Guess)\n\\(= -2\\)\n\n\n\\(f(x_0)\\) (Y at \\(x_0\\))\n\\(\\approx −3.63719\\)\n\n\n\\(f'(x_0)\\) (Slope of tangent at \\(x_0\\))\n\\(\\approx −1.66459\\)\n\n\n\nWe can proceed to find the full equation of the tangent line by writing out the point-slope form of a linear equation with slope \\(m=f'(x_0)\\). \\[\n\\begin{align*}\n(y - y_0) &= m(x - x_0) \\\\\n(y - f(x_0)) &= f'(x_0)(x - x_0) \\\\\ny &= f'(x_0)(x - x_0) + f(x_0)\n\\end{align*}\n\\] Plugging in our values, we get:\n\\[\n\\begin{align*}\ny &\\approx −1.66459x + (1.66459)(-2) - 3.63719 \\\\\ny &\\approx −1.66459x - 6.966364 \\\\\n\\end{align*}\n\\]\nWe’ll save that into a python function and plot it to make sure it does look like the tangent.\n\ndef f_1(x): return -1.6645873*x - 6.9663643\n\n\nfig, ax = base_plot(y, x, x_low=-5, x_high=-1)\nplot_truth(ax)\n\nplot_f(f_1, x_low=-5, x_high=-1, ax=ax)\n\nplot_guess(ax, x_0, \"$x_0$ (initial guess)\")\nplot_guess_coords(ax, x_0, \"$(x_0, f(x_0))$\")\n\nax.legend();\n\n\n\n\n\n\n\n\n\n\nStep 3: Find the x-intercept of the tangent line\nGiven that the tangent line is the best linear approximation of the original function, we can use its x-intercept as an approximation of the x-intercept of the original function. Thus, the root of the tangent line is the new “best-guess” of the original function’s root.\n\\[\n\\begin{align*}\n0 &\\approx −1.6645873x_1 - 6.9663643 \\\\\n\\frac{6.9663643}{−1.6645873} &\\approx x_1 \\\\\nx_1 &\\approx -4.1850\n\\end{align*}\n\\]\n\nx_1 = -4.1850\n\n\nfig, ax = base_plot(y, x, x_low=-5, x_high=-1)\nplot_truth(ax)\nplot_guess(ax, x_0, \"$x_0$ (initial guess)\")\nplot_f(f_1, x_low=-5, x_high=-1, ax=ax)\nplot_guess(ax, x_1, \"$x_1$ (next guess)\")\nax.legend();\n\n\n\n\n\n\n\n\nWhile this guess still isn’t particularly great, we can see that we have actually reduced the “error” of our guess:\n\nfor i, x_n in enumerate([x_0, x_1]):\n    print(f\"error for guess x_{i}:\", euclidean_dist_to_truth(x_n))\n\nerror for guess x_0: 1.1415926535897931\nerror for guess x_1: 1.0434073464102065\n\n\nThe big reveal of Newton-Raphson is that this error will continue to shrink as we repeat steps 1-3.\n\n\nStep 4: Repeat\nWe will again find the tangent line at this new point, \\((x_1, f(x_1))\\). We could take the old tangent line equation, \\(y = f'(x_0)(x - x_0) + f(x_0)\\) and simply update all of those \\(x_0\\) to \\(x_1\\), but at this point it will benefit us to move towards a more general equation: \\[\ny = f'(x_n)(x - x_n) + f(x_n)\n\\] This allows us to generate the tangent line at any given guess, \\(x_n\\). The following code leverages sympy to write that equation as a python function.\n\ndef y_n(x_n): return (\n    y.diff().subs(x, x_n) * (x - x_n) +  # f_1(x_1)(x-x_1) +\n    y.subs(x, x_n)  # f(x_1)\n)\n\n\ny_n(x_1)\n\n\\(\\displaystyle - 2.01311525745113 x - 4.96839101072835\\)\n\n\nWe’ll also use sympy to easily solve for the new tangent line’s x-intercept, \\(x_2\\)\n\nx_2 = sympy.solve(y_n(x_1), x)[0]\nx_2\n\n\\(\\displaystyle -2.46801120419652\\)\n\n\n\nfig, ax = base_plot(y, x, x_low=-5, x_high=-1)\nplot_truth(ax)\nplot_f(sympy.lambdify(x, y_n(x_1)), x_low=-5, x_high=-1, ax=ax)\n\nplot_guess_coords(ax, x_1, \"$(x_1, f(x_1))$\")\n\nplot_guess(ax, x_0, \"$x_0$ (initial guess)\")\nplot_guess(ax, x_1, \"$x_1$ (previous guess)\")\nplot_guess(ax, x_2, \"$x_2$ (next guess)\")\n\nax.legend();\n\n\n\n\n\n\n\n\nWe can verify that this new guess again reduces our “error,” which should encourage us to continue this process.\n\nfor i, x_n in enumerate([x_0, x_1, x_2]):\n    print(f\"error for guess x_{i}:\", euclidean_dist_to_truth(x_n))\n\nerror for guess x_0: 1.1415926535897931\nerror for guess x_1: 1.0434073464102065\nerror for guess x_2: 0.6735814493932737\n\n\n\n\nStep 5: Generalize the procedure\n\nA.) Make the equation more direct\nThus far we have used the general equation \\(y = f'(x_n)(x - x_n) + f(x_n)\\), where \\(x_n\\) is our current guess, and we solve for \\(x\\) to define our next guess. Given that we solve the equation for \\(x\\), we can rewrite it as follows:\n\\[\n\\begin{align*}\n0 &= f'(x_n)(x - x_n) + f(x_n) \\\\\n0 &= f'(x_n)(x - x_n) + f(x_n) \\\\\n0 - f(x_n) &= f'(x_n)(x - x_n) \\\\\n-\\frac{f(x_n)}{f'(x_n)} &= x - x_n \\\\\nx &= x_n -\\frac{f(x_n)}{f'(x_n)} \\\\\n\\end{align*}\n\\] This expresses one step of Newton’s Method – solving for the x-intercept of the tangent line at the point \\((x_n, f(x_n))\\).\n\n\nB.) Move from equation to algorithm\nWe now build on this single step and express the general process of Newton’s method. To start, it’s more accurate to label the left hand side \\(x_{n+1}\\) given that it represents the next guess: \\[\nx_{n+1} = x_n -\\frac{f(x_n)}{f'(x_n)}\n\\]\nGiven this equation, we can think of Newton’s Method as essentially searching for good guesses – defining new \\(x_{n+1}\\) – until it’s right. But how do we define “right”? Put in other words, when do we stop?\nIn our case, we are working with a simple function and we know the correct answer – we can just stop once we get close to \\(-\\pi\\) – but in any real application that won’t be the case. In those cases, it is common practice to define “right” as when the guesses stop changing much with each iteration. Stated semi-formally, we wait until \\(|x_{n+1} - x_{n}|\\) gets small.\nFor example: if the last guess was -3.14159 and the new guess is -3.141592, the guess only changed by .0000002, and we might conclude that we’ve gotten as close to the answer as is necessary. In this case, we set a stopping condition – when the next guess is less than or equal to .0000002 away from the previous one, we stop. We can write out the stopping condition as follows:\n\\[\n\\begin{align*}\n|x_{n+1} - x_{n}| &\\leq 2\\times 10^{-7} \\\\\n|(x_n -\\frac{f(x_n)}{f'(x_n)}) - x_{n}| &\\leq 2\\times 10^{-7} \\\\\n|-\\frac{f(x_n)}{f'(x_n)}| &\\leq 2\\times 10^{-7} \\\\\n|\\frac{f(x_n)}{f'(x_n)}| &\\leq 2\\times 10^{-7}\n\\end{align*}\n\\]\nWe can try writing out the recursive algorithm as a piece-wise equation:\n\\[\n\\begin{align*}\n\\text{Let } x_0 := \\text{initial guess, } \\\\\n\\text{For all natural numbers } n \\ge 0, \\\\\n\\text{Define }  x_{n+1} \\text{ as:}\n\\end{align*}\n\\]\n\\[\nx_{n+1} = \\begin{cases}\n        x_n & \\text{if }\\quad |\\frac{f(x_n)}{f'(x_n)}| \\leq 2\\times 10^{-7} \\\\\n        x_n -\\frac{f(x_n)}{f'(x_n)} & \\text{Otherwise}\n        \\end{cases}\n\\]\nHowever, now that we are moving into the realm of algorithms, I think it’s clearer to write this as code:\n\nx_n = -2\n\n# We'll use this to count which guess we are on\ncounter = 1\nwhile True:\n    # Disregard the following utility code\n    print(f\"Guess {counter}:\",\n          str(round(float(x_n), 5)).ljust(8, '0'),\n          \" --- Error:\",\n          euclidean_dist_to_truth(x_n))\n    ################################################\n    # The following is the code for newton's method\n    # 1.) Check for the stopping condition,\n    # |f(x_n)/f'(x_n)| &lt; 2 * 10^-7\n    stop_condition = (\n        np.abs(sympy.lambdify(x, y)(x_n) /\n               sympy.lambdify(x, y.diff())(x_n))\n        &lt; 2e-7\n    )\n    if stop_condition:\n        print(f\"Converged in {counter} steps.\")\n        break\n    # 2.) If stopping condition not met, make a new guess\n    x_n = x_n - (\n        # f(x_n) /\n        sympy.lambdify(x, y)(x_n) /\n        # f'(x_n)\n        sympy.lambdify(x, y.diff())(x_n)\n    )\n    ################################################\n    # Update the counter\n    counter += 1\n\nGuess 1: -2.00000  --- Error: 1.1415926535897931\nGuess 2: -4.18504  --- Error: 1.0434472096717258\nGuess 3: -2.46789  --- Error: 0.6736989790751275\nGuess 4: -3.26619  --- Error: 0.1245936239793135\nGuess 5: -3.14094  --- Error: 0.00064874127215786\nGuess 6: -3.14159  --- Error: 9.101119857746198e-11\nConverged in 6 steps.\n\n\nWe’ve converged at our best-guess after six steps, which we can see animated below.\n\n\nCode\n%%capture\nx_n = -2\nmax_iter = 10\ntolerance = 1e-6\n\nfig, ax = base_plot(y, x, x_low=-5, x_high=-1)\nplot_truth(ax)\nax.set_ylim(-4, 4)\nax.set_xlim(-4.5, -1.5)\n\nimages = []\nbreaking_condition = False\nfor j in range(max_iter):\n    error = euclidean_dist_to_truth(x_n)\n    if error &lt; tolerance:\n        breaking_condition = True\n\n    ax.set_title(\n        f\"Iteration {j+1}\\nGuess: {round(x_n, 6)}\\n Error: {round(error, 8)}\")\n\n    # Plot the current guess\n    plot_guess(ax, x_n, \"$x_n$\")\n    filename = f'newton_iteration_{j}_0.png'\n    fig.savefig(filename)\n    images.append(imageio.imread(filename))\n\n    if breaking_condition:\n        ax.text(x_n, 2, s=f\"CONVERGED IN {j} STEPS\", size=15)\n        filename = f'newton_iteration_{i}_3.png'\n        fig.savefig(filename)\n        images.append(imageio.imread(filename))\n        ax.text(x_n, 2, s=f\"CONVERGED IN {j} STEPS\", size=15)\n        filename = f'newton_iteration_{i}_4.png'\n        fig.savefig(filename)\n        images.append(imageio.imread(filename))\n        break\n    # Plot the coordinates of the current guess\n    plot_guess_coords(ax, x_n, \"$(x_n, f(x_n))$\")\n    filename = f'newton_iteration_{j}_1.png'\n    fig.savefig(filename)\n    images.append(imageio.imread(filename))\n    # Plot the tangent line of that coordinate to inform next guess\n    plot_f(sympy.lambdify(x, y_n(x_n)), x_low=-5, x_high=-1, ax=ax)\n    filename = f'newton_iteration_{j}_2.png'\n    fig.savefig(filename)\n    images.append(imageio.imread(filename))\n\n    # Reset plot\n    ax.clear()\n    fig, ax = base_plot(y, x, x_low=-5, x_high=-1)\n    plot_truth(ax)\n    ax.set_ylim(-4, 4)\n    ax.set_xlim(-4.5, -1.5)\n\n    x_n -= (\n        sympy.lambdify(x, y)(x_n) /\n        sympy.lambdify(x, y.diff())(x_n)\n    )\nimageio.mimsave('newton_iterations.gif', images, duration=1)\n\n# Clean out pngs\nwd = os.getcwd()\nfiles = os.listdir(wd)\nfor item in files:\n    if item.endswith(\".png\"):\n        os.remove(os.path.join(wd, item))\n\n\n\n\n\n\n\nUsing Newton’s Method to Solve a Real Problem\nIn the previous example, we dealt with a function, \\(f(x) = 4\\sin(x)\\) with a well known analytical solution for its x-intercept. Other simple functions can typically be solved with known formulas – e.g. a second degree polynomial’s roots can be found using the quadratic formula. In cases of known analytical solutions or readily available root-finding formulas, there is no reason to use Newton’s Method beyond as a learning exercise.\nHowever, many functions do not have have readily available methods for finding the root. For example, if \\(f(x)\\) is a polynomial of degree 5 or greater, it is known that no formula for finding its roots exist (Strang and Herman 2016). Consider the following polynomial of degree 5: \\[\nf(x) =x^{5} + 8 x^{4} + 4 x^{3} - 2 x - 7\n\\] What if we are asked to solve the following: \\[\nf(x)=0 \\quad \\text{where} -5\\leq x \\leq 0\n\\] There is no formula that solves this. Even plugging the polynomial into sympy and running its solver, sympy.solveset, doesn’t give a clear answer.\n\nx, y = sympy.symbols('x y')\ny = x**5 + 8*x**4 + 4*x**3 - 2*x-7\ny\n\n\\(\\displaystyle x^{5} + 8 x^{4} + 4 x^{3} - 2 x - 7\\)\n\n\n\nprint(sympy.solveset(y, x, sympy.Interval(-5, 0)))\n\n{CRootOf(x**5 + 8*x**4 + 4*x**3 - 2*x - 7, 1)}\n\n\n(where CRootOf is an indexed complex root of the polynomial – not an analytical solution)\nWe might proceed to visually inspect the function on this domain – but it’s even pretty hard to visually spot a root here!\n\nx_vec = np.linspace(-5, 0, 1000000)\nfig, ax = plt.subplots()\nax.grid(alpha=.5)\nax.plot(x_vec, sympy.lambdify(x, y)(x_vec))\nax.set_title(f\"$y={sympy.latex(y)}$\");\n\n\n\n\n\n\n\n\nHere’s a good use-case for Newton’s Method. I set up the algorithm with \\(x_0=0\\) and a stopping condition that \\(|x_{n+1} - x_{n}| \\leq 10^{-12}\\).\nI quickly converge at an answer:\n\nx_n = 0\ntolerance = 1e-12\ncounter = 1\n\nlast = np.inf\nwhile True:\n    # Disregard the following utility code\n    print(\"Guess {0:3}:\".format(counter),\n          str(round(float(x_n), 5)).ljust(8, '0'),\n          \" --- Change:\",\n          round(np.abs(float(x_n) - last), 8))\n    last = float(x_n)\n    ################################################\n    # The following is the code for newton's method\n    # 1.) Check for the stopping condition,\n    # |f(x_n)/f'(x_n)| &lt; 2 * 10^-7\n    stop_condition = (\n        np.abs(sympy.lambdify(x, y)(x_n) /\n               sympy.lambdify(x, y.diff())(x_n))\n        &lt; tolerance\n    )\n    if stop_condition:\n        print(f\"Converged in {counter} steps.\")\n        break\n    # 2.) If stopping condition not met, make a new guess\n    x_n = x_n - (\n        # f(x_n) /\n        sympy.lambdify(x, y)(x_n) /\n        # f'(x_n)\n        sympy.lambdify(x, y.diff())(x_n)\n    )\n    ################################################\n    counter += 1\n\nGuess   1: 0.000000  --- Change: inf\nGuess   2: -3.50000  --- Change: 3.5\nGuess   3: -2.44316  --- Change: 1.05683755\nGuess   4: -1.81481  --- Change: 0.6283498\nGuess   5: -1.41471  --- Change: 0.40010602\nGuess   6: -1.19062  --- Change: 0.22409096\nGuess   7: -1.11070  --- Change: 0.07991323\nGuess   8: -1.10108  --- Change: 0.0096197\nGuess   9: -1.10095  --- Change: 0.00012977\nGuess  10: -1.10095  --- Change: 2e-08\nConverged in 10 steps.\n\n\nWhen we plot our best guess, \\(x_n\\), we see that it is indeed the root of the function.\n\nfig, ax = plt.subplots()\n\nax.axvline(x_n, linestyle=\"--\", color=\"tab:red\",\n           linewidth=2, label=\"Final Guess\")\nax.legend()\n\nx_vec = np.linspace(-1.5, 0, 1000000)\n\nax.grid(alpha=.5)\nax.plot(x_vec, sympy.lambdify(x, y)(x_vec))\nax.set_title(f\"$y={sympy.latex(y)}$\")\nax.axhline(0, color=\"black\", alpha=.5);\n\n\n\n\n\n\n\n\nWe can also compare our answer to what sympy gets from numerically solving for the function’s root:\n\nprint(\"Newton's Method:\", x_n)\n\nNewton's Method: -1.1009529619409872\n\n\n\nprint(\"Sympy's answer:\", sympy.solveset(y, x, sympy.Interval(-5, 0)).evalf())\n\nSympy's answer: {-1.10095296194099}\n\n\nWe see that we have basically the same answer as sympy."
  },
  {
    "objectID": "posts/newtons-method/index.html#newtons-method-as-optimization",
    "href": "posts/newtons-method/index.html#newtons-method-as-optimization",
    "title": "Newton’s Method From The Ground Up",
    "section": "Newton’s Method as Optimization",
    "text": "Newton’s Method as Optimization\nRecall that the derivative of a function is 0 at a critical point – its maximum or minimum. We have been using Newton’s Method to find the root of a function, but in the process we’ve also implicitly been finding the critical point of the anti-derivative, or, integral of that function.\nFor visual intuition of this fact, consider the following plot, in which we visualize the integral of our function, \\(\\int f(x) dx\\), and plot the root of \\(f(x)\\) that we just found using Newton’s Method:\n\nfig, ax = plt.subplots()\n\nax.axvline(x_n, linestyle=\"--\", color=\"tab:red\",\n           linewidth=2, label=\"Final Guess\")\n\nax.grid(alpha=.5)\nax.plot(x_vec, sympy.lambdify(x, y)(x_vec), label=\"$f(x)$\")\nax.plot(x_vec, sympy.lambdify(x, y.integrate())(x_vec), label=\"$\\int f(x) dx$\")\nax.legend()\nax.set_ylim(-5, 7)\nax.set_title(f\"$\\int ({sympy.latex(y)})dx$\");\n\n\n\n\n\n\n\n\nThe root of \\(f(x)\\) seems to also be the maximum of \\(\\int f(x) dx\\).\nWhy is this important? Based on this idea, we can extend Newton’s Method for general use in finding the critical value of a function, which means that we can use it for solving optimization problems. We can setup the optimization approach as follows:\nIf the following equation will converge to the root of \\(f(x)\\) and the critical point of \\(\\int f(x) dx\\): \\[\n\\begin{align*}\nx_{n+1} = x_n -\\frac{f(x_n)}{f'(x_n)}\n\\end{align*}\n\\] Then the following equation will converge to the root of \\(f'(x)\\) and thus the critical point of \\(f(x)\\) (equivalent to \\(\\int f'(x)dx\\)): \\[\n\\begin{align*}\nx_{n+1} = x_n -\\frac{f'(x_n)}{f''(x_n)}\n\\end{align*}\n\\]\n(“Newton’s Method” 2023, “Minimization and maximization problems”)"
  },
  {
    "objectID": "posts/linear-approximation-3d/index.html",
    "href": "posts/linear-approximation-3d/index.html",
    "title": "Linear Approximation in 3D",
    "section": "",
    "text": "Code\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import axes3d\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nThe following is a calculus problem that I think provides a good overview of linear approximation, a method for approximating a general function using a linear function. Common applications of linear approximation exist in optics, oscillation, and electric resistivity problems (“Linear Approximation” 2023). The following image from (Strang and Herman 2016, chap. 4.4) provides some good intuition of what this approximation looks like:"
  },
  {
    "objectID": "posts/linear-approximation-3d/index.html#the-question",
    "href": "posts/linear-approximation-3d/index.html#the-question",
    "title": "Linear Approximation in 3D",
    "section": "The Question",
    "text": "The Question\nUse a tangent plane to approximate the value of the following function at the point \\((2.1, -5.1)\\).\n\\[f(x,y) = \\sqrt{42-4x^2-y^2}\\]\nHere’s some code setup for the question:\n\nf = lambda x, y: np.sqrt(42-4*x**2 - y**2)\nrange_x = np.linspace(-8, 8, 500)\nrange_y = range_x.copy()\nX, Y = np.meshgrid(range_x, range_y)\nZ = f(X, Y)\n\n\nWhy do we need to approximate this?\nWe can’t just plug in and compute this output because the point (2.1, -5.1) is outside of the domain of this function – thus \\(f(2.1, -5.1)\\) does not exist.\n\nf(2.1, -5.1)\n\nnan\n\n\n\\[f(2.1, -5.1) \\approx \\sqrt{-1.65} = DNE\\]\n\n\nCode\ndef style_plot(ax, z=False):\n    ax.grid(alpha=.5)\n    ax.set(xlabel=\"$x$\", ylabel=\"$y$\")\n    if z:\n        ax.set(zlabel=\"$z$\")\n\nfig, ax = plt.subplots(figsize=(6, 4))\nax.contourf(X, Y, Z)\nax.scatter(x=2.2,\n           y=-5.1,\n           s=15,\n           color='tab:orange',\n           label=r\"$f(2.1, -5.1) = DNE$\")\nstyle_plot(ax)\nax.legend();\n\n\n\n\n\n\n\n\n\n\n\nDeveloping a linear approximation of the function\nTo approximate this quantity, we find a nearby point where the function is defined, and construct a tangent plane to extrapolate our quantity of interest. For a nearby point, I select \\(P_0 = (x_0=2, y_0=-5)\\), where the function still produces a real number output:\n\\[f(x_0, y_0) = f(2, -5) =\\sqrt{1} = 1\\]\n\n\nCode\nfig = plt.figure(figsize=(8, 3))\nax1 = plt.subplot(122)\nax2 = plt.subplot(121, projection='3d')\n# Countour plot\nax1.contourf(X, Y, Z)\nax1.scatter(2, -5, color='red', s=15, label=r\"$f(2, -5)$\")\nax1.grid(alpha=.5)\nax1.legend()\nstyle_plot(ax1)\n# 3d surface\nax2.plot_wireframe(X, Y, Z)\nstyle_plot(ax2, z=True)\nax2.set_box_aspect(None, zoom=0.9)\nax2.set_xlim(-5, 5)\nax2.set_ylim(-8, 8)\nax2.set_zlim(-5, 5)\nax2.scatter(2, -5, f(2, -5), color='red', s=15)\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nWe then derive a linear approximation of \\(f\\) at this point using the following equation:\n\\[\n\\begin{align}\n   L(x, y) = f\\left( {{x_0},{y_0}} \\right) + {f_x}\\left( {{x_0},{y_0}} \\right)\\left( {x - {x_0}} \\right) + {f_y}\\left( {{x_0},{y_0}} \\right)\\left( {y - {y_0}} \\right)\n\\end{align}\n\\tag{1}\\]\n\n[…] This equation […] represents the tangent plane to the surface defined by \\(z=f(x,y)\\) at the point \\((x_0,y_0)\\). The idea behind using a linear approximation is that, if there is a point \\((x_0,_0)\\) at which the precise value of \\(f(x,y)\\) is known, then for vales of \\((x,y)\\) reasonably close to \\((x_0,y_0)\\), the linear approximation (i.e., tangent plane) yields a value that is also reasonably close to the exact value of \\(f(x,y)\\).\n– (Strang and Herman 2016, chap. 4.4)\n\nRecall that \\(f_x\\) and \\(f_y\\) refer to components of the gradient vector, \\(\\nabla f\\). We calculate that gradient as follows: \\[  \n\\begin{align*}\n\\nabla f &= \\left&lt; f_x, f_y \\right&gt; \\\\\n\\nabla f &= \\left&lt; \\frac{df}{dx}, \\frac{df}{dy} \\right&gt; \\\\\n    \\nabla f &= \\left&lt; \\frac{-4x}{\\sqrt{42-4x^2-y^2}}, \\frac{-y}{\\sqrt{42-4x^2-y^2}} \\right&gt;\n\\end{align*}\n\\] Now we can plug in our point, \\(P_0\\):\n\\[\n\\begin{align*}\n\\nabla f(P_0) &= \\left&lt; {f_x}\\left( {{x_0},{y_0}} \\right), {f_y}\\left( {{x_0},{y_0}} \\right) \\right&gt; \\\\\n    \\nabla f(P_0) &= \\left&lt; \\frac{-4(2)}{1}, \\frac{-(-5)}{1} \\right&gt; = \\left&lt; -8, 5 \\right&gt;\n\\end{align*}\n\\] We now have all elements of the tangent plane/linear approximation, Equation 1, and we can simply plug-in and compute: \\[\n\\begin{align*}\n         L(x,y) &= f(2, -5) + -8 \\left( {x - 2} \\right) + 5\\left( y - (-5) \\right) \\\\\n         L(x,y) &= 1 + -8x + 16 + 5y + 25 \\\\\n         L(x,y) &= -8x + 5y + 42\n\\end{align*}\n\\]\n\nL = lambda x, y: -8*x + 5*y + 42\n\n\n\nCode\nfig = plt.figure(figsize=(8, 4))\nax1 = plt.subplot(121,)\nax2 = plt.subplot(122, projection='3d')\n# Countour plot\nax1.contour(X, Y, Z, label=r\"$f(x)$\", alpha=.6)\nax1.contour(X, Y, L(X, Y), levels= 25, alpha=.8, cmap=\"coolwarm\", label=r\"$L(x)$\")\nax1.scatter(2, -5, s=15, color='red', label=r\"$f(2, -5) = 1$\")\nax1.scatter(2.2, -5.1, s=15, color='tab:orange', label=f\"$L(2.1, -5.1) = {round(L(2.1, -5.1), 2)}$\")\nax1.grid(alpha=.5)\nax1.legend(loc=\"upper left\", framealpha=1)\nstyle_plot(ax1)\n# ax1.view_init(-130, -90, 0)\n# 3d surface\nZ = np.ma.masked_where(Z &lt;= 0, Z)\nX = np.ma.masked_where(Z &lt;= 0, X)\nY = np.ma.masked_where(Z &lt;= 0, Y)\nax2.contour3D(X, Y, Z, corner_mask=True)\nax2.plot_surface(X, Y, L(X, Y), alpha=1)\n# ax2.contour3D(X, Y, Z, 20, cmap='gray')\nstyle_plot(ax2, z=True)\nax2.set_xlim(-5, 5)\nax2.set_ylim(-8, 8)\nax2.set_zlim(-5, 5)\nax2.scatter(2, -5, f(2, -5), color='red', s=15)\nax2.scatter(2.2, -5.1, s=15, color='tab:orange', label=f\"$L(2.1, -5.1) = {round(L(2.1, -5.1), 2)}$\")\n\nax2.view_init(0, -133, 0)\nfig.suptitle(r\"$f(x, y)$ & $L(x, y)$\")\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nWith that linear approximation established, we can now estimate our original quantity of interest, \\(f(2.1, -5.1)\\). \\[f(2.1, -5.1) \\approx L(2.1, -5.1) = -8(2.1) + 5(-5.1) + 42 \\approx \\boxed{-0.3} \\]\n\nL(2.1, -5.1)\n\n-0.29999999999999716"
  },
  {
    "objectID": "posts/iterated-expectations/index.html",
    "href": "posts/iterated-expectations/index.html",
    "title": "Iterated Expectations",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nI recently came across a list of 10 theorems/proofs that you “need to know” if you do econometrics. These were compiled by Jeffrey Wooldridge, an economist and textbook author whose introductory textbook has been fundamental to my interest in econometrics. As an exercise, I’m working through these 10 items, compiling resources, textbook passages, and data exercises that I think can make them easier to understand. The first item I’m trying to write my notes on is the Law of Iterated Expectations, but I’ll be prefacing/augmenting the notes with some discussion of basic probability for completeness.\nMy core reference is Introduction to Probability, Second Edition By Joseph K. Blitzstein, Jessica Hwang.\nTo start, I’ll simulate some data.\n# Set a random seed for reproducability\nnp.random.seed(42)\n# Define the number of people in the dataset\nnum_people = 100\n# Generate random ages - X ~ Uniform(min, max)\nages = np.random.randint(71, 79, num_people)\n# Create DataFrame\ndata = {'Person_ID': range(1, num_people + 1), 'Age': ages}\npeople_df = pd.DataFrame(data).set_index(\"Person_ID\")\n\npeople_df.head()\n\n\n\n\n\n\n\n\n\nAge\n\n\nPerson_ID\n\n\n\n\n\n1\n77\n\n\n2\n74\n\n\n3\n75\n\n\n4\n77\n\n\n5\n73\nLet’s say that these data represent life spans, thus \\(\\text{age}_i\\) is an individual’s lifespan, e.g. \\(\\text{age}_2=\\)\npeople_df.loc[2]\n\nAge    74\nName: 2, dtype: int32"
  },
  {
    "objectID": "posts/iterated-expectations/index.html#expectation",
    "href": "posts/iterated-expectations/index.html#expectation",
    "title": "Iterated Expectations",
    "section": "Expectation",
    "text": "Expectation\nThe mean of a random variable, like age above, is also referred to as its “expected value,” denoted \\(E(\\text{age})\\).\n\npeople_df['Age'].mean()\n\n74.6\n\n\nThe mean above is specifically called an arithmetic mean, defined as follows:\n\\[ \\bar{x} = \\frac{1}{n} \\sum_i^n x_i\\]\n\n(1/len(people_df)) * people_df['Age'].sum()\n\n74.60000000000001\n\n\nBut the arithmetic mean is just a special case of the more general weighted mean:\n\\[\\begin{align*}\n\\text{weighted-mean}(x) &= \\sum_i^n x_i p_i \\\\\n\\end{align*}\\]\nWhere the weights, \\(p_1, p_2, ...,p_n\\) are non-negative numbers that sum to 1. We can see that the arithmetic mean is the specific case of the weighted mean where all weights are equal\n\\[\\begin{align*}\n\\text{If } [p_1=p_2=...=p_n] &\\text{ And } [\\sum_i^n p_i =1]\\\\\n\\text{weighted-mean}(x) &= \\sum_i^n x_i \\frac{1}{n}\\\\\n\\text{weighted-mean}(x) &= \\frac{1}{n} \\sum_i^n x_i = \\bar{x}\\\\\n\\end{align*}\\]\nWe use the more general weighted mean when we define expectation.\n\nthe expected value of \\(X\\) is a weighted average of the possible values that \\(X\\) can take on, weighted by their probabilities\n– Blitzstein and Hwang (2019)\n\nMore formally, given a random variable, \\(X\\), with distinct possible values, \\(x_1, x_2, ... x_n\\), the expected value \\(E(X)\\) is defined as:\n\\[\\begin{align*}\nE(X) = & x_1P(X = x_1) + \\\\\n&x_2P(X = x_2) +\\\\\n&... + x_nP(X = x_n) \\\\\n= &\\sum_{i=1}^n x_iP(X = x_i)\n\\end{align*}\\]\nNow we’ll demonstrate this formula on our data. It’s useful here to move from our individual-level dataset, where each row is a person, to the following, where each row is a lifespan, which the probability that an individual has that lifespan.\n\nprob_table = people_df['Age'].value_counts(normalize=True)\nprob_table = prob_table.sort_index()\nprob_table\n\n71    0.08\n72    0.13\n73    0.11\n74    0.19\n75    0.12\n76    0.13\n77    0.13\n78    0.11\nName: Age, dtype: float64\n\n\nAdapting the formula above to our data, we must solve the following:\n\\[\\begin{align*}\nE(\\text{Age}) = &\\text{Age}_1P(\\text{Age}=\\text{Age}_1) + \\\\\n&\\text{Age}_2P(\\text{Age}=\\text{Age}_2) + \\\\\n&... + \\text{Age}_nP(\\text{Age}=\\text{Age}_n) \\\\\n= &\\sum_{i=1}^n \\text{Age}_iP(\\text{Age}=\\text{Age}_i)\n\\end{align*}\\]\nWhich we can do transparently using a for-loop:\n\nsummation = 0\nfor i in range(len(prob_table)):\n  summation += prob_table.index[i] * prob_table.values[i]\nsummation\n\n74.60000000000001\n\n\nAs a quick aside – this can also be expressed as the dot product of two vectors, where the dot product is defined as follows:\n\\[\n\\begin{align*}\n\\vec{\\text{Age}}\\cdot P(\\vec{\\text{Age}}) = &\\text{Age}_1P(\\text{Age}=\\text{Age}_1) + \\\\\n&\\text{Age}_2P(\\text{Age}=\\text{Age}_2) + \\\\\n&... + \\text{Age}_3P(\\text{Age}=\\text{Age}_3)\n\\end{align*}\n\\]\n\nprob_table.index.values @ prob_table.values\n\n74.6\n\n\nThough we will stick to the summation notation paired with python for-loops for consistency"
  },
  {
    "objectID": "posts/iterated-expectations/index.html#conditional-expectation",
    "href": "posts/iterated-expectations/index.html#conditional-expectation",
    "title": "Iterated Expectations",
    "section": "Conditional Expectation",
    "text": "Conditional Expectation\nWe often have more than one variable available to us in an analysis. Below I simulate the variable gender:\n\nnp.random.seed(45)\npeople_df['Gender'] = np.random.choice(['Female', 'Male'], len(people_df))\npeople_df.head()\n\n\n\n\n\n\n\n\n\nAge\nGender\n\n\nPerson_ID\n\n\n\n\n\n\n1\n77\nMale\n\n\n2\n74\nFemale\n\n\n3\n75\nMale\n\n\n4\n77\nFemale\n\n\n5\n73\nFemale\n\n\n\n\n\n\n\n\nEach row in our dataset represents an individual person, and we now have access to both their gender and their life-span. It follows that we may be interested in how life-span varies across gender. In code, this entails a groupby operation, grouping on gender before calculting the mean age:\n\npeople_df.groupby('Gender')['Age'].mean()\n\nGender\nFemale    74.672727\nMale      74.511111\nName: Age, dtype: float64\n\n\nThe code in this case resembles the formal notation of a conditional expectation: \\(E(\\text{Age} \\mid \\text{Gender}=\\text{Gender}_j)\\), where each \\(\\text{Gender}=\\text{Gender}_j\\) is a distinct event.\nIf we are interested specifically in the mean life-span given the event that gender is equal to male (a roundabout way of saying the average life-span for males in the data), we could calculate the following\n\\(E(\\text{Age} \\mid \\text{Gender}=\\text{Male})\\)\n\npeople_df.groupby('Gender')['Age'].mean()['Male']\n\n74.5111111111111\n\n\nThese groupby operations in pandas obscure some of the conceptual stuff happening inside the conditional expectation, which we’ll delve deeper into now.\nSo what exactly is the conditional expectation, \\(E(X \\mid Y=y)\\)?\nBefore answering this, it will be useful to refresh the related concept of conditional probability:\n\nIf \\(X=x\\) and \\(Y=y\\) are events with \\(P(Y=y)&gt;0\\), then the conditional probability of \\(X=x\\) given \\(Y=y\\) is denoted by \\(P(X=x \\mid Y=y)\\), defined as\n\\[ P(X=x \\mid Y=y) = \\frac{P(X=x , Y=y)}{P(Y=y)} \\]\n– Blitzstein and Hwang (2019)\n\nThis formula specifically describes the probability of the event, \\(X=x\\), given the evidence, an observed event \\(Y=y\\).\nWe want to shift to describing a mean conditional on that evidence, and we include that information via the weights in the expectation.\n\nRecall that the expectation \\(E(X)\\) is a weighted average of the possible values of \\(X\\), where the weights are the PMF values \\(P(X = x)\\). After learning that an event \\(Y=y\\) occurred, we want to use weights that have been updated to reflect this new information.\n– Blitzstein and Hwang (2019)\n\nThe key point here is that just the weights that each \\(x_i\\) gets multiplied by will change, going from the probability \\(P(X=x)\\) to the conditional probability \\(P(X=x \\mid Y=y)\\).\nArmed with conditional probability formula above, we can define how to compute the conditional expected value \\[\n\\begin{align*}\nE(X \\mid Y=y) &= \\sum_{x} x P(X=x \\mid Y=y) \\\\\n&= \\sum_{x} x \\frac{P(X=x , Y=y)}{P(Y=y)}\n\\end{align*}\n\\]\nReturning to our example with data, we substitute terms to find the following: \\[\n\\begin{align*}\nE(\\text{Age} \\mid \\text{Gender}=\\text{Male}) &= \\sum_{i=1}^n \\text{Age}_iP(\\text{Age}=\\text{Age}_i \\mid \\text{Gender}=\\text{Male}) \\\\\n&= \\sum_{i=1}^n \\text{Age}_i \\frac{P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Male})}{P(\\text{Gender}=\\text{Male})}\n\\end{align*}\n\\]\nWe can explicitly compute this with a for-loop in python, as we did for \\(E(X)\\), but this time we will need to do a little up front work and define components we need for calculating the weights, \\(\\frac{P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Male})}{P(\\text{Gender}=\\text{Male})}\\)\n\nComponents\n\nThe joint probability distribution: \\(P(\\text{Age}=\\text{Age}_i, \\text{Gender} = \\text{Male})\\)\nThe probability of the event, \\(P(\\text{Gender}=\\text{Male})\\)\n\nWhere 1.) is the following:\n\nP_Age_Gender = pd.crosstab(people_df['Age'],\n                           people_df['Gender'],\n                           normalize='all')\nP_Age_Gender['Male']\n\nAge\n71    0.03\n72    0.07\n73    0.05\n74    0.08\n75    0.07\n76    0.05\n77    0.06\n78    0.04\nName: Male, dtype: float64\n\n\nand 2.) is:\n\nP_Gender = people_df['Gender'].value_counts(normalize=True)\nP_Gender.loc['Male']\n\n0.45\n\n\nWith those two pieces, we’ll convert the following into a for-loop: \\[\n\\sum_{i=1}^n \\text{Age}_i \\frac{P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Male})}{P(\\text{Gender}=\\text{Male})}\n\\]\n\nE_age_male = 0\nn = len(P_Age_Gender['Male'])\nfor i in range(n):\n  weight = P_Age_Gender['Male'].values[i] / P_Gender.loc['Male']\n  E_age_male += P_Age_Gender['Male'].index[i] * weight\nE_age_male\n\n74.51111111111112\n\n\nWe confirm that this is equal to the result of the more direct groupby:\n\npeople_df.groupby('Gender')['Age'].mean()['Male']\n\n74.5111111111111"
  },
  {
    "objectID": "posts/iterated-expectations/index.html#the-law-of-iterated-expectations",
    "href": "posts/iterated-expectations/index.html#the-law-of-iterated-expectations",
    "title": "Iterated Expectations",
    "section": "The Law of Iterated Expectations",
    "text": "The Law of Iterated Expectations\nThe law of iterated expectations, also referred to as the law of total expectation, the tower property, Adam’s law, or, my favorite, LIE, states the following: \\[E(X) = E(E(X \\mid Y))\\] Which is to say, the weighted average of \\(X\\) is equal to the weighted average of the weighted averages of \\(X\\) conditional on each value of \\(Y\\). This isn’t a particularly useful sentence, so let’s return to our example data. We plug in our values as follows: \\[E(\\text{Age}) = E(E(\\text{Age} \\mid \\text{Gender}))\\] Now it is useful to break this into some components that we’ve seen before. We previously found \\[\nE(\\text{Age} \\mid \\text{Gender}=\\text{Male}) =  \\sum_{i=1}^n \\text{Age}_i \\frac{P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Male})}{P(\\text{Gender}=\\text{Male})}\n\\]\nOver all \\(\\text{Gender}_j\\), we have the more generalizable expression: \\[\nE(\\text{Age} \\mid \\text{Gender}=\\text{Gender}_j)\n\\]\nWhich can tell us about any gender, not just \\(\\text{Gender}=\\text{Male}\\). This is equivalent to the expression:\n\\[\n\\begin{align*}\nE(\\text{Age} \\mid \\text{Gender} = \\text{Gender}_j) = \\sum_{i=1}^n \\text{Age}_i \\frac{P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Gender}_j) }{P(\\text{Gender}=\\text{Gender}_j)}\n\\end{align*}\n\\]\nGiven this, let’s return to the informal definition of the LIE, but break it into parts. The weighted average of \\(X\\) is equal to:\n1. The weighted average of 2. the weighted averages of \\(X\\) conditional on each value of \\(Y\\)“.\nThe expression above, \\(E(\\text{Age} \\mid \\text{Gender}=\\text{Gender}_j)\\) is equivalent to 2.) “the weighted averages of \\(X\\) conditional on each value of \\(Y\\).” So what we need to do now is find the weighted average of that expression. We’ll set up in the next few lines\n\\[\\begin{align*}\nE(\\text{Age}) &=E( \\underbrace{E(\\text{Age} \\mid \\text{Gender}=\\text{Gender}\\_j)}_{\\text{weighted averages conditional on each gender}} ) \\\\\n&=E(\\sum_{i} \\text{Age}_i \\frac{P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Gender}_j) }{P(\\text{Gender}=\\text{Gender}_j)}) \\\\\n\\end{align*}\\]\nWith that set up, we’ll now write out the last weighted average explicitly. Note that the variation in \\(\\text{Age}_i\\) has been accounted for – we are now averaging over gender, \\(\\text{Gender}_j\\).\n\\[\\begin{align*}\n&=\\sum_j (\\sum_{i} \\text{Age}_i \\frac{P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Gender}_j) }{P(\\text{Gender}=\\text{Gender}_j)}) P(\\text{Gender}=\\text{Gender}_j) \\\\\n&=\\sum_j \\sum_{i} \\text{Age}_i P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Gender}_j) \\\\\n\\end{align*}\\]\nSince \\(j\\) only appears in one of these two terms, we can rewrite this as follows:\n\\[\\begin{align*}\n&=  \\sum_{i} \\text{Age}_i \\sum_j P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Gender}_j)\n\\end{align*}\\]\nHere I’ll pause, because the next steps can be clarified with code. \\(P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Gender}_j)\\) is the joint probability distribution of age and gender, and it helps to take a look at exactly what it is in pandas:\n\nP_Age_Gender\n\n\n\n\n\n\n\n\nGender\nFemale\nMale\n\n\nAge\n\n\n\n\n\n\n71\n0.05\n0.03\n\n\n72\n0.06\n0.07\n\n\n73\n0.06\n0.05\n\n\n74\n0.11\n0.08\n\n\n75\n0.05\n0.07\n\n\n76\n0.08\n0.05\n\n\n77\n0.07\n0.06\n\n\n78\n0.07\n0.04\n\n\n\n\n\n\n\n\nLet’s compute the summation of \\(P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Gender}_j)\\) over \\(\\text{Gender}_j\\) and see what we get.\n\nP_Age_Gender[\"Male\"] + P_Age_Gender[\"Female\"]\n\nAge\n71    0.08\n72    0.13\n73    0.11\n74    0.19\n75    0.12\n76    0.13\n77    0.13\n78    0.11\ndtype: float64\n\n\nInterestingly, that is the exact same thing we get if we simply compute the probability of each age, \\(P(\\text{Age}=\\text{Age}_i)\\)\n\npeople_df['Age'].value_counts(normalize=True).sort_index()\n\n71    0.08\n72    0.13\n73    0.11\n74    0.19\n75    0.12\n76    0.13\n77    0.13\n78    0.11\nName: Age, dtype: float64\n\n\nSo when you sum \\(P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Gender}_j)\\) only over \\(\\text{Gender}_j\\), you’re just left with \\(P(\\text{Age}=\\text{Age}_i)\\). This result stems from the definition of the Marginal PMF:\n\nFor the discrete random variables \\(X\\) and \\(Y\\), the marginal PMF of \\(X\\) is:\n\\[P(X=x) = \\sum_y P(X=x, Y=y)\\] – Blitzstein and Hwang (2019)\n\nand with this definition in mind we can finish the proof for the LIE: \\[\n\\begin{align*}\nE(\\text{Age})  &= \\sum_{i} \\text{Age}_i \\sum_j P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Gender}_j) \\\\\n&= \\sum_{i} \\text{Age}_i  P(\\text{Age}=\\text{Age}_i) \\\\\n&= E(\\text{Age})\n\\end{align*}\n\\]\nWe can directly show the last bit, \\(E(\\text{Age}) = \\sum_j \\sum_{i} \\text{Age}_i P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Gender}_j)\\) using the joint probability distribution object from before:\n\nP_Age_Gender.head()\n\n\n\n\n\n\n\n\nGender\nFemale\nMale\n\n\nAge\n\n\n\n\n\n\n71\n0.05\n0.03\n\n\n72\n0.06\n0.07\n\n\n73\n0.06\n0.05\n\n\n74\n0.11\n0.08\n\n\n75\n0.05\n0.07\n\n\n\n\n\n\n\n\n\n(P_Age_Gender\n .sum(axis=1) # sum over j\n .reset_index() # bring out Age_i\n .product(axis=1) # Age_i * P(Age=Age_i)\n .sum() # sum over i\n )\n\n74.6\n\n\n\npeople_df['Age'].mean()\n\n74.6"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Peter Amerkhanian",
    "section": "",
    "text": "Data Scientist @ CA Department of Social Services\nUC Berkeley MPP ’23, BA ’16\n\n\nI’m Data Scientist/Policy Analyst, currently working on computational and data analysis problems at The California Department of Social Services.\n\nI’m skilled in policy analysis, data engineering, machine learning, and various computational methods.\n\nI’m passionate about public service."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "The following are a collection of my informal notes on topics in programming, math, statistics, and policy analysis. If you find errors or else find them helpful, I’d love to hear from you.\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nTopics\n\n\nTime\n\n\n\n\n\n\nMar 17, 2024\n\n\nProduction Maximization with Lagrange Mutlipliers\n\n\nR, Calculus\n\n\n5 min\n\n\n\n\nMar 9, 2024\n\n\nA Minima Problem in 3D\n\n\nR, Calculus\n\n\n7 min\n\n\n\n\nMar 2, 2024\n\n\nLinear Approximation in 3D\n\n\nPython, Calculus\n\n\n3 min\n\n\n\n\nDec 1, 2023\n\n\nIterated Expectations\n\n\nPython, Probability\n\n\n7 min\n\n\n\n\nNov 3, 2023\n\n\nNewton’s Method From The Ground Up\n\n\nPython, Calculus\n\n\n10 min\n\n\n\n\nJul 4, 2023\n\n\nSimple Constrained Optimization in 2D\n\n\nPython, Calculus\n\n\n5 min\n\n\n\n\nAug 6, 2022\n\n\nBuy vs. Rent, A Financial Modeling Workflow in Python\n\n\nPython, Applications\n\n\n7 min\n\n\n\n\nJul 20, 2022\n\n\nReliable PDF Scraping with tabula-py\n\n\nPython, Programming\n\n\n6 min\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/lagrange-cobb-douglas/index.html",
    "href": "posts/lagrange-cobb-douglas/index.html",
    "title": "Utility Maximization with Lagrange Mutlipliers",
    "section": "",
    "text": "Code\nlibrary(plotly)\nlibrary(dplyr)\n\nscene &lt;- list(\n  camera = list(eye = list(\n    x = -2.2, y = 1.1, z = 1.2\n  )),\n  xaxis = list(title = \"L\"),\n  yaxis = list(title = \"K\"),\n  zaxis = list(title = \"$\")\n)\n\n\n\nThe Optimization Problem\n\nA company has determined that its production level is given by the Cobb-Douglas function \\(f(x,y)=2.5x^{0.45}y^{0.55}\\) where \\(x\\) represents the total number of labor hours in 1 year and \\(y\\) represents the total capital input for the company. Suppose 1 unit of labor costs $40 and 1 unit of capital costs $50. Use the method of Lagrange multipliers to find the maximum value of \\(f(x,y)=2.5x^{0.45}y^{0.55}\\) subject to a budgetary constraint of $500,000 per year.\n– (Strang and Herman 2016, chap. 4.8)\n\n\\[\n\\begin{align}\nP(L, K) &:= f(x, y) \\\\\nP(L, K) &= 2.5L^{0.45}K^{0.55} \\\\\ng(L, K) &= 40L + 50K - 500,000\n\\end{align}\n\\tag{1}\\]\nWe set the equation up in R so that we can inspect a plot and better understand the optimization problem.\n\nP_l_k &lt;- function(L, K) {\n  2.5 * L ^ (0.45) * K ^ (0.55)\n}\ng_l_k &lt;- function(L, K) {\n  40 * L + 50 * K - 500000\n}\nn &lt;- 100\nL &lt;- seq(0, 6000, length.out = n)\nK &lt;- seq(0, 6000, length.out = n)\nP &lt;- outer(L, K, P_l_k)\ng &lt;- outer(L, K, g_l_k)\n\n\n\nCode\nplot_ly(\n  x = L,\n  y = K,\n  z = P,\n  type = \"surface\",\n  name = \"P(L,K)\"\n) %&gt;%\n  colorbar(title = \"P(L,K) - Production\") %&gt;%\n  add_trace(\n    x = L,\n    y = K,\n    z = g,\n    type = \"surface\",\n    colorscale = \"coolwarm\",\n    name = \"g(L,K)\",\n    colorbar = list(title = \"g(L,K) - Budget Constraint\")\n  ) %&gt;% layout(scene = scene)\n\n\n\n\n\n\nWe see that the production function \\(P\\) and the cost function \\(g\\) are both surfaces that intersect. We are looking for the highest possible point in \\(P\\) that does not exceed the constraint \\(g\\), which will be somewhere around their intersection. Note that generally, all values below the intersection are possible, though not profit-maximizing, points. The points higher than the intersection are more profit-maximizing, but are not possible with this budget constraint.\n\n\nMaximizing using the Method of Lagrange Multipliers\nWe adapt the Lagrange multiplier problem-solving strategy from (Strang and Herman 2016, chap. 4.8) to our function input, and set up the following system of equations, for which we will solve for \\(L_0\\) and \\(K_0\\):\n\\[\n\\begin{align*}\n\\nabla P(L_0, K_0) &= \\lambda \\nabla g(L_0, K_0) \\\\\ng(L_0, K_0) &= 0\n\\end{align*}\n\\tag{2}\\]\nAt this point, we will need to do some calculations to find each function in Equation 1’s gradient.\n\\[\n\\begin{align*}\n\\nabla P(L_0, K_0) &= \\left&lt;  \\frac{1.125K^{0.55}}{L^{0.55}} , \\frac{1.375L^{0.45}}{K^{0.45}}\\right&gt; \\\\\n\\nabla g(L_0, K_0) &= \\left&lt; 40, 50 \\right&gt; \\\\\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\n&\\begin{cases}\n\\left&lt;  \\frac{1.125K^{0.55}}{L^{0.55}} , \\frac{1.375L^{0.45}}{K^{0.45}}\\right&gt; &= \\lambda \\left&lt; 40, 50 \\right&gt; \\\\\n40L + 50K - 500,000 &= 0\n\\end{cases} \\\\\n&\\begin{cases}\n\\frac{1.125K^{0.55}}{L^{0.55}} &= 40 \\lambda\\\\\n\\frac{1.375L^{0.45}}{K^{0.45}} &= 50 \\lambda \\\\\n40L + 50K - 500,000 &= 0\n\\end{cases} \\\\\n&\\begin{cases}\n\\frac{1.125K^{0.55}}{40L^{0.55}} &= \\lambda\\\\\n\\frac{1.375L^{0.45}}{50K^{0.45}} &= \\lambda \\\\\n40L + 50K - 500,000 &= 0\n\\end{cases} \\\\\n&\\begin{cases}\n\\frac{1.125K^{0.55}}{40L^{0.55}} &= \\frac{1.375L^{0.45}}{50K^{0.45}} \\\\\n40L + 50K - 500,000 &= 0\n\\end{cases} \\\\\n&\\begin{cases}\n5.5L &= 5.625K \\\\\n40L + 50K - 500,000 &= 0\n\\end{cases} \\\\\n&\\begin{cases}\n5.5L- 5.625K &= 0 \\\\\n40L + 50K &= 500,000\n\\end{cases}\n\\end{align*}\n\\] We not have a clear linear system of equations that we can solve via some substitution and algebraic manipulation:\n\\[\n\\begin{align*}\nL &= \\frac{5.625K}{5.5} \\\\\n40 (\\frac{5.625K}{5.5}) + 50K &= 500,000 \\\\\nK &= \\frac{500,000}{(40 (\\frac{5.625}{5.5}) + 50)} = \\boxed{K = \\$ 5,500} \\\\\n40 L + 50(5,500) &= 500,000 \\\\\nL &= \\frac{500,000 - 50(5,500)}{40} = \\boxed{L = 5,625 \\, \\text{labor hours}}\n\\end{align*}\n\\]\nWe’ll now plug those values for capital and labor into our production function and see how much output this maximizing parameter combination produces (we’ll round given we are solving for whole output):\n\nP_l_k(5625, 5500) %&gt;% round()\n\n[1] 13890\n\n\nWhen we return to the plot of the product function and budget constraint, we can see that this point clearly is the highest possible output under the constraints.\n\n\nCode\nplot_ly(\n  x = L,\n  y = K,\n  z = P,\n  type = \"surface\",\n  name = \"P(L,K)\"\n) %&gt;%\n  colorbar(title = \"P(L,K) - Production\") %&gt;%\n  add_trace(\n    x = L,\n    y = K,\n    z = g,\n    type = \"surface\",\n    colorscale = \"coolwarm\",\n    name = \"g(L,K)\",\n    colorbar = list(title = \"g(L,K) - Budget Constraint\")\n  ) %&gt;%\n  add_trace(\n    x = 5625,\n    y = 5500,\n    z = P_l_k(5625, 5500) %&gt;% round(),\n    type = \"scatter3d\",\n    mode = \"markers\",\n    marker = list(size = 5, color = \"black\")\n  ) %&gt;%\n  layout(scene = scene)\n\n\n\n\n\n\nHowever, in \\(R^3\\), contour plots offer a much more clear way of visualizing the result of our solution point for this contrained optimization problem.\n\n\nCode\nplot_ly(\n  x = L,\n  y = K,\n  z = P,\n  type = \"contour\",\n  name = \"P(L,K)\"\n) %&gt;%\n  colorbar(title = \"P(L,K) - Production\") %&gt;%\n  add_trace(\n    x =  L,\n    y = 10000 - 4 * K / 5,\n    type = 'scatter',\n    mode = 'lines',\n    name = \"Budget Constraint\",\n    color = \"red\"\n  ) %&gt;%\n  add_trace(\n    x = 5625,\n    y = 5500,\n    type = \"scatter\",\n    mode = \"markers\",\n    marker = list(\n      size = 10,\n      color = \"black\",\n      name = \"P(L*,K*)\"\n    )\n  ) %&gt;%\n  layout(xaxis = list(range = c(0, max(L))),\n         yaxis = list(range = c(0, max(K))))\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nStrang, Gilbert, and Edwin Herman. 2016. Calculus Volume 3. OpenStax."
  },
  {
    "objectID": "posts/minima-3d/index.html",
    "href": "posts/minima-3d/index.html",
    "title": "A Minima Problem in 3D",
    "section": "",
    "text": "Code\nlibrary(plotly)\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/minima-3d/index.html#footnotes",
    "href": "posts/minima-3d/index.html#footnotes",
    "title": "A Minima Problem in 3D",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is beyond the scope of this post, but based on the inequality \\(0 \\leq x \\leq y \\rightarrow  x^2 \\leq y^2\\).↩︎"
  },
  {
    "objectID": "posts/pdf-scraping/index.html",
    "href": "posts/pdf-scraping/index.html",
    "title": "Reliable PDF Scraping with tabula-py",
    "section": "",
    "text": "Summary\n\nUse a combination of tabula‘s read_pdf() function and pandas’ various data manipulation functions in Python to accurately scrape .pdf files\n\n\n\nPrerequisites/Assumptions\n\nWindows 10 with administrator privileges (for setting environmental variables)\nJava SE Development Kit installed on your machine (download)\n\nset Java’s PATH environmental variable to point to the Java directory (see more here under “Get tabula-py working (Windows 10)”)\n\n\nPython version ~3.8 ish (I’m using Python 3.9.12 in Anaconda)\n\nAnaconda included packages - Pandas and NumPy\nLibraries maybe not included in Anaconda: requests, tabula-py\n\n\n\n\nProblem Narrative\nI’m interested in conducting a data analysis that involves the market value of single family homes in San Mateo County, California. This data can be hard to come by, but I’ve found a good county level resource – The San Mateo Association of Realtors’ “Market Data” page.\n\n\n\n\n\n\n\n\nFig 1: San Mateo Realtors Data Download Page\n\n\n\nHowever, to my dismay, I find that when I download one of these reports, I only get a .pdf containing a single table. It seems to be some sort of export of an Excel table, but the Association of Realtors has not made the actual spreadsheet available. Here is an example of one of their .pdf reports – in this case for April 2022:\n\n\n\n\n\n\n\n\nFig 2: Example PDF report :(\n\n\n\nThis is the exact data I want, but there are a few key issues: - The data are in .pdf files - You can only download monthly data files one at a time\n\n\nSolution\nI’ll solve this issue by writing a script to do the following: - Iterate through the urls of each of the monthly reports going back to 2011. For each report: - download its .pdf - parse and save the data from its .pdf\nStart with loading in the necessary libraries:\nimport pandas as pd\nimport numpy as np\nimport requests\nfrom tabula import read_pdf\nGet right into the script, which implements the pseudo-code I outlined above:\nagent = ['Mozilla/5.0 (Windows NT 10.0; Win64; x64)',\n         'AppleWebKit/537.36 (KHTML, like Gecko)',\n         'Chrome/91.0.4472.114 Safari/537.36']\nrows = []\nheaders = {'user-agent': \" \".join(agent)}\nfor year in range(2011, 2021):\n    for month in range(1, 12):\n        base = \"https://www.samcar.org/userfiles/file/salesstats/\"\n        url = base + f\"SF_{year}{str(month).zfill(2) }.pdf\"\n        print(url)\n        r = requests.get(url,\n                         stream=True,\n                         headers=headers)\n        open('holder.pdf', 'wb').write(r.content)\n        df = read_pdf(\"holder.pdf\", pages=\"all\")\n        table = df[0].iloc[-1, :]\n        table[\"date\"] = f\"{year}-{str(month).zfill(2)}\"\n        rows.append(table)\nNote: I’m defining agent in order to preempt being blocked by the site (read more).\nWhat’s remarkable about tabula.read_pdf() is that it just works. I didn’t have to really do any tinkering or iterating to get it going. Once it had access to the downloaded .pdf files, it easily and quickly parsed them.\nNow I run into something unique to this data – some of the .pdf tables had slightly different column names over the years. I implement a fix for that with the following code:\ncleaned_rows = []\nfor row in rows:\n    try:\n        new = row.rename(\n            {\"Sales\": \"Closed Sales\",\n             \"Sold\": \"Closed Sales\",\n             \"Avg Sales Price\": \"Average Sales Price\",\n             \"Avg SalePrice\": \"Average Sales Price\",\n             \"Unnamed: 3\": \"Closed Sales\",\n             \"Unnamed: 5\": \"Average Sales Price\"})[[\"date\",\n                                                    \"Closed Sales\",\n                                                    \"Average Sales Price\"]]\n        cleaned_rows.append(new.to_frame())\n    except KeyError:\n        print(\"******error\")\nWith the data retrieved and parsed, I perform some final cleaning and arrangement steps before exporting to a .csv\nall_years = pd.concat(cleaned_rows, axis=1)\n# Transpose the data and set `date` as the index\nfinal_df = all_years.T.set_index(\"date\")\n# Get the dollar signs and commas out. E.g. $1,658,900 -&gt; 1658900\nfinal_df[\"Average Sales Price\"] = (final_df[\"Average Sales Price\"]\n                                   .str.replace(\"[\\$,]\",\n                                                \"\",\n                                                regex=True)\n                                   .astype(int))\n# Closed Sales is discrete count data, so we convert to `int`\nfinal_df[\"Closed Sales\"] = final_df[\"Closed Sales\"].astype(int)\nfinal_df.to_csv(\"realtors_data_san_mateo.csv\")\nThe final product is a satisfying time series data set of the number of closed single family home sales and the average price of those sales over time.\n\n\n\n\n\n\n\n\nFig 3: Final .csv open in Excel\n\n\n\nThat’s surprisingly it.\n\n\nConclusion\nTabula-py is a very convenient and powerful .pdf parser (ported from Java) and easily handled basically all of the .pdfs I put through it."
  },
  {
    "objectID": "posts/simple-constrained-optimization/index.html",
    "href": "posts/simple-constrained-optimization/index.html",
    "title": "Simple Constrained Optimization in 2D",
    "section": "",
    "text": "Code\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nThe following are my notes on a basic calculus 1 homework question. I liked the question a lot, so decided to write out it all out for my future use."
  },
  {
    "objectID": "posts/simple-constrained-optimization/index.html#a-note-on-the-second-derivative-test",
    "href": "posts/simple-constrained-optimization/index.html#a-note-on-the-second-derivative-test",
    "title": "Simple Constrained Optimization in 2D",
    "section": "A note on the second derivative test",
    "text": "A note on the second derivative test\nWhen we determined \\(V''(x^\\star)&lt;0\\), we determined that coming off of this critical value, the slope of the function is decreasing – we are coming down from a maximum. In the figure below, the critical value is plotted as a black point. It’s clear that if we were to move slightly to the right of the critical value, the slope of the function would decrease, and we see this directly in the plots of the first and second derivatives.\n\n\nCode\nfig, ax = plt.subplots(1, 3)\nax[0].plot(x, V(x))\nax[0].grid(alpha=.5)\nax[0].set_title(r\"$V(x^*) = 8606.62$\")\nax[0].set_ylim(8500,8650)\nax[0].set_xlim(24, 28)\nax[0].plot(x_critical, V(x_critical), \"&gt;\", color=\"grey\", markeredgecolor=\"black\")\n\nax[1].axhline(0, color=\"grey\", linestyle=\"--\")\nax[1].plot(x, V_1(x), c=\"tab:blue\")\nax[1].grid(alpha=.5)\nax[1].set_xlabel(r\"$x$\")\nax[1].set_title(r\"$V'(x^*) = 0$\")\nax[1].set_xlim(20, 30)\nax[1].plot(x_critical, V_1(x_critical), \"&gt;\", color=\"grey\", markeredgecolor=\"black\")\n\nax[2].axhline(0, color=\"grey\", linestyle=\"--\")\nax[2].plot(x[V_2(x) &gt; 0], V_2(x)[V_2(x) &gt; 0], c=\"tab:green\")\nax[2].plot(x[V_2(x) &lt; 0], V_2(x)[V_2(x) &lt; 0], c=\"tab:red\")\nax[2].grid(alpha=.5)\nax[2].set_title(r\"$V''(x^*) \\approx -39$\")\nax[2].plot(x_critical, V_2(x_critical), \"o\", alpha=1, color=\"red\", markeredgecolor=\"black\")\n\nfig.tight_layout();\n\n\n\n\n\n\n\n\n\nExpanding on this point, consider another arbitrary function that has more than one critical point, \\(f(x)=2x^3 - 100x^2\\). In this case the function has one maximum and one minimum, so the second derivative test will be more important for analyzing each point. The function is plotted below, along with its first and second derivative.\n\n\nCode\nf = lambda x: 2*x**3 - 100*x**2\nf_1 = lambda x: 6*x**2 - 200*x\nf_2 = lambda x: 12*x - 200\n\nx = np.linspace(-25, 50, 100)\n\nfig, ax = plt.subplots(1, 3)\nax[0].plot(x, f(x))\nax[0].grid(alpha=.5)\nax[0].plot(100/3, f(100/3), \"&gt;\", color=\"grey\", markeredgecolor=\"black\")\nax[0].plot(0, f(0), \"&gt;\", color=\"grey\", markeredgecolor=\"black\")\n\nax[0].set_title(r\"$f(x)$\")\n\nax[1].axhline(0, color=\"grey\", linestyle=\"--\")\nax[1].plot(x, f_1(x), c=\"tab:blue\")\nax[1].grid(alpha=.5)\nax[1].plot(100/3, f_1(100/3), \"&gt;\", color=\"grey\", markeredgecolor=\"black\")\nax[1].plot(0, f_1(0), \"&gt;\", color=\"grey\", markeredgecolor=\"black\")\n\nax[1].set_title(r\"$f'(x)$\")\n\nax[2].axhline(0, color=\"grey\", linestyle=\"--\")\nax[2].plot(x[f_2(x) &gt; 0], f_2(x)[f_2(x) &gt; 0], c=\"tab:green\")\nax[2].plot(x[f_2(x) &lt; 0], f_2(x)[f_2(x) &lt; 0], c=\"tab:red\")\n\nax[2].grid(alpha=.5)\nax[2].plot(100/3, f_2(100/3), \"o\", color=\"green\", markeredgecolor=\"black\")\nax[2].plot(0, f_2(0), \"o\", color=\"red\", markeredgecolor=\"black\")\n\nax[2].set_title(r\"$f''(x)$\")\n\nfig.suptitle(r\"$f(x) = 2x^3 - 100x^2$\")\n\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nIf we were to want to maximize this function by taking the first derivative and solving for 0, we would find two critical values – a maximum and a minimum. In this case, the second derivative test would be used to conclude which of these points is the maximum and which is the minimum. We substitute each critical value into the second derivative- \\(f''(x^\\star)&lt;0\\) denotes the maximum (the slope of the function is decreasing off of this point), while \\(f''(x^\\star)&gt;0\\) denotes the maximum (the slope of the function is increasing off of this point)."
  }
]