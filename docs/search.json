[
  {
    "objectID": "posts/ipums-wth-r/index.html",
    "href": "posts/ipums-wth-r/index.html",
    "title": "Working with ACS microdata in R",
    "section": "",
    "text": "Code\npacman::p_load(dplyr,\n               ggplot2,\n               # Statistics\n               modelsummary,\n               srvyr,\n               survey,\n               # Webscraping\n               httr,\n               rvest,\n               readr,\n               glue,\n               # Census\n               tidycensus,\n               ipumsr,\n               # XML Parsing\n               xml2,\n               purrr,\n               stringr)\nThe following are my notes on how to use American Community Survey (ACS) microdata, leveraging the University of Minnesota’s Integrated Public Use Microdata Series (IPUMS).1 I cover:\nI’ll start with a motivating question:"
  },
  {
    "objectID": "posts/ipums-wth-r/index.html#aggregate-data-with-tidycensus",
    "href": "posts/ipums-wth-r/index.html#aggregate-data-with-tidycensus",
    "title": "Working with ACS microdata in R",
    "section": "Aggregate data with tidycensus",
    "text": "Aggregate data with tidycensus\nAnswering that question is straightforward using the Census’ aggregate data – pre-calculated descriptive statistics for aggregate geographies. I typically retrieve aggregate data via the Census API.2 In R, the tidycensus package provides an easy-to-use wrapper for that API. Note that I set up an API key for the U.S. Census and I’m storing it in my .Renviron file as census_api_key=\"yourkeyhere\".\n\ncensus_api_key(Sys.getenv(\"census_api_key\"))\n\nI query B19013_001, the median household income variable, using the 2022 1-year American Community Survey sample and I filter down to Oakland’s GEOID, 0653000, which is a combination of the state code for California, 06, and the place code for Oakland, 53000.3 I’ll throw in the total population variable for good measure:\n\noakland_stats &lt;- get_acs(\n  geography = \"place\",\n  variables = c(\n    median_hh_income = \"B19013_001\",\n    total_pop = \"B17001_001\"\n  ),\n  state = \"CA\",\n  year = 2022,\n  survey = \"acs1\"\n)\noakland_stats &lt;- oakland_stats %&gt;% filter(GEOID == '0653000')\noakland_stats %&gt;% select(c(variable, estimate, moe))\n\n\n\nTable 1: Total Population and Median Household Income in Oakand, CA via Aggregate Data\n\n\n\n\n\n\n\nvariable\nestimate\nmoe\n\n\n\n\ntotal_pop\n426323\n811\n\n\nmedian_hh_income\n93146\n6232\n\n\n\n\n\n\n\n\n\nDone! We now know the population and median household income for Oakland in 2022, along with a margin of error. See (Walker 2023) for a comprehensive treatment of working with aggregate census data."
  },
  {
    "objectID": "posts/ipums-wth-r/index.html#microdata-with-ipumsipumsr",
    "href": "posts/ipums-wth-r/index.html#microdata-with-ipumsipumsr",
    "title": "Working with ACS microdata in R",
    "section": "Microdata with IPUMS/ipumsr",
    "text": "Microdata with IPUMS/ipumsr\nWhat if I wanted to have access to the underlying data used to calculate the median? Maybe I want to try a different standard error specification for that statistic, or calculate other statistics, like the average household income. These tasks would all entail accessing census microdata – household and/or individual level census data.\nOne of the most popular sources for downloading census microdata is the University of Minnesota’s Integrated Public Use Microdata Series (IPUMS). The IPUMS team provides a centralized API for downloading census microdata, comprehensive documentation for working with the data, and harmonized variables across time (Walker 2023, chap. 9).\nThe easiest way to access IPUMS data in R is with the ipumsr package, which the IPUMS team maintains and which allows users to submit API requests to IPUMS directly from R (Greg Freedman Ellis, Derek Burk, and Finn Roberts 2024) . To get set up, I registered for an IPUMS API key, stored the key in my .Renviron file, and will configure the key in ipumsr as follows:\n\nset_ipums_api_key(Sys.getenv(\"ipums_api_key\"))\n\nThe ipumsr website provides details on what survey products the project currently supports, as does the ipums_data_collections() function:\n\nipums_data_collections() %&gt;%\n  filter(api_support == TRUE) %&gt;% \n  arrange(desc(collection_type))\n\n\n\nTable 2: IPUMS API Collections\n\n\n\n\n\n\n\ncollection_name\ncollection_type\ncode_for_api\napi_support\n\n\n\n\nIPUMS USA\nmicrodata\nusa\nTRUE\n\n\nIPUMS CPS\nmicrodata\ncps\nTRUE\n\n\nIPUMS International\nmicrodata\nipumsi\nTRUE\n\n\nIPUMS ATUS\nmicrodata\natus\nTRUE\n\n\nIPUMS AHTUS\nmicrodata\nahtus\nTRUE\n\n\nIPUMS MTUS\nmicrodata\nmtus\nTRUE\n\n\nIPUMS NHIS\nmicrodata\nnhis\nTRUE\n\n\nIPUMS MEPS\nmicrodata\nmeps\nTRUE\n\n\nIPUMS NHGIS\naggregate data\nnhgis\nTRUE\n\n\n\n\n\n\n\n\n\nI’ll look at IPUMS USA since my motivating question involves median household income for a year (2022), and IPUMS USA offers annual data from decennial censuses 1790-2010 and American Community Surveys (ACS) 2000-present (Ruggles et al. 2024). We can check out the newest products they have in the USA collection as follows:\n\nget_sample_info(collection=\"usa\") %&gt;%\n  arrange(desc(name)) %&gt;%\n  head(5)\n\n\n\nTable 3: IPUMS USA Products\n\n\n\n\n\n\n\nname\ndescription\n\n\n\n\nus2023d\n2019-2023, PRCS 5-year\n\n\nus2023c\n2019-2023, ACS 5-year\n\n\nus2023b\n2023 PRCS\n\n\nus2023a\n2023 ACS\n\n\nus2022d\n2018-2022, PRCS 5-year\n\n\n\n\n\n\n\n\n\nNote that “PRCS” refers to the Puerto Rico Community Survey (an ACS equivalent specifically tailored to Puerto Rico). We are principally interested in the ACS, which comes in either one-year (e.g. 2023 ACS) or five-year (e.g. 2018-2022, ACS 5-year) estimates. The differences between these two estimates are described in detail in Census Data: An Overview in (Walker 2023). One differentiating point is that one-year estimates come from a smaller, but more contemporary sample. In our case we’ll use the one-year to get the best sense of the 2022 income dynamics.\nLet’s return to the motivating question for this post:\n\n\n\n\n\n\nMotivating Question\n\n\n\nWhat was the median household income in Oakland, California in 2022?\n\n\nTo answer that we will:\n\nGet income data from the 2022 1-year ACS (Section 3).\nFilter our data down to households in the city of Oakland (Section 4 and Section 5).\nCalculate the median (Section 6 and Section 7)."
  },
  {
    "objectID": "posts/ipums-wth-r/index.html#sec-step-1",
    "href": "posts/ipums-wth-r/index.html#sec-step-1",
    "title": "Working with ACS microdata in R",
    "section": "Step 1: Retrieving data",
    "text": "Step 1: Retrieving data\nFor the first task, I’ll define several helper functions:\n\nretrieve_sample() retrieves a list of variables from a given ACS sample from the IPUMS API. This is the only truly necessary code.\ncheck_description() and ipums_data() are functions for checking if the target extract already exists locally. Retrieving an extract can take some time, and I’d like to avoid doing it repeatedly when I’ve already downloaded the extract in the past.\n\nEach of these are defined in the following folded code block.\n\n\nCode\nretrieve_sample &lt;- function(sample, variables, description) {\n  extract &lt;- define_extract_micro(\n    description = description,\n    collection = \"usa\",\n    samples = c(sample),\n    variables = variables\n  )\n  data_path &lt;- extract %&gt;%\n    submit_extract() %&gt;%\n    wait_for_extract() %&gt;%\n    download_extract(download_dir = here::here(\"data\"),\n                     overwrite = TRUE)\n  data &lt;- read_ipums_micro(data_path)\n  return(data)\n}\n\ncheck_description &lt;- function(file, description) {\n  xml_text &lt;- readLines(file, warn = FALSE) %&gt;% paste(collapse = \"\\n\")\n  user_description &lt;- str_match(xml_text, \"User-provided description:\\\\s*(.*)]]\")[, 2]\n  # Check if it matches the target\n  if (str_detect(user_description, fixed(description, ignore_case = TRUE))) {\n    return(file)\n  } else {\n    return(NULL)\n  }\n}\n\nipums_data &lt;- function(sample, variables, description) {\n  local_ipums_extracts &lt;- list.files(\n    path = here::here('data'),\n    pattern = \"\\\\.xml$\",\n    full.names = TRUE\n  )\n  matching_files &lt;- compact(map(local_ipums_extracts, \\(x) check_description(x, description)))\n  # If there is a match, open the first matching file\n  if (length(matching_files) &gt; 0) {\n    matched_file &lt;- matching_files[[1]]\n    message(\"Opening: \", matched_file)\n    data &lt;- read_ipums_micro(matched_file)\n  } else {\n    message(\"No matching dataset found.\")\n    data &lt;- retrieve_sample(sample, variables, description)\n  }\n}\n\n\nI’ll proceed to define a list of variables that I want, including HHINCOME (household income) and INCTOT (individual income). Some of these variables refer to census-specific language – i.e. PUMA , REPWT , REPWTP. I’ll cover exactly what each of these represent later in the post, but in the meantime I’ll also note that the IPUMS website has full reference materials for all available variables.\n\nvariables &lt;- list(\n  \"PUMA\",\n  \"AGE\",\n  \"SEX\",\n  \"EDUC\",\n  \"HHINCOME\",\n  \"INCTOT\",\n  \"REPWT\",\n  \"REPWTP\",\n  var_spec(\"STATEFIP\",\n           case_selections = \"06\")\n)\n\nNote the line, var_spec(\"STATEFIP\", case_selections = \"06\"). This selects the variable STATEFIP, while also specifying that we want to restrict our request to data where STATEFIP=='06' (California). Using var_spec() is important, as accidentally/unnecessarily downloading an unfiltered, full sample of the entire U.S. is time-consuming.\nAnyways, I can request these variables from the 2022 1-year ACS via myipums_data() function.\n\ndata &lt;- ipums_data(\"us2022a\", variables, \"Incomes by PUMA\")\n\nHere are the resulting data, the 2022 1-year ACS for California.\n\ndata %&gt;% head()\n\n\n\nTable 4: Unfiltered 2022 1-year ACS data for California\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nSAMPLE\nSERIAL\nCBSERIAL\nHHWT\nREPWT\nCLUSTER\nSTATEFIP\nPUMA\nSTRATA\nGQ\nHHINCOME\nREPWT1\nREPWT2\nREPWT3\nREPWT4\nREPWT5\nREPWT6\nREPWT7\nREPWT8\nREPWT9\nREPWT10\nREPWT11\nREPWT12\nREPWT13\nREPWT14\nREPWT15\nREPWT16\nREPWT17\nREPWT18\nREPWT19\nREPWT20\nREPWT21\nREPWT22\nREPWT23\nREPWT24\nREPWT25\nREPWT26\nREPWT27\nREPWT28\nREPWT29\nREPWT30\nREPWT31\nREPWT32\nREPWT33\nREPWT34\nREPWT35\nREPWT36\nREPWT37\nREPWT38\nREPWT39\nREPWT40\nREPWT41\nREPWT42\nREPWT43\nREPWT44\nREPWT45\nREPWT46\nREPWT47\nREPWT48\nREPWT49\nREPWT50\nREPWT51\nREPWT52\nREPWT53\nREPWT54\nREPWT55\nREPWT56\nREPWT57\nREPWT58\nREPWT59\nREPWT60\nREPWT61\nREPWT62\nREPWT63\nREPWT64\nREPWT65\nREPWT66\nREPWT67\nREPWT68\nREPWT69\nREPWT70\nREPWT71\nREPWT72\nREPWT73\nREPWT74\nREPWT75\nREPWT76\nREPWT77\nREPWT78\nREPWT79\nREPWT80\nPERNUM\nPERWT\nREPWTP\nFAMUNIT\nRELATE\nRELATED\nSEX\nAGE\nEDUC\nEDUCD\nINCTOT\nREPWTP1\nREPWTP2\nREPWTP3\nREPWTP4\nREPWTP5\nREPWTP6\nREPWTP7\nREPWTP8\nREPWTP9\nREPWTP10\nREPWTP11\nREPWTP12\nREPWTP13\nREPWTP14\nREPWTP15\nREPWTP16\nREPWTP17\nREPWTP18\nREPWTP19\nREPWTP20\nREPWTP21\nREPWTP22\nREPWTP23\nREPWTP24\nREPWTP25\nREPWTP26\nREPWTP27\nREPWTP28\nREPWTP29\nREPWTP30\nREPWTP31\nREPWTP32\nREPWTP33\nREPWTP34\nREPWTP35\nREPWTP36\nREPWTP37\nREPWTP38\nREPWTP39\nREPWTP40\nREPWTP41\nREPWTP42\nREPWTP43\nREPWTP44\nREPWTP45\nREPWTP46\nREPWTP47\nREPWTP48\nREPWTP49\nREPWTP50\nREPWTP51\nREPWTP52\nREPWTP53\nREPWTP54\nREPWTP55\nREPWTP56\nREPWTP57\nREPWTP58\nREPWTP59\nREPWTP60\nREPWTP61\nREPWTP62\nREPWTP63\nREPWTP64\nREPWTP65\nREPWTP66\nREPWTP67\nREPWTP68\nREPWTP69\nREPWTP70\nREPWTP71\nREPWTP72\nREPWTP73\nREPWTP74\nREPWTP75\nREPWTP76\nREPWTP77\nREPWTP78\nREPWTP79\nREPWTP80\n\n\n\n\n2022\n202201\n74692\n2.02201e+12\n14\n1\n2.022001e+12\n6\n6509\n650906\n4\n9999999\n12\n14\n14\n12\n14\n14\n12\n12\n11\n12\n13\n13\n11\n13\n13\n11\n13\n12\n12\n14\n13\n13\n13\n13\n11\n11\n13\n12\n14\n13\n10\n12\n13\n10\n13\n13\n15\n12\n14\n14\n12\n11\n12\n12\n10\n14\n13\n13\n12\n12\n11\n13\n12\n11\n12\n14\n13\n12\n14\n12\n13\n13\n12\n12\n12\n12\n12\n12\n12\n11\n12\n13\n13\n11\n14\n12\n11\n11\n13\n10\n1\n14\n1\n1\n12\n1270\n2\n56\n6\n64\n14500\n12\n14\n14\n12\n14\n14\n12\n12\n11\n12\n13\n13\n11\n13\n13\n11\n13\n12\n12\n14\n13\n13\n13\n13\n11\n11\n13\n12\n14\n13\n10\n12\n13\n10\n13\n13\n15\n12\n14\n14\n12\n11\n12\n12\n10\n14\n13\n13\n12\n12\n11\n13\n12\n11\n12\n14\n13\n12\n14\n12\n13\n13\n12\n12\n12\n12\n12\n12\n12\n11\n12\n13\n13\n11\n14\n12\n11\n11\n13\n10\n\n\n2022\n202201\n74693\n2.02201e+12\n27\n1\n2.022001e+12\n6\n6501\n650106\n3\n9999999\n27\n28\n48\n49\n5\n47\n26\n26\n6\n7\n6\n28\n26\n48\n6\n28\n47\n27\n26\n44\n50\n50\n25\n28\n27\n27\n6\n47\n25\n27\n29\n6\n49\n27\n27\n6\n29\n6\n6\n26\n28\n28\n6\n6\n51\n6\n26\n27\n48\n46\n47\n28\n27\n6\n51\n27\n6\n26\n29\n4\n6\n6\n28\n27\n29\n27\n48\n5\n27\n29\n27\n46\n6\n25\n27\n48\n27\n43\n47\n25\n1\n27\n1\n1\n13\n1301\n1\n52\n0\n2\n0\n27\n28\n48\n49\n5\n47\n26\n26\n6\n7\n6\n28\n26\n48\n6\n28\n47\n27\n26\n44\n50\n50\n25\n28\n27\n27\n6\n47\n25\n27\n29\n6\n49\n27\n27\n6\n29\n6\n6\n26\n28\n28\n6\n6\n51\n6\n26\n27\n48\n46\n47\n28\n27\n6\n51\n27\n6\n26\n29\n4\n6\n6\n28\n27\n29\n27\n48\n5\n27\n29\n27\n46\n6\n25\n27\n48\n27\n43\n47\n25\n\n\n2022\n202201\n74694\n2.02201e+12\n70\n1\n2.022001e+12\n6\n8101\n810106\n3\n9999999\n19\n11\n11\n59\n71\n71\n71\n71\n78\n91\n89\n91\n19\n57\n80\n89\n70\n78\n90\n11\n20\n12\n11\n58\n70\n71\n71\n71\n80\n91\n90\n91\n18\n60\n79\n89\n70\n79\n89\n92\n80\n92\n90\n90\n71\n72\n69\n71\n20\n11\n11\n59\n80\n92\n19\n58\n71\n18\n58\n90\n79\n90\n90\n90\n70\n69\n70\n71\n20\n12\n10\n57\n78\n91\n20\n58\n71\n20\n56\n12\n1\n70\n1\n1\n13\n1301\n1\n61\n7\n71\n80\n19\n11\n11\n59\n71\n71\n71\n71\n78\n91\n89\n91\n19\n57\n80\n89\n70\n78\n90\n11\n20\n12\n11\n58\n70\n71\n71\n71\n80\n91\n90\n91\n18\n60\n79\n89\n70\n79\n89\n92\n80\n92\n90\n90\n71\n72\n69\n71\n20\n11\n11\n59\n80\n92\n19\n58\n71\n18\n58\n90\n79\n90\n90\n90\n70\n69\n70\n71\n20\n12\n10\n57\n78\n91\n20\n58\n71\n20\n56\n12\n\n\n2022\n202201\n74695\n2.02201e+12\n22\n1\n2.022001e+12\n6\n8303\n830306\n4\n9999999\n22\n26\n29\n36\n2\n32\n36\n27\n3\n2\n4\n22\n18\n55\n3\n22\n43\n18\n27\n4\n2\n3\n19\n26\n20\n20\n46\n5\n21\n23\n20\n38\n2\n27\n23\n45\n20\n31\n53\n20\n20\n20\n54\n42\n6\n49\n16\n19\n3\n3\n4\n21\n27\n33\n4\n25\n37\n29\n18\n2\n4\n3\n29\n18\n27\n28\n34\n2\n26\n23\n22\n38\n5\n19\n21\n35\n25\n56\n30\n24\n1\n22\n1\n1\n12\n1270\n2\n26\n7\n71\n9000\n22\n26\n29\n36\n2\n32\n36\n27\n3\n2\n4\n22\n18\n55\n3\n22\n43\n18\n27\n4\n2\n3\n19\n26\n20\n20\n46\n5\n21\n23\n20\n38\n2\n27\n23\n45\n20\n31\n53\n20\n20\n20\n54\n42\n6\n49\n16\n19\n3\n3\n4\n21\n27\n33\n4\n25\n37\n29\n18\n2\n4\n3\n29\n18\n27\n28\n34\n2\n26\n23\n22\n38\n5\n19\n21\n35\n25\n56\n30\n24\n\n\n2022\n202201\n74696\n2.02201e+12\n8\n1\n2.022001e+12\n6\n6712\n671206\n3\n9999999\n9\n15\n10\n8\n8\n2\n2\n8\n2\n15\n1\n16\n8\n14\n15\n10\n8\n7\n9\n10\n14\n9\n3\n16\n16\n9\n10\n1\n7\n8\n8\n8\n3\n9\n10\n16\n15\n1\n1\n8\n16\n9\n2\n15\n15\n9\n8\n1\n8\n8\n8\n8\n1\n9\n8\n15\n16\n3\n2\n2\n8\n15\n8\n8\n8\n1\n1\n9\n2\n16\n3\n15\n8\n14\n17\n9\n8\n8\n8\n3\n1\n8\n1\n1\n13\n1301\n2\n38\n6\n63\n48000\n9\n15\n10\n8\n8\n2\n2\n8\n2\n15\n1\n16\n8\n14\n15\n10\n8\n7\n9\n10\n14\n9\n3\n16\n16\n9\n10\n1\n7\n8\n8\n8\n3\n9\n10\n16\n15\n1\n1\n8\n16\n9\n2\n15\n15\n9\n8\n1\n8\n8\n8\n8\n1\n9\n8\n15\n16\n3\n2\n2\n8\n15\n8\n8\n8\n1\n1\n9\n2\n16\n3\n15\n8\n14\n17\n9\n8\n8\n8\n3\n\n\n2022\n202201\n74697\n2.02201e+12\n49\n1\n2.022001e+12\n6\n7301\n730106\n4\n9999999\n52\n51\n47\n5\n79\n45\n45\n4\n49\n5\n3\n103\n91\n53\n49\n49\n46\n5\n102\n88\n47\n48\n55\n4\n98\n49\n47\n5\n44\n5\n3\n93\n104\n48\n54\n46\n53\n4\n103\n3\n46\n50\n51\n93\n4\n48\n50\n77\n49\n93\n108\n5\n4\n46\n47\n51\n57\n89\n4\n4\n54\n42\n56\n101\n4\n45\n55\n94\n48\n96\n99\n5\n4\n42\n49\n47\n50\n75\n4\n90\n1\n49\n1\n1\n12\n1270\n1\n23\n6\n63\n24000\n52\n51\n47\n5\n79\n45\n45\n4\n49\n5\n3\n103\n91\n53\n49\n49\n46\n5\n102\n88\n47\n48\n55\n4\n98\n49\n47\n5\n44\n5\n3\n93\n104\n48\n54\n46\n53\n4\n103\n3\n46\n50\n51\n93\n4\n48\n50\n77\n49\n93\n108\n5\n4\n46\n47\n51\n57\n89\n4\n4\n54\n42\n56\n101\n4\n45\n55\n94\n48\n96\n99\n5\n4\n42\n49\n47\n50\n75\n4\n90"
  },
  {
    "objectID": "posts/ipums-wth-r/index.html#sec-step-2",
    "href": "posts/ipums-wth-r/index.html#sec-step-2",
    "title": "Working with ACS microdata in R",
    "section": "Step 2: Using Geocorr to identify small geographies",
    "text": "Step 2: Using Geocorr to identify small geographies\nWe now have microdata for all of California, but we need to filter down to just Oakland. Unfortunately, this isn’t as simple as just running filter(CITY == 'Oakland') – ACS microdata does not include a field for explicitly identifying cities (note that a city is typically referred to as a “place” in census data).\nThe smallest geographic area explicitly identified in the microdata is something called a public use microdata area (PUMA) (Pastoor 2024). PUMAS are unique geographies that always aggregate to the state-level (e.g. California can be constructed with a collection of PUMAs), but only sometimes aggregate to other small geographic areas, such as city, metro area, and county (See Census Hierarchies in Walker 2023, chap. 1).\n\n\nCode\nlibrary(tigris)\nlibrary(sf)\nlibrary(cowplot)\noptions(tigris_use_cache = TRUE)\n\nbay_area_counties &lt;- c(\n  \"Alameda\", \"Contra Costa\", \"Marin\", \"Napa\",\n  \"San Francisco\", \"San Mateo\", \"Santa Clara\", \"Solano\", \"Sonoma\"\n)\n\nbay_area &lt;- counties(state = \"CA\", class = \"sf\", year=2022) %&gt;%\n  filter(NAME %in% bay_area_counties) %&gt;%\n  st_union()  # Merge counties into a single boundary\n\n# Get all California Places & PUMAs\nplaces &lt;- places(state = \"CA\", class = \"sf\", year=2022)\npumas &lt;- pumas(state = \"CA\", class = \"sf\",  year=2022)\n# Filter only those in the Bay Area\nplaces_bay &lt;- places[st_intersects(places, bay_area, sparse = FALSE), ]\npumas_bay &lt;- pumas[st_intersects(pumas, bay_area, sparse = FALSE), ]\n\npumas_bbox &lt;- st_bbox(bay_area)\n\ncrop_theme &lt;- theme(\n    panel.spacing = unit(0, \"lines\"),      \n    plot.margin = margin(0, 0, 0, 0, \"cm\")\n)\n\nmap_places &lt;- ggplot() +\n  geom_sf(data = places_bay, fill = \"lightblue\", color = \"darkgrey\", size = 0.1) +\n  geom_sf(data = bay_area, fill = NA, color = \"red\", size = 4) +  \n  coord_sf(ylim = c(pumas_bbox[\"ymin\"], pumas_bbox[\"ymax\"]), xlim = c(pumas_bbox[\"xmin\"], pumas_bbox[\"xmax\"]), expand=FALSE) +  # Apply PUMAs y-limits\n  crop_theme +\n  theme_minimal() +\n  ggtitle(\"Places\")\n\nmap_pumas &lt;- ggplot() +\n  geom_sf(data = pumas_bay, fill = \"lightblue\", color = \"darkgrey\", size = 0.1) +\n  geom_sf(data = bay_area, fill = NA, color = \"red\", size = 4) +  \n  coord_sf(ylim = c(pumas_bbox[\"ymin\"], pumas_bbox[\"ymax\"]), xlim = c(pumas_bbox[\"xmin\"], pumas_bbox[\"xmax\"]), expand=FALSE) +  # Ensure y-axis is the same\n  crop_theme +\n  theme_minimal() +\n  ggtitle(\"PUMAs\")\n\n\nplot_grid(map_places, map_pumas, nrow = 1, align = \"h\", axis=\"t\", vjust = 0)\n\n\n\n\nPlaces and PUMAS in the Bay Area (region border in red)\n\n\n\n\n\nTo find out if a city corresponds to a collection of PUMAs and which PUMAs those are, we’ll use Geocorr (geographic correspondence engine), an application that generates correlation lists showing relationships between two or more geographic coverages in the United States (Mihalik, Rice, and Hesser 2022). Geocorr is a sponsored program of the Missouri State library and published by the University of Missouri Center for Health Policy.4\n\n\nGeocorr 2022: Geographic Correspondence Engine\n\n\n\nTo use Geocorr, I’ll define a function, geocorr_2022() that queries Geocorr 2022 and retrieves a .csv file establishing the relationships between two sets of geographies within a given state. See the following folded code chunk for the specifics:\n\n\nCode\ngeocorr_2022 &lt;- function(state, geo_1, geo_2, weight_var) {\n  base_url &lt;- \"https://mcdc.missouri.edu\"\n  params &lt;- glue(\n    \"cgi-bin/broker?_PROGRAM=apps.geocorr2022.sas&\",\n    \"_SERVICE=MCDC_long&_debug=0&\",\n    \"state={state}&g1_={geo_1}&g2_={geo_2}&wtvar={weight_var}&\",\n    \"nozerob=1&fileout=1&filefmt=csv&lstfmt=txt&title=&\",\n    \"counties=&metros=&places=&oropt=&latitude=&longitude=&\",\n    \"distance=&kiloms=0&locname=\")\n  initial_url &lt;- params %&gt;% url_absolute(base = base_url)\n  initial_response &lt;- GET(initial_url)\n  html_content &lt;- content(initial_response, as = \"text\")\n  parsed_html &lt;- read_html(html_content)\n  # Extract the one link\n  csv_url &lt;- parsed_html %&gt;%\n    html_node(\"a\") %&gt;%\n    html_attr(\"href\") %&gt;%\n    stringr::str_trim() %&gt;%\n    url_absolute(base = base_url)\n  csv_data &lt;- read_csv(csv_url)\n  return(csv_data)\n}\n\n\nWe’ll use that function to establish the relationships between California’s 2022 PUMAs and its “places,” using individual population as measured in the 2020 Decenial Census to weight the relationships.\n\ncsv_data &lt;- geocorr_2022(\"Ca06\", \"puma22\", \"place\", \"pop20\")\n\nWith that, we can see which PUMAs correspond to the City of Oakland.\n\ncsv_data %&gt;%\n  select(-c(state, stab, place, PUMA22name)) %&gt;%\n  filter(PlaceName == 'Oakland city, CA')\n\n\n\nTable 5: PUMA to Place correspondence for Oakand, CA\n\n\n\n\n\n\n\npuma22\nPlaceName\npop20\nafact\n\n\n\n\n00111\nOakland city, CA\n106433\n1\n\n\n00112\nOakland city, CA\n106896\n1\n\n\n00113\nOakland city, CA\n125840\n1\n\n\n00114\nOakland city, CA\n9\n0\n\n\n00123\nOakland city, CA\n101468\n1\n\n\n\n\n\n\n\n\n\nThe AFACT (allocation factor) column shows the proportion of the source area contained in the target area – in this case the proportion of the PUMA population that belongs to Oakland. In this case, 100% of the populations in PUMAs 111, 112, 113, and 123 belong to Oakland, and 0% of PUMA 114. GEOCORR does believe that 9 individuals from 114 live in Oakland, but based on the AFACT of 0 and the fact that I know the PUMA overlays the neighboring city of Piedmont, I’ll feel comfortable dropping that PUMA.5\nNote that when you plot place to PUMA equivalence, you may find slight differences due to the fact that places can include un-populated areas, such as bodies of water in the case of Oakland.\n\n\nCode\n# Filter PUMAs for Oakland\noakland_pumas &lt;- pumas_bay %&gt;% filter(PUMACE20 %in% c('00111', '00112', '00113', '00123'))\noakland_place &lt;- places_bay %&gt;% filter(NAME == 'Oakland')\n# Get bounding box for Oakland PUMAs\noakland_bbox &lt;- st_bbox(oakland_place)\n\ncrop_theme &lt;- theme(\n    panel.spacing = unit(0, \"lines\"),      \n    plot.margin = margin(0, 0, 0, 0, \"cm\"),\n    axis.title.y = element_blank(),\n    axis.title.x = element_blank()\n)\n# Oakland Places Map\nmap_places &lt;- ggplot() +\n  geom_sf(data = oakland_place, fill = \"lightblue\", color = \"darkgrey\", size = 0.1) +\n  geom_sf(data = oakland_place, fill = NA, color = \"red\", size = 4) +  \n  coord_sf(ylim = c(oakland_bbox[\"ymin\"], oakland_bbox[\"ymax\"]), \n           xlim = c(oakland_bbox[\"xmin\"], oakland_bbox[\"xmax\"]), \n           expand = FALSE) +\n  theme_minimal() +\n  crop_theme +\n  ggtitle(\"Oakland (Place)\")\n\n\n# Oakland PUMAs Map\nmap_pumas &lt;- ggplot() +\n  geom_sf(data = oakland_pumas, fill = \"lightblue\", color = \"darkgrey\", size = 0.1) +\n  geom_sf_text(data = oakland_pumas, aes(label = PUMACE20)) +\n  geom_sf(data = oakland_place, fill = NA, color = \"red\", size = 4) +\n  #geom_sf(data = oakland_pumas, fill = NA, color = \"red\", size = 4) +  \n  coord_sf(ylim = c(oakland_bbox[\"ymin\"], oakland_bbox[\"ymax\"]), \n           xlim = c(oakland_bbox[\"xmin\"], oakland_bbox[\"xmax\"]), \n           expand = FALSE) +\n  theme_minimal() +\n  crop_theme +\n  ggtitle(\"Oakland (PUMAs)\")\n\nplot_grid(map_places, map_pumas, nrow = 1, align = \"h\", axis = \"t\", vjust = 0)\n\n\n\n\nPlace to PUMAs Correspondence in Oakland (place border in red)\n\n\n\n\n\nFiltering to those PUMAs gets us the 2022 1-year ACS microdata for the City of Oakland (I use haven::zap_labels() just to remove some unnecessary formatting that comes with the data).\n\noakland_pumas &lt;- c(111, 112, 113, 123)\noak &lt;- data %&gt;%\n  filter(PUMA %in% oakland_pumas) %&gt;% \n  haven::zap_labels()\noak %&gt;% head()\n\n\n\nTable 6: 2022 1-year ACS data for Oakland, CA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nSAMPLE\nSERIAL\nCBSERIAL\nHHWT\nREPWT\nCLUSTER\nSTATEFIP\nPUMA\nSTRATA\nGQ\nHHINCOME\nREPWT1\nREPWT2\nREPWT3\nREPWT4\nREPWT5\nREPWT6\nREPWT7\nREPWT8\nREPWT9\nREPWT10\nREPWT11\nREPWT12\nREPWT13\nREPWT14\nREPWT15\nREPWT16\nREPWT17\nREPWT18\nREPWT19\nREPWT20\nREPWT21\nREPWT22\nREPWT23\nREPWT24\nREPWT25\nREPWT26\nREPWT27\nREPWT28\nREPWT29\nREPWT30\nREPWT31\nREPWT32\nREPWT33\nREPWT34\nREPWT35\nREPWT36\nREPWT37\nREPWT38\nREPWT39\nREPWT40\nREPWT41\nREPWT42\nREPWT43\nREPWT44\nREPWT45\nREPWT46\nREPWT47\nREPWT48\nREPWT49\nREPWT50\nREPWT51\nREPWT52\nREPWT53\nREPWT54\nREPWT55\nREPWT56\nREPWT57\nREPWT58\nREPWT59\nREPWT60\nREPWT61\nREPWT62\nREPWT63\nREPWT64\nREPWT65\nREPWT66\nREPWT67\nREPWT68\nREPWT69\nREPWT70\nREPWT71\nREPWT72\nREPWT73\nREPWT74\nREPWT75\nREPWT76\nREPWT77\nREPWT78\nREPWT79\nREPWT80\nPERNUM\nPERWT\nREPWTP\nFAMUNIT\nRELATE\nRELATED\nSEX\nAGE\nEDUC\nEDUCD\nINCTOT\nREPWTP1\nREPWTP2\nREPWTP3\nREPWTP4\nREPWTP5\nREPWTP6\nREPWTP7\nREPWTP8\nREPWTP9\nREPWTP10\nREPWTP11\nREPWTP12\nREPWTP13\nREPWTP14\nREPWTP15\nREPWTP16\nREPWTP17\nREPWTP18\nREPWTP19\nREPWTP20\nREPWTP21\nREPWTP22\nREPWTP23\nREPWTP24\nREPWTP25\nREPWTP26\nREPWTP27\nREPWTP28\nREPWTP29\nREPWTP30\nREPWTP31\nREPWTP32\nREPWTP33\nREPWTP34\nREPWTP35\nREPWTP36\nREPWTP37\nREPWTP38\nREPWTP39\nREPWTP40\nREPWTP41\nREPWTP42\nREPWTP43\nREPWTP44\nREPWTP45\nREPWTP46\nREPWTP47\nREPWTP48\nREPWTP49\nREPWTP50\nREPWTP51\nREPWTP52\nREPWTP53\nREPWTP54\nREPWTP55\nREPWTP56\nREPWTP57\nREPWTP58\nREPWTP59\nREPWTP60\nREPWTP61\nREPWTP62\nREPWTP63\nREPWTP64\nREPWTP65\nREPWTP66\nREPWTP67\nREPWTP68\nREPWTP69\nREPWTP70\nREPWTP71\nREPWTP72\nREPWTP73\nREPWTP74\nREPWTP75\nREPWTP76\nREPWTP77\nREPWTP78\nREPWTP79\nREPWTP80\n\n\n\n\n2022\n202201\n74718\n2.02201e+12\n5\n1\n2.022001e+12\n6\n111\n11106\n3\n9999999\n5\n4\n5\n4\n5\n5\n5\n2\n6\n5\n5\n3\n5\n2\n2\n5\n4\n4\n4\n5\n4\n4\n5\n5\n4\n5\n5\n2\n2\n2\n5\n4\n5\n3\n3\n4\n5\n5\n4\n3\n5\n5\n4\n5\n5\n5\n4\n2\n5\n4\n4\n4\n5\n2\n3\n4\n5\n5\n4\n5\n5\n4\n5\n5\n4\n5\n5\n2\n2\n3\n3\n4\n4\n2\n2\n5\n4\n4\n5\n4\n1\n5\n1\n1\n13\n1301\n1\n20\n7\n71\n0\n5\n4\n5\n4\n5\n5\n5\n2\n6\n5\n5\n3\n5\n2\n2\n5\n4\n4\n4\n5\n4\n4\n5\n5\n4\n5\n5\n2\n2\n2\n5\n4\n5\n3\n3\n4\n5\n5\n4\n3\n5\n5\n4\n5\n5\n5\n4\n2\n5\n4\n4\n4\n5\n2\n3\n4\n5\n5\n4\n5\n5\n4\n5\n5\n4\n5\n5\n2\n2\n3\n3\n4\n4\n2\n2\n5\n4\n4\n5\n4\n\n\n2022\n202201\n74737\n2.02201e+12\n56\n1\n2.022001e+12\n6\n111\n11106\n3\n9999999\n6\n81\n56\n66\n67\n81\n65\n7\n55\n80\n79\n11\n55\n83\n43\n12\n6\n42\n56\n56\n7\n82\n56\n65\n66\n79\n65\n7\n56\n81\n80\n12\n55\n83\n42\n12\n6\n43\n55\n55\n7\n82\n55\n68\n68\n81\n67\n6\n56\n78\n80\n12\n55\n82\n43\n11\n6\n42\n56\n54\n6\n82\n56\n66\n67\n78\n67\n6\n56\n81\n80\n12\n55\n82\n44\n12\n7\n43\n54\n55\n1\n56\n1\n1\n13\n1301\n1\n56\n6\n63\n480\n6\n81\n56\n66\n67\n81\n65\n7\n55\n80\n79\n11\n55\n83\n43\n12\n6\n42\n56\n56\n7\n82\n56\n65\n66\n79\n65\n7\n56\n81\n80\n12\n55\n83\n42\n12\n6\n43\n55\n55\n7\n82\n55\n68\n68\n81\n67\n6\n56\n78\n80\n12\n55\n82\n43\n11\n6\n42\n56\n54\n6\n82\n56\n66\n67\n78\n67\n6\n56\n81\n80\n12\n55\n82\n44\n12\n7\n43\n54\n55\n\n\n2022\n202201\n74738\n2.02201e+12\n15\n1\n2.022001e+12\n6\n113\n11306\n4\n9999999\n15\n13\n15\n15\n15\n15\n14\n15\n13\n15\n15\n15\n14\n15\n15\n13\n14\n13\n15\n17\n13\n15\n17\n15\n14\n13\n14\n14\n17\n15\n14\n14\n15\n12\n14\n15\n16\n15\n15\n16\n13\n15\n15\n15\n14\n15\n13\n14\n13\n13\n12\n14\n15\n12\n13\n15\n16\n15\n15\n12\n13\n15\n15\n15\n15\n14\n15\n15\n14\n15\n15\n15\n15\n15\n16\n13\n14\n12\n14\n13\n1\n15\n1\n1\n12\n1270\n1\n34\n6\n63\n1200\n15\n13\n15\n15\n15\n15\n14\n15\n13\n15\n15\n15\n14\n15\n15\n13\n14\n13\n15\n17\n13\n15\n17\n15\n14\n13\n14\n14\n17\n15\n14\n14\n15\n12\n14\n15\n16\n15\n15\n16\n13\n15\n15\n15\n14\n15\n13\n14\n13\n13\n12\n14\n15\n12\n13\n15\n16\n15\n15\n12\n13\n15\n15\n15\n15\n14\n15\n15\n14\n15\n15\n15\n15\n15\n16\n13\n14\n12\n14\n13\n\n\n2022\n202201\n75005\n2.02201e+12\n38\n1\n2.022001e+12\n6\n113\n11306\n4\n9999999\n36\n38\n36\n37\n35\n36\n38\n39\n33\n39\n37\n37\n37\n37\n37\n36\n37\n35\n39\n37\n37\n34\n36\n37\n38\n35\n36\n35\n39\n35\n36\n34\n38\n37\n36\n38\n36\n36\n36\n36\n35\n36\n39\n36\n36\n37\n37\n37\n38\n35\n33\n38\n36\n34\n36\n37\n35\n37\n37\n36\n35\n39\n37\n39\n37\n38\n38\n36\n37\n37\n37\n35\n37\n37\n35\n37\n35\n37\n37\n37\n1\n38\n1\n1\n12\n1270\n2\n40\n2\n23\n41300\n36\n38\n36\n37\n35\n36\n38\n39\n33\n39\n37\n37\n37\n37\n37\n36\n37\n35\n39\n37\n37\n34\n36\n37\n38\n35\n36\n35\n39\n35\n36\n34\n38\n37\n36\n38\n36\n36\n36\n36\n35\n36\n39\n36\n36\n37\n37\n37\n38\n35\n33\n38\n36\n34\n36\n37\n35\n37\n37\n36\n35\n39\n37\n39\n37\n38\n38\n36\n37\n37\n37\n35\n37\n37\n35\n37\n35\n37\n37\n37\n\n\n2022\n202201\n75119\n2.02201e+12\n20\n1\n2.022001e+12\n6\n111\n11106\n3\n9999999\n22\n21\n21\n22\n20\n20\n21\n21\n20\n22\n22\n22\n20\n22\n21\n21\n22\n22\n20\n22\n22\n22\n22\n22\n20\n21\n20\n22\n21\n21\n20\n22\n21\n22\n22\n20\n21\n22\n22\n22\n20\n22\n22\n22\n21\n21\n22\n22\n21\n22\n21\n20\n22\n21\n20\n20\n22\n23\n21\n22\n22\n22\n22\n20\n20\n22\n22\n21\n22\n22\n22\n21\n20\n22\n22\n20\n20\n22\n22\n22\n1\n20\n1\n1\n13\n1301\n2\n88\n2\n23\n5800\n22\n21\n21\n22\n20\n20\n21\n21\n20\n22\n22\n22\n20\n22\n21\n21\n22\n22\n20\n22\n22\n22\n22\n22\n20\n21\n20\n22\n21\n21\n20\n22\n21\n22\n22\n20\n21\n22\n22\n22\n20\n22\n22\n22\n21\n21\n22\n22\n21\n22\n21\n20\n22\n21\n20\n20\n22\n23\n21\n22\n22\n22\n22\n20\n20\n22\n22\n21\n22\n22\n22\n21\n20\n22\n22\n20\n20\n22\n22\n22\n\n\n2022\n202201\n75131\n2.02201e+12\n11\n1\n2.022001e+12\n6\n123\n12306\n3\n9999999\n12\n0\n8\n20\n8\n0\n1\n14\n14\n1\n16\n11\n13\n20\n8\n14\n0\n8\n13\n12\n11\n15\n8\n1\n10\n13\n17\n15\n12\n19\n1\n12\n0\n1\n8\n1\n16\n8\n12\n10\n11\n1\n9\n21\n7\n0\n1\n13\n14\n1\n16\n10\n13\n18\n10\n13\n1\n9\n14\n11\n10\n13\n9\n1\n9\n15\n17\n15\n13\n19\n0\n14\n0\n1\n9\n1\n16\n9\n11\n9\n1\n11\n1\n1\n13\n1301\n2\n86\n2\n23\n0\n12\n0\n8\n20\n8\n0\n1\n14\n14\n1\n16\n11\n13\n20\n8\n14\n0\n8\n13\n12\n11\n15\n8\n1\n10\n13\n17\n15\n12\n19\n1\n12\n0\n1\n8\n1\n16\n8\n12\n10\n11\n1\n9\n21\n7\n0\n1\n13\n14\n1\n16\n10\n13\n18\n10\n13\n1\n9\n14\n11\n10\n13\n9\n1\n9\n15\n17\n15\n13\n19\n0\n14\n0\n1\n9\n1\n16\n9\n11\n9"
  },
  {
    "objectID": "posts/ipums-wth-r/index.html#sec-step-3",
    "href": "posts/ipums-wth-r/index.html#sec-step-3",
    "title": "Working with ACS microdata in R",
    "section": "Step 3: Specify the unit of analysis",
    "text": "Step 3: Specify the unit of analysis\nEach row in the ACS microdata represents an individual, identified by a unique combination of SAMPLE, which defines the year when the individual was surveyed, SERIAL, a unique identifier for that individual’s household, and PERNUM, a unique identifier for the individual within their household (Ruggles et al. 2024). Given that, we can define a straightforward definition for unique individuals in the ACS:\n\nIndividuals: The combination of SAMPLE, SERIAL, and PERNUM provides a unique identifier for every person in the IPUMS. ACS microdata is typically individual-level, so this filtering should not be necessary.\n\nDefining a household is slightly more complex.\n\nHousing Units: The combination of SAMPLE and SERIAL provides a unique identifier for every housing unit in the IPUMS. However, this includes both households and general group quarter housing, e.g. correctional facilities. Housing units are further categorized using the GQ variable.\n\nHouseholds: Non group quarter housing units, defined by filtering housing units to those where GQ, is 1 or 2 (Bloem 2018). The comparability section of the GQ variable documentation explains this choice in greater detail, but in short it represents non-institutional housing-units where 9 or fewer people unrelated to the head-of-household reside.\n\n\nWe can group/filter by these variable combinations and see how many individuals, housing units, and households were surveyed across PUMAs in Oakland for the 2022 1-year ACS.\n\noak %&gt;%\n  group_by(PUMA) %&gt;%\n  summarise(\n    n_rows = n(),\n    n_individuals = n_distinct(SAMPLE, SERIAL, PERNUM),\n    n_housing_units = n_distinct(SAMPLE, SERIAL)) %&gt;%\n  inner_join(oak %&gt;%\n               filter(GQ &lt;= 2) %&gt;%\n               group_by(PUMA) %&gt;%\n               summarise(n_households = n_distinct(SAMPLE, SERIAL)),\n             by = join_by(PUMA))\n\n\n\nTable 7: Oakland Dataset Granularity by PUMA\n\n\n\n\n\n\n\nPUMA\nn_rows\nn_individuals\nn_housing_units\nn_households\n\n\n\n\n111\n1083\n1083\n578\n539\n\n\n112\n1152\n1152\n547\n517\n\n\n113\n989\n989\n385\n337\n\n\n123\n905\n905\n389\n354\n\n\n\n\n\n\n\n\n\nWe see that each row in the data represents an individual (n_rows equals n_individuals) and, as we would expect, the number of housing units and households are successively lower than the number of individuals.\n\nHouseholds, group quarters, and institutions\nSo, given that our motivating question concerns household income, we can apply the following filtering, per (Bloem 2018), and isolate just the heads of all the households in Oakland.\n\nhouseholds &lt;- oak %&gt;%\n  filter(\n    PERNUM == 1,\n    GQ &lt;= 2\n    )\n\nBut to build a little intuition around that selection, let’s also look at what different housing units look like, and why the household definition is restrictive.\n\nA household\nI’ll randomly select a household in the data to see what such a unit looks like in practice.\n\n\nCode\nhousehold_serials &lt;- oak %&gt;%\n  filter(GQ &lt;= 2) %&gt;%\n  group_by(SERIAL) %&gt;%\n  count() %&gt;%\n  filter(n &gt; 1) %&gt;%\n  pull(SERIAL)\nset.seed(5)\nsample_household_true &lt;- sample(household_serials, 1)\nn &lt;- oak %&gt;% filter(SERIAL == sample_household_true) %&gt;% dim() %&gt;% .[1]\noak %&gt;% filter(SERIAL == sample_household_true) %&gt;% \n  select(c(SERIAL, GQ, PERNUM, RELATE, AGE, SEX, HHINCOME, INCTOT))\n\n\n\n\nTable 8: An example household with income data\n\n\n\n\n\n\n\nSERIAL\nGQ\nPERNUM\nRELATE\nAGE\nSEX\nHHINCOME\nINCTOT\n\n\n\n\n196265\n1\n1\n1\n41\n2\n184500\n104500\n\n\n196265\n1\n2\n2\n40\n1\n184500\n60000\n\n\n196265\n1\n3\n3\n6\n2\n184500\n9999999\n\n\n196265\n1\n4\n3\n4\n1\n184500\n9999999\n\n\n196265\n1\n5\n6\n65\n2\n184500\n20000\n\n\n\n\n\n\n\n\n\nThis household, with SERIAL 196265 has 5 members, each with a unique PERNUM. To identify household members’ relationship to the head-of-household, we reference RELATE. The first row, where RELATE==1 denotes a 41 year old female (SEX==2) head-of-household. The second, where RELATE==2 denotes their spouse, in this case a 40 year old male. This couple has two children (RELATE==3) and the husband’s mother (RELATE==6), also living in the household.\n\n\nNon-institutional group-quarters\nWe can contrast that fairly “nuclear family” household with a non-institutional group-quarters housing unit in Oakland:\n\n\nCode\nhousehold_serials &lt;- oak %&gt;%\n  filter(GQ == 5) %&gt;%\n  pull(SERIAL)\nset.seed(6)\nsample_household &lt;- sample(household_serials, 1)\nsampled_data &lt;- oak %&gt;% \n  filter(SERIAL == sample_household) %&gt;% \n  select(SERIAL, PUMA, GQ, PERNUM, RELATE, AGE, SEX, HHINCOME, INCTOT)\nsampled_data\n\n\n\n\nTable 9: An example group quarters unit with income data\n\n\n\n\n\n\n\nSERIAL\nPUMA\nGQ\nPERNUM\nRELATE\nAGE\nSEX\nHHINCOME\nINCTOT\n\n\n\n\n165880\n111\n5\n1\n1\n26\n1\n474800\n0\n\n\n165880\n111\n5\n2\n11\n26\n2\n474800\n125000\n\n\n165880\n111\n5\n3\n11\n39\n1\n474800\n35000\n\n\n165880\n111\n5\n4\n11\n33\n2\n474800\n60000\n\n\n165880\n111\n5\n5\n11\n31\n2\n474800\n35000\n\n\n165880\n111\n5\n6\n11\n26\n1\n474800\n21000\n\n\n165880\n111\n5\n7\n11\n26\n1\n474800\n0\n\n\n165880\n111\n5\n8\n11\n26\n2\n474800\n80000\n\n\n165880\n111\n5\n9\n11\n26\n1\n474800\n80800\n\n\n165880\n111\n5\n10\n11\n25\n2\n474800\n38000\n\n\n165880\n111\n5\n11\n11\n22\n1\n474800\n0\n\n\n\n\n\n\n\n\n\nWe see that there is a head-of-household, a 26 year old male, but all 10 other housing unit members are “partners, friends, [or] visitors” (RELATE==11). These are largely adults with earnings, so this group quarters unit would have a very high “household income” and bias the estimated median income up were it included for that calculation.\n\n\nInstitutional housing units\nGroup quarters also include institutional housing units, such as correctional facilities.\n\n\nCode\nhousehold_serials &lt;- oak %&gt;%\n  filter(GQ == 3) %&gt;%\n  pull(SERIAL)\nset.seed(1)\nsample_household &lt;- sample(household_serials, 1)\nsampled_data &lt;- oak %&gt;% \n  filter(SERIAL == sample_household) %&gt;% \n  select(SERIAL, PUMA, GQ, PERNUM, RELATE, AGE, SEX, HHINCOME, INCTOT)\nsampled_data\n\n\n\n\nTable 10: An example institutional housing unit with income data\n\n\n\n\n\n\n\nSERIAL\nPUMA\nGQ\nPERNUM\nRELATE\nAGE\nSEX\nHHINCOME\nINCTOT\n\n\n\n\n75131\n123\n3\n1\n13\n86\n2\n9999999\n0\n\n\n\n\n\n\n\n\n\nThis housing unit only includes one member, an 86 year old woman who is an “institutional inmate” (RELATE==13) with no income. This unit would likely bias median household income down, were it included.\n\n\n\nHousehold Income in the ACS\nLet’s return to the motivating question:\n\n\n\n\n\n\nMotivating Question\n\n\n\nWhat was the median household income in Oakland, California in 2022?\n\n\nNote the income variables we observe for this family:\n\nINCTOT reports each respondent’s total pre-tax personal income or losses from all sources for the previous year. 9999999 is code to denote that the value is missing, which makes sense given that the missing values above correspond to children in the household (See INCTOT in Ruggles et al. 2024).\nHHINCOME reports the total money income of all household members age 15+ during the previous year. The amount should equal the sum of all household members’ individual incomes, as recorded in the person-record variable INCTOT (See HHINCOME in Ruggles et al. 2024)\n\nGiven what we know about the unique identifier for households, and the HHINCOME variable, we can construct the appropriate dataset for answering our motivating question – every household in Oakland that had household income.\n\nhouseholds_w_income &lt;- households %&gt;%\n  filter(\n    HHINCOME != 9999999,\n    HHINCOME &gt;= 0,\n    )\n\nIt seems we can proceed to simply calculate the median of the HHINCOME column? Not so fast… Data in the ACS microdata are not what they seem. Before we do any analysis, we have to account for sample weights."
  },
  {
    "objectID": "posts/ipums-wth-r/index.html#sec-step-4",
    "href": "posts/ipums-wth-r/index.html#sec-step-4",
    "title": "Working with ACS microdata in R",
    "section": "Step 4: Applying sample weights",
    "text": "Step 4: Applying sample weights\nLet’s return to our sample family from above, but also examine the variables PERWT and HHWT.\n\noak %&gt;% filter(SERIAL == sample_household_true) %&gt;% \n  select(c(AGE, SEX, HHINCOME, INCTOT, PERWT, HHWT))\n\n\n\nTable 11: An example household with weight data\n\n\n\n\n\n\n\nAGE\nSEX\nHHINCOME\nINCTOT\nPERWT\nHHWT\n\n\n\n\n41\n2\n184500\n104500\n51\n51\n\n\n40\n1\n184500\n60000\n64\n51\n\n\n6\n2\n184500\n9999999\n51\n51\n\n\n4\n1\n184500\n9999999\n51\n51\n\n\n65\n2\n184500\n20000\n76\n51\n\n\n\n\n\n\n\n\n\nThese are the two primary sample weights in ACS microdata, and they can be interpreted fairly directly.\n\nPERWT gives the population represented by each individual in the sample, thus in the first row of the sample household, the 41 year old woman with an individual income of $104,500 represents 51 individuals in the PUMA.\nHHWT gives the number of households in the general population represented by each household in the sample, thus this household is representative of 51 households in that PUMA.\n\nAny analysis or visualization of ACS microdata should be weighted by PERWT for work at the person-level, and HHWT for the household-level (See Sample Weights in Ruggles et al. 2024).\nFor visualizations, we pass the weights to the plotting API. For example, in ggplot many chart types support a weight argument.\n\nggplot(households_w_income, aes(x = HHINCOME, weight = HHWT)) +\n  geom_histogram(\n    binwidth = 50000,\n    fill = \"lightblue\",\n    alpha = 1,\n    color = \"grey\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(\n    labels = scales::label_currency(scale_cut = scales::cut_short_scale()),\n    breaks = seq(0, 1600000, 200000)\n  ) +\n  scale_y_continuous(labels=scales::label_comma()) +\n  labs(x = \"Household Income\", y = \"Number of Households\")\n\n\n\nHousehold Income Distribution in Oakland (with household weights)\n\n\n\n\n\nFor analysis, we use the srvyr package to define the survey weights before using them to calculate statistics. For example, here we’ll finally address the motivating question.\n\n\n\n\n\n\nMotivating Question\n\n\n\nWhat was the median household income in Oakland, California in 2022?\n\n\nThe answer, as measured in the IPUMS microdata is as follows:\n\nhouseholds_w_income %&gt;%\n  as_survey(weights=HHWT) %&gt;%\n  summarise(weighted_median = survey_median(HHINCOME)) %&gt;% \n  select(weighted_median)\n\n\n\nTable 12: 2022 Median Household Income in Oakland, CA\n\n\n\n\n\n\n\nweighted_median\n\n\n\n\n89000\n\n\n\n\n\n\n\n\n\nLet’s do a quick comparison of our IPUMS results to the aggregate census data we retrieved in the first section. Here are our full IPUMS results for both median household income and population:\n\n\nCode\nmedian_table &lt;- households_w_income %&gt;%\n  as_survey(weights=HHWT) %&gt;% \n  summarise(weighted_median = survey_median(HHINCOME)) %&gt;% \n  mutate(variable = \"median_hh_income\",\n         ipums_estimate = weighted_median,\n         se = weighted_median_se)\n\ncount_table &lt;- oak %&gt;%\n  as_survey(weights=PERWT) %&gt;% \n  survey_count() %&gt;% \n  mutate(variable = \"total_pop\",\n         ipums_estimate = n,\n         se = n_se)\n\naggregate_data &lt;- oakland_stats %&gt;%\n  select(c(variable, estimate)) %&gt;%\n  rename(ACS_aggregate_estimate = estimate)\n\nbind_rows(count_table, median_table) %&gt;% \n  select(c(variable, ipums_estimate)) %&gt;% inner_join(aggregate_data, by='variable')\n\n\n\n\nTable 13: IPUMS versus ACS aggregate results\n\n\n\n\n\n\n\nvariable\nipums_estimate\nACS_aggregate_estimate\n\n\n\n\ntotal_pop\n430052\n426323\n\n\nmedian_hh_income\n89000\n93146\n\n\n\n\n\n\n\n\n\nThese are clearly different. What gives? Unfortunately, summary statistics calculated using IPUMS data typically cannot match aggregate ACS figures!\nOne major reason for this gap is additional sampling error. Recall that the American Community Survey is a sample. A given one-year ACS is typically a 1% sample of the U.S. population, with associated sampling error. When the census makes microdata available, they create a sample of that sample – we do not get the full 1%. This second sampling process introduces further sampling error in the microdata that is not reflected in figures sourced from aggregate ACS data, which are calculated using the full ACS sample (See ACS in Ruggles et al. 2024)."
  },
  {
    "objectID": "posts/ipums-wth-r/index.html#sec-step-5",
    "href": "posts/ipums-wth-r/index.html#sec-step-5",
    "title": "Working with ACS microdata in R",
    "section": "Step 5: Calculating standard errors",
    "text": "Step 5: Calculating standard errors\nNow we have estimates derived from the ACS microdata, but how do we communicate uncertainty around those estimates? For the sake of this example, let’s explore the problem of standard errors via a slightly different motivating question – what was the average household income in Oakland in 2022?\nI’m switching to the average because we are about to calculate some standard errors from-scratch, and that’s much more clear-cut for linear statistics like the average than for non-linear statistics like the median. We will return to the median at the end of this section, but rely on a package implementation and not do anything from-scratch.\nAnyways, let’s calculate the average household income in Oakland using the sample household weights. The formula for the weighted average is: \\[\n\\bar{X}_w = \\frac{\\sum_{i=1}^{n}w_ix_i}{\\sum_{i=1}^{n}w_i}\n\\] We’ll put that into code and get the following estimate of the average household income in Oakland:\n\nweighted_mean &lt;- (\n  sum(households_w_income$HHINCOME * households_w_income$HHWT)\n  / sum(households_w_income$HHWT)\n  )\nweighted_mean\n\n[1] 140889.1\n\n\nNow, let’s get a standard error for that estimate. Correctly utilizing the sample weights in the variance calculation is a little tricky and there are a few different approaches to the problem. I’ll cover two techniques for variance estimation: the Taylor series method (also referred to as “linearized variance” or Taylor series linearization) and replication weights. Each of these are implemented in the survey package (Lumley 2004).\n\nThe Taylor series method\nMost packages (e.g. SAS, Stata, and R’s survey package) default to using the Taylor series method for variance estimation of weighted statistics. This is because weighted statistics, like the average above, are not linear with respect to the weights, making analytical calculation of the variance difficult or else undefined (Lohr 2021). The Taylor series method entails calculating the first-order linear approximation of the statistic (e.g. the linear approximation of the weighted mean formula above), then finding the variance of that approximation. The details of those calculations are beyond the scope of this post, but (Lohr 2021, chap 9.1) covers the process in detail, and Wikipedia has a nice walkthrough as well. We will just work with the formula for linearized variance that is arrived at in the Wikipedia post:\n\\[\n\\hat{\\text{Var}}(\\bar{X}_w) = \\frac{\\sum w_i^2 (x_i - \\bar{X}_w)^2}{\\left(\\sum w_i\\right)^2}\n\\] The standard error is just the square root of that variance.\n\\[\n\\hat{\\text{SE}}(\\bar{X}_w) = \\sqrt{\\hat{\\text{Var}}(\\bar{X}_w)}\n\\] We’ll code that up in R:\n\nnumerator &lt;- sum(households_w_income$HHWT^2 * (households_w_income$HHINCOME - weighted_mean)^2)\ndenominator &lt;- sum(households_w_income$HHWT)^2\nvariance &lt;- numerator / denominator\nse &lt;- sqrt(variance)\nse\n\n[1] 4141.197\n\n\nThat result is a usable estimate for the standard error of the average household income in Oakland. Of course in practice we would always use a package, in this case survey, for calculating that. Here I’ll calculate the average household income in Oakland using survey, and it will default to returning a standard error via the Taylor series method.\n\nhouseholds_w_income %&gt;%\n  as_survey(weights=HHWT) %&gt;%\n  summarise(weighted_mean = survey_mean(HHINCOME))\n\n\n\nTable 14: Taylor Series Method Standard Error\n\n\n\n\n\n\n\nweighted_mean\nweighted_mean_se\n\n\n\n\n140889.1\n4142.384\n\n\n\n\n\n\n\n\n\nThis standard error is off from ours by about $1, which could be due to other corrections applied in the package’s variance calculation. We needn’t be too worried about this discrepancy, though. While the Taylor series method is popular in survey analysis, IPUMS recommends that we use the replication weights method when working with ACS microdata.\n\n\nReplication weights\nIn theory, the standard error of an estimate measures the variation of a statistic across multiple samples from a given population. Our sample standard error above, calculated using just the one sample, is an estimate of that theoretical standard error.\nReplicate methods operate under the assumption that one sample can be conceived of as a miniature version of the population. Instead of taking multiple samples from the population to construct a variance estimator, one may simply resample from the full, original sample to mimic the theoretical basis of standard errors (Lohr 2021, chap 9.1).\nIn practice, we don’t explicitly do any resampling, instead relying on “replicate weights” that the census pre-computes. Here we can see what replicate weights (in this case, household replicate weights) look like in our data. Each of REPWT, 1 through 80, is a set of alternative household weights, slightly different from the “production weight,” HHWT.\n\nhouseholds_w_income %&gt;%\n  mutate(` ` = \"...\") %&gt;% \n  select(c(HHINCOME, HHWT, REPWT1, REPWT2, ` `, REPWT79, REPWT80)) %&gt;% \n  head()\n\n\n\nTable 15: ACS Replicate Household Weights\n\n\n\n\n\n\n\nHHINCOME\nHHWT\nREPWT1\nREPWT2\n\nREPWT79\nREPWT80\n\n\n\n\n129200\n50\n80\n47\n…\n49\n50\n\n\n107000\n73\n20\n25\n…\n72\n74\n\n\n138000\n74\n86\n85\n…\n21\n73\n\n\n44800\n41\n12\n43\n…\n39\n44\n\n\n380000\n311\n99\n86\n…\n323\n105\n\n\n189150\n65\n22\n20\n…\n108\n122\n\n\n\n\n\n\n\n\n\nWe can use replicate weights in a variety of alternative variance estimates. In the IPUMS documentation on ACS replicate weights, they outline a variance estimation procedure that matches the “successive difference replication (SDR) variance” method. We obtain SDR variance by calculating our statistic of interest with the production weights (e.g. HHWT) as follows:\n\\[\n\\bar{X}_w = \\frac{\\sum_{i=1}^{n}\\text{HHWT}_ix_i}{\\sum_{i=1}^{n}\\text{HHWT}_i}\n\\]\nThen, for each of the 80 household replicate weights (e.g. REPWT1), we calculate the weighted average income using the replicate weight\n\\[\n\\bar{X}_r = \\frac{\\sum_{i=1}^{n}\\text{REPWT}_{ri}x_i}{\\sum_{i=1}^{n}\\text{REPWT}_{ri}}\n\\] and sum the squared deviations between that “replicate-weighted” estimate, \\(\\bar{X}_r\\), and the production weighted estimate. Specifically: \\[\n\\begin{align*}\n\\hat{\\text{Var}}(\\bar{X}_w) &= \\frac{4}{80} \\sum_{r=1}^{80} (\\bar{X}_r - \\bar{X}_w)^2\n\\end{align*}\n\\]\n(Fay and Train 1995) initially proposed that variance estimator and their paper gives the full derivation. The standard error is once again just the square root of that variance.\n\\[\n\\hat{\\text{SE}}(\\bar{X}_w) = \\sqrt{\\hat{\\text{Var}}(\\bar{X}_w)}\n\\] In code, calculated via a for-loop, we’d get the following:\n\n# Calculate X_r\nX_r &lt;- vector()\nfor (r in 1:80){\n  X_r[r] &lt;- households_w_income %&gt;%\n    as_survey(weights=glue(\"REPWT\", r)) %&gt;%\n    summarise(weighted_mean = survey_mean(HHINCOME)) %&gt;% \n    .$weighted_mean\n}\n# Calculate X\nX &lt;- households_w_income %&gt;%\n    as_survey(weights=HHWT) %&gt;%\n    summarise(weighted_mean = survey_mean(HHINCOME)) %&gt;% \n    .$weighted_mean\n\n# Sum over r\nsqrt( (4/80) * sum( (X_r - X)^2 ) )\n\n[1] 3432.032\n\n\nTo be clear, we don’t do that manually in practice – survey supports specifying survey designs with replicate weights. The following is an implementation of the same standard error calculation procedure. Note the identical output.\n\nhouseholds_w_income %&gt;% \n  as_survey_rep(\n  weight = HHWT ,\n  repweights = matches(\"REPWT[0-9]+\"),\n  type = \"successive-difference\",\n  mse = TRUE) %&gt;%\n  summarise(weighted_mean = survey_mean(HHINCOME))\n\n\n\n\n\nweighted_mean\nweighted_mean_se\n\n\n\n\n140889.1\n3432.032\n\n\n\n\n\n\nThere are a few different replicate methods for variance estimation, several of which are equivalent. For example, the successive-difference standard error above is equivalent to using Fay’s Balanced Repeated Replication method with the Faye coefficient set to \\(\\epsilon=.5\\).\n\nhouseholds_w_income %&gt;% \n  as_survey_rep(\n  weight = HHWT ,\n  repweights = matches(\"REPWT[0-9]+\"),\n  type = \"Fay\",\n  rho=.5,\n  mse = TRUE) %&gt;%\n  summarise(weighted_mean = survey_mean(HHINCOME))\n\n\n\n\n\nweighted_mean\nweighted_mean_se\n\n\n\n\n140889.1\n3432.032\n\n\n\n\n\n\nIPUMS’ documentation recommends the following specification, where the replicate method is set to “ACS.” This has the same output as the previous two specifications, and is presumably implementing SDR variance.\n\nhouseholds_w_income %&gt;% \nas_survey_rep(\n  weight = HHWT,\n  repweights = matches(\"REPWT[0-9]+\"),\n  type = \"ACS\",\n  mse = TRUE) %&gt;%\n  summarise(weighted_mean = survey_mean(HHINCOME))\n\n\n\n\n\nweighted_mean\nweighted_mean_se\n\n\n\n\n140889.1\n3432.032\n\n\n\n\n\n\nNow that we have the methods established, we’ll return to the median, rather than the mean, household income. Some of the above formulas would be different for the median, but the code and package implementations are the same. Here we will calculate the median household income in Oakland, and the standard error for that statistic using three methods:\n\nACS aggregate data retrieval\nEstimation via IPUMS microdata with Taylor series variance/standard error\nEstimation via IPUMS microdata with replicate weight variance/standard error\n\n\n\nCode\ndf1 &lt;- oakland_stats %&gt;% \n  mutate(`Standard Error` = moe/1.645, Method = 'Aggregate Data') %&gt;%\n  filter(variable == 'median_hh_income') %&gt;% \n  select(c(Method, estimate, `Standard Error`)) %&gt;% \n  rename(`Weighted Median HH Income` = estimate)\ndf2 &lt;- households_w_income %&gt;% \nas_survey(\n  weights = HHWT) %&gt;%\n  summarise(`Weighted Median HH Income` = survey_median(HHINCOME)) %&gt;% \n  rename(`Standard Error` = `Weighted Median HH Income_se`) %&gt;% \n  mutate(Method = 'Microdata w/ Taylor Series Method') %&gt;% \n  select(c(Method, `Weighted Median HH Income`, `Standard Error`))\ndf3 &lt;- households_w_income %&gt;% \nas_survey_rep(\n  weight = HHWT,\n  repweights = matches(\"REPWT[0-9]+\"),\n  type = \"ACS\",\n  mse = TRUE) %&gt;%\n  summarise(`Weighted Median HH Income` = survey_median(HHINCOME)) %&gt;% \n  rename(`Standard Error` = `Weighted Median HH Income_se`) %&gt;% \n  mutate(Method = 'Microdata w/ Replicate Weights') %&gt;% \n  select(c(Method, `Weighted Median HH Income`, `Standard Error`))\n\ncombined_df &lt;- bind_rows(df1, df2, df3)\ncombined_df\n\n\n\n\n\n\n\n\n\n\n\nMethod\nWeighted Median HH Income\nStandard Error\n\n\n\n\nAggregate Data\n93146\n3788.450\n\n\nMicrodata w/ Taylor Series Method\n89000\n4354.199\n\n\nMicrodata w/ Replicate Weights\n89000\n3793.112\n\n\n\n\n\n\nIn IPUMS testing of ACS/PRCS data, replicate weights usually increase standard errors, though in our case the replicate weights produced a slightly smaller standard error than the Taylor series method."
  },
  {
    "objectID": "posts/ipums-wth-r/index.html#a-production-workflow",
    "href": "posts/ipums-wth-r/index.html#a-production-workflow",
    "title": "Working with ACS microdata in R",
    "section": "A Production Workflow",
    "text": "A Production Workflow\nPutting all of this post together, here is production code that I would use to build a dataset for analyzing household-level variables in Oakland, leaning on the IPUMS and geocorr helper functions I defined above, and specifying replicate weights and “ACS” variance.\n\n# Get the IPUMS data for California\ndata &lt;- ipums_data(\"us2022a\", variables, \"Incomes by PUMA\")\n# Find the geographic correspondence for places -&gt; pumas\ncsv_data &lt;- geocorr_2022(\"Ca06\", \"puma22\", \"place\", \"pop20\")\n# isolate pumas that correspond to oakland and have oak population\noak_pumas &lt;- csv_data %&gt;%\n  select(-c(state, stab, place, PUMA22name)) %&gt;%\n  filter(PlaceName == 'Oakland city, CA', afact &gt; 0) %&gt;% \n  pull(puma22) %&gt;% \n  as.integer()\n# Filter the IPUMS data to those pumas\noak_hh_w_inc &lt;- data %&gt;%\n  filter(PUMA %in% oak_pumas) %&gt;% \n  distinct(SAMPLE, SERIAL, .keep_all = TRUE) %&gt;% \n  filter(HHINCOME != 9999999, HHINCOME &gt; 0) %&gt;% \n  haven::zap_labels()\n# convert the IPUMS data to a survey dataset\noak_hh_inc_svy &lt;- households_w_income %&gt;% \n  as_survey_rep(\n    weight = HHWT,\n    repweights = matches(\"REPWT[0-9]+\"),\n    type = \"ACS\",\n    mse = TRUE)\n\nAnd here is the median household income.\n\noak_hh_inc_svy %&gt;%\n  summarise(median_hh_inc = survey_median(HHINCOME))\n\n\n\n\n\nmedian_hh_inc\nmedian_hh_inc_se\n\n\n\n\n89000\n3793.112\n\n\n\n\n\n\n\n\nWarning: package 'reticulate' was built under R version 4.3.3"
  },
  {
    "objectID": "posts/ipums-wth-r/index.html#footnotes",
    "href": "posts/ipums-wth-r/index.html#footnotes",
    "title": "Working with ACS microdata in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI want to note that several of the points I cover here are things I learned from some coworkers– Bert Wilden and Stephanie Peng.↩︎\nFor an informal analysis, I might just use a web-based tool like Census Reporter to quickly look something up.↩︎\nSee the full variable list for the 2022 1-year ACS for the available variables, and see the Census Place table for looking up GEOIDs↩︎\nI’ll to note that the combination of IPUMS and Geocorr is a fantastic public good, and it’s extremely generous of the public Universities of Minnesota and Missouri to publish these.↩︎\nWere the AFACT higher, e.g. 1%, I could randomly sample 1% of the individuals from that PUMA and include them in my Oakland sample.↩︎"
  },
  {
    "objectID": "posts/minima-3d/index.html",
    "href": "posts/minima-3d/index.html",
    "title": "Simple Optimization in 3D",
    "section": "",
    "text": "I’ve previously blogged about optimization in two dimensional space:\nIn this post, I’m going to expand into three dimensions, with an overview of the analytical solution to a distance minimization problem.\nCode\nlibrary(plotly)\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/minima-3d/index.html#footnotes",
    "href": "posts/minima-3d/index.html#footnotes",
    "title": "Simple Optimization in 3D",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n0 is the smallest possible real number output of the square root function.↩︎\nThis is beyond the scope of this post, but based on the inequality \\(0 \\leq x \\leq y \\rightarrow  x^2 \\leq y^2\\).↩︎"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Professional",
    "section": "",
    "text": "The following are some of my formal, public-facing projects:\n Interactive CalWORKs Resource Model\nStephanie Peng, Peter Amerkhanian, Joaquín Carbonell (2024)\nDashboard for The California Department of Social Services.\nLive Report Dashboard Static Report\nSummary: We produce an interactive tool to model the anti-poverty effects of state/federal supports on families participating in CalWORKs.\nContribution: Co-wrote R code-base, developed Tableau visual.\nTools: ipumsr, Tableau.\n\n A Tale of Two Cities: Alameda and Alameda Point\nPeter Amerkhanian (2023)\nConsultant Report for The City of Alameda Department of Finance.\nReport Executive Summary\nSummary: I utilize various demographic and financial data to analyze redevelopment alternatives in the Alameda Point region.\nTools: geopandas, sklearn.\n\n A Reexamination of Proposition 13 Using Parcel Level Data\nPeter Amerkhanian, Max Zhang, and James Hawkins (2023)\nReport for The UC Berkeley Institute for Young Americans.\nReport Executive Summary Data Appendix\nSummary: We utilize 12 million property records in California to estimate the tax discount effects of Proposition 13 across property types.\nContribution: Co-wrote R code-base, co-developed statistical design, wrote report draft.\nTools: tidyverse, marginaleffects.\n\n Supporting Whole Families: SparkPoint® Community Schools\nPeter Amerkhanian and Ena Yasuhara Li (2021)\nReport for United Way Bay Area.\nReport\nSummary: I utilize client outcome data and interviews with clients and program staff to evaluate United Way Bay Area’s Community Schools program.\nTools: pandas.\n\n Grade Migration Manager\nPeter Amerkhanian (2019)\nSoftware developed for Colegio Ismael Perez Pazmiño via The U.S. Peace Corps.\nCode Slides Demo Video\nSummary: I developed a full stack software application to migrate student grade data from a school’s excel-based system into the education ministry’s java application.\nTools: pyautogui, xlwings."
  },
  {
    "objectID": "projects.html#professional-reports-dashboards-software",
    "href": "projects.html#professional-reports-dashboards-software",
    "title": "Professional",
    "section": "",
    "text": "The following are some of my formal, public-facing projects:\n Interactive CalWORKs Resource Model\nStephanie Peng, Peter Amerkhanian, Joaquín Carbonell (2024)\nDashboard for The California Department of Social Services.\nLive Report Dashboard Static Report\nSummary: We produce an interactive tool to model the anti-poverty effects of state/federal supports on families participating in CalWORKs.\nContribution: Co-wrote R code-base, developed Tableau visual.\nTools: ipumsr, Tableau.\n\n A Tale of Two Cities: Alameda and Alameda Point\nPeter Amerkhanian (2023)\nConsultant Report for The City of Alameda Department of Finance.\nReport Executive Summary\nSummary: I utilize various demographic and financial data to analyze redevelopment alternatives in the Alameda Point region.\nTools: geopandas, sklearn.\n\n A Reexamination of Proposition 13 Using Parcel Level Data\nPeter Amerkhanian, Max Zhang, and James Hawkins (2023)\nReport for The UC Berkeley Institute for Young Americans.\nReport Executive Summary Data Appendix\nSummary: We utilize 12 million property records in California to estimate the tax discount effects of Proposition 13 across property types.\nContribution: Co-wrote R code-base, co-developed statistical design, wrote report draft.\nTools: tidyverse, marginaleffects.\n\n Supporting Whole Families: SparkPoint® Community Schools\nPeter Amerkhanian and Ena Yasuhara Li (2021)\nReport for United Way Bay Area.\nReport\nSummary: I utilize client outcome data and interviews with clients and program staff to evaluate United Way Bay Area’s Community Schools program.\nTools: pandas.\n\n Grade Migration Manager\nPeter Amerkhanian (2019)\nSoftware developed for Colegio Ismael Perez Pazmiño via The U.S. Peace Corps.\nCode Slides Demo Video\nSummary: I developed a full stack software application to migrate student grade data from a school’s excel-based system into the education ministry’s java application.\nTools: pyautogui, xlwings."
  },
  {
    "objectID": "projects.html#teaching",
    "href": "projects.html#teaching",
    "title": "Professional",
    "section": "Teaching",
    "text": "Teaching\nThe following are courses and workshops that I’ve taught, largely at UC Berkeley:\n Python Data Wrangling Workshop\nPeter Amerkhanian, Pratik Sachdeva, Tom van Nuenen, Aniket Kesari (2023)\nWorkshop designed and taught for the UC Berkeley D-Lab.\nCode\nSummary: I led a re-design of this 3-hour, flagship Python workshop for the UC Berkeley D-Lab. It provides an introduction to basic data wrangling actions – index, merge, group, and plot – using pandas.\n\n Python for MBAs\nPeter Amerkhanian (2023)\nWorkshop designed and taught for the UC Berkeley D-Lab.\nCode\nSummary: I designed this one and a half hour workshop and taught it to MBA students at UC Berkeley’s Haas Business School. The course directly connects pandas functions to equivalent workflows an intermediate Excel user would be familiar with."
  },
  {
    "objectID": "projects.html#academic-course-papers",
    "href": "projects.html#academic-course-papers",
    "title": "Professional",
    "section": "Academic Course Papers",
    "text": "Academic Course Papers\nThe following are some of my course papers from graduate school:\nSimulating School Desegregation in San Francisco\nPeter Amerkhanian (2022), Public Policy 275 Final Paper.\nPaper Repo\nSummary: I use a synthetic dataset of San Francisco public high school students and spatial optimization methods to simulate the effects of various busing strategies for racial desegregation outcomes.\nTools: geopandas, networkx. Methods: dijkstra’s algorithm, entropy/dissimilarity statistics.\n\nMeasuring Differences in California Politician Agendas in Press Releases\nPeter Amerkhanian (2021), Information 254 Final Paper.\nPaper\nSummary: I use a novel dataset of press releases issued by governors and mayors in California to 1.) Develop a regression model to identify press release authorship, 2.) Cluster press releases by topic, and 3.) Estimate the political similarities between mayors and governors.\nTools: gensim, sklearn. Methods: logistic regression, latent dirichlet allocation."
  },
  {
    "objectID": "memos.html",
    "href": "memos.html",
    "title": "Memos",
    "section": "",
    "text": "The following are short, standalone policy analyses that I completed as part of graduate studies, consulting projects, or else for fun. Let me know if you find errors or else find them interesting! Note that the findings of these memos are my own and do no reflect the views of the University of California.\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "The following are a collection of my informal notes on topics in programming, math, statistics, and policy analysis. Note that these posts do not reflect the views of my employer. If you find errors or else find them helpful, I’d love to hear from you!\nSubscribe for Email Updates  RSS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing The Excel Object Model in Python\n\n\n\n\n\n\nPython\n\n\nData Management\n\n\n\nReplace VBA with Python. Interacting with Excel using xlwings/pywin32 to produce formatted tables.\n\n\n\n\n\nApr 20, 2025\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with ACS microdata in R\n\n\n\n\n\n\nR\n\n\nData Management\n\n\n\nWhat’s the median household income in Oakland? Using ipumsr, geocorr, and survey to analyze census micro-data for a small-geography.\n\n\n\n\n\nFeb 20, 2025\n\n\n30 min\n\n\n\n\n\n\n\n\n\n\n\n\nGrouping Problems with SQL\n\n\n\n\n\n\nSQL\n\n\nData Management\n\n\n\nUsing DuckDB to look at some GROUP BY and PARTITION BY statements in SQL.\n\n\n\n\n\nNov 20, 2024\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Time-series Utilities for matplotlib\n\n\n\n\n\n\nPython\n\n\n\nWriting utility functions for matplotlib and using them to visualize BART’s ridership data.\n\n\n\n\n\nJun 16, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nThe Covariance Matrix from Scratch\n\n\n\n\n\n\nPython\n\n\nLinear Algebra\n\n\n\nNotes on matrix algebra and covariance, with a mix of formal notation and python code.\n\n\n\n\n\nMay 31, 2024\n\n\n22 min\n\n\n\n\n\n\n\n\n\n\n\n\nIntegrals in Probability\n\n\n\n\n\n\nR\n\n\nCalculus\n\n\nProbability\n\n\n\nNotes on the definite integral and probability modeling, with a mix of formal notation, plotly visuals, and sympy computation.\n\n\n\n\n\nApr 14, 2024\n\n\n18 min\n\n\n\n\n\n\n\n\n\n\n\n\nBenchmarking parquet and dask\n\n\n\n\n\n\nPython\n\n\nData Management\n\n\n\nUsing parquet+dask to efficiently read and manipulate BART’s historical ridership data.\n\n\n\n\n\nApr 12, 2024\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nProduction Maximization with Lagrange Mutlipliers\n\n\n\n\n\n\nR\n\n\nCalculus\n\n\n\nSolving an optimization problem in (Strang and Herman 2016) using lagrange multipliers, with some plotly visuals mixed in.\n\n\n\n\n\nMar 17, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Optimization in 3D\n\n\n\n\n\n\nR\n\n\nCalculus\n\n\n\nSolving an optimization problem in (Strang and Herman 2016), with plotly visuals.\n\n\n\n\n\nMar 9, 2024\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Approximation in 3D\n\n\n\n\n\n\nPython\n\n\nCalculus\n\n\n\nUsing taylor series to approximate function output, with python visuals.\n\n\n\n\n\nMar 2, 2024\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nLaw Enforcement Activity in San Francisco, 2018-2024\n\n\n\n\n\n\nPolicy Analysis\n\n\n\nI analyze trends in arrests and citations in San Francisco \n\n\n\n\n\nFeb 17, 2024\n\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n\nOverfitting and The Train-Test Split\n\n\n\n\n\n\nPython\n\n\nStatistics\n\n\n\nUsing re-sampling to better estimate model performance.\n\n\n\n\n\nDec 8, 2023\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nIterated Expectations\n\n\n\n\n\n\nPython\n\n\nProbability\n\n\n\nNotes on conditional probability and adam’s law, with a mix of formal notation and python code.\n\n\n\n\n\nDec 1, 2023\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nNewton’s Method From The Ground Up\n\n\n\n\n\n\nPython\n\n\nCalculus\n\n\n\nUsing newton’s method for root-finding and optimization, with python code and a .gif of the search.\n\n\n\n\n\nNov 3, 2023\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Constrained Optimization in 2D\n\n\n\n\n\n\nPython\n\n\nCalculus\n\n\n\nIntroductory optimization, with formal notation, python code, and matplotlib visuals.\n\n\n\n\n\nJul 4, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nPolicy Responses to a Coronavirus Pandemic\n\n\n\n\n\n\nPolicy Analysis\n\n\n\nI develop a mathematical model to analyze the effects of policy interventions during a Coronavirus pandemic.\n\n\n\n\n\nDec 15, 2022\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nBuy vs. Rent, A Financial Modeling Workflow in Python\n\n\n\n\n\n\nPython\n\n\nProbability\n\n\n\nUsing numpy-financial and monte-carlo simulation to evaluate investments.\n\n\n\n\n\nAug 6, 2022\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nReliable PDF Scraping with tabula-py\n\n\n\n\n\n\nPython\n\n\nData Management\n\n\n\nUsing tabula in python to read in tables from .pdf files\n\n\n\n\n\nJul 20, 2022\n\n\n6 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/sql-count-cases/index.html",
    "href": "posts/sql-count-cases/index.html",
    "title": "Grouping Problems with SQL",
    "section": "",
    "text": "import duckdb\nimport pandas as pd\nimport numpy as np\nimport random\nfrom IPython.display import display, Markdown\nLately I’ve been struggling through some SQL query development, and I thought it would be useful to write out some of what I’ve been learning in a blog post. Specifically, I’ve been thinking about how to do various grouping tasks when data have a hierarchical structure. I’ll cover the following SQL concepts:\nThis post is also an opportunity to overview how I display and run SQL code with syntax highlighting in Quarto – a nontrivial task as it turns out!\nTo start, I’ll synthesize a simple dataset that examines car accidents on the Interstate 280 and 580 highways in the Bay Area. I’ll go over the specifics later, but for now just note that this is a standard pandas dataframe stored in local memory as accidents. It contains data on cars’ trips on the two highways, and a flag denoting whether the trip was associated with an accident or not.\nnp.random.seed(42)\nn = 1000\ndata = {\n    \"license_plate\": [f\"{random.randint(90000, 90099)}\" for _ in range(n)],\n    \"highway\": [random.choice([\"I280\", \"I580\"]) for _ in range(n)],\n    \"trip_date\": [f\"2024-11-{random.randint(13, 15)}\" for _ in range(n)],\n    \"accident_flag\": np.random.binomial(1, .1, n),\n}\naccidents = pd.DataFrame(data)"
  },
  {
    "objectID": "posts/sql-count-cases/index.html#connecting-to-a-pandas-data-frame-in-duckdb",
    "href": "posts/sql-count-cases/index.html#connecting-to-a-pandas-data-frame-in-duckdb",
    "title": "Grouping Problems with SQL",
    "section": "Connecting to a pandas data frame in duckdb",
    "text": "Connecting to a pandas data frame in duckdb\nThe data I simulated are quite small, so using SQL isn’t going to be very practically useful. However, I find that it’s helpful to play with small SQL databases like this to build general comfort with the language and to get a better intuition of how operations will play out when I’m are working at scale.\nI’m going to use the duckdb database system to directly query the pandas data frame I have in local memory (Note that duckdb can also directly query parquet files, csv files, and more).\n\nduckdb.sql(\"SELECT * FROM accidents LIMIT 5;\")\n\n┌───────────────┬─────────┬────────────┬───────────────┐\n│ license_plate │ highway │ trip_date  │ accident_flag │\n│    varchar    │ varchar │  varchar   │     int32     │\n├───────────────┼─────────┼────────────┼───────────────┤\n│ 90092         │ I580    │ 2024-11-14 │             0 │\n│ 90095         │ I580    │ 2024-11-14 │             1 │\n│ 90080         │ I280    │ 2024-11-15 │             0 │\n│ 90026         │ I280    │ 2024-11-15 │             0 │\n│ 90093         │ I280    │ 2024-11-13 │             0 │\n└───────────────┴─────────┴────────────┴───────────────┘\n\n\n\nExecuting and displaying SQL in Quarto\nThe fact that duckdb can directly query data in memory is great, but two issues arise – the SQL code doesn’t have any syntax highlighting, and it would be preferable if the output were a data frame rather than plain text. I’ll create a wrapper function, run_query that solves these issues:\n\ndef run_query(sql_str: str) -&gt; pd.DataFrame:\n  display(Markdown(\"```sql\\n\" + sql_str.strip(\"\\n\") + \"\\n```\"))\n  return duckdb.sql(sql_str).df()\n\nNote the following code elements:\n\ndisplay(Markdown(...)) will display sql_str, the string code for the query, but with a wrapper around it as follows:\n\n```{sql}\nsql_str \n```\nThe wrapper alerts Quarto that it should provide SQL syntax-highlighting for the code output.\n\nduckdb.sql(sql_str).df() returns the results of the SQL query as a pd.DataFrame object rather than the text-table rendered above.\n\nFor the rest of the post, I will use that function, within cells with the setting echo: false to carry out my SQL queries. Thus, you won’t see the python function, just a code chunk of the SQL query code and the query result. Here’s an example query, looking at the first five rows of accidents:\n\n\nSELECT * FROM accidents\nLIMIT 5;\n\n\n\n\n\n\n\n\n\nlicense_plate\nhighway\ntrip_date\naccident_flag\n\n\n\n\n0\n90092\nI580\n2024-11-14\n0\n\n\n1\n90095\nI580\n2024-11-14\n1\n\n\n2\n90080\nI280\n2024-11-15\n0\n\n\n3\n90026\nI280\n2024-11-15\n0\n\n\n4\n90093\nI280\n2024-11-13\n0\n\n\n\n\n\n\n\nNote the hierarchy of these trip data. We start with Dates and proceed downward:\n\nDate\n\nHighway\n\nA highway will have multiple cars that use it.\n\nCar (license plate)\n\nA car can make multiple trips.\n\nTrip (each row is a trip)\n\nAssociated with:\n\nAccident flag\n\n\n\n\n\n\n\n\nI’ll look at a few key SQL methods using these data."
  },
  {
    "objectID": "posts/sql-count-cases/index.html#aggregating-with-group-by",
    "href": "posts/sql-count-cases/index.html#aggregating-with-group-by",
    "title": "Grouping Problems with SQL",
    "section": "Aggregating with GROUP BY",
    "text": "Aggregating with GROUP BY\nLet’s say we were interested in counting total trips and total accidents for each highway on each day. This is a group aggregation task, where we want to calculate summary statistics (two counts) for subsets of the data grouped by one or more columns. In SQL this is done using the GROUP BY clause and an aggregation function (COUNT and SUM in this case). Equivalent pandas code is available via the navbar. In this case, I like the SQL and pandas syntax about equally.\n\nSQLpandas\n\n\n\n\nSELECT\n  highway,\n  trip_date,\n  COUNT(*) as n_trips,\n  SUM(accident_flag) as n_accidents\nFROM\n  accidents\nGROUP BY\n  highway,\n  trip_date\nORDER BY\n  highway,\n  trip_date\n\n\n\n\n\n\n\n\n\nhighway\ntrip_date\nn_trips\nn_accidents\n\n\n\n\n0\nI280\n2024-11-13\n151\n19.0\n\n\n1\nI280\n2024-11-14\n181\n22.0\n\n\n2\nI280\n2024-11-15\n176\n15.0\n\n\n3\nI580\n2024-11-13\n171\n16.0\n\n\n4\nI580\n2024-11-14\n167\n13.0\n\n\n5\nI580\n2024-11-15\n154\n15.0\n\n\n\n\n\n\n\n\n\n\n(\n    accidents\n    .groupby(['highway', 'trip_date'], as_index=False)\n    .agg(\n        n_trips=('trip_date', 'size'),\n        n_accidents_date_highway=('accident_flag', 'sum')\n    )\n    .sort_values(by=['highway', 'trip_date'])\n)\n\n\n\n\n\n\n\n\nhighway\ntrip_date\nn_trips\nn_accidents_date_highway\n\n\n\n\n0\nI280\n2024-11-13\n151\n19\n\n\n1\nI280\n2024-11-14\n181\n22\n\n\n2\nI280\n2024-11-15\n176\n15\n\n\n3\nI580\n2024-11-13\n171\n16\n\n\n4\nI580\n2024-11-14\n167\n13\n\n\n5\nI580\n2024-11-15\n154\n15\n\n\n\n\n\n\n\n\n\n\nNote that the GROUP BY operation reduces our data granularity down to the unique highway-date pairs. This is typical aggregation behavior. Also note in lines 4 and 5 of the SQL code, that for accidents, I use COUNT(*). This simply counts all rows within the group. In the case of the accident flags, I use SUM(accident_flag) within groups since the column is a 1/0 integer flag."
  },
  {
    "objectID": "posts/sql-count-cases/index.html#aggregating-with-partition-by",
    "href": "posts/sql-count-cases/index.html#aggregating-with-partition-by",
    "title": "Grouping Problems with SQL",
    "section": "Aggregating with PARTITION BY",
    "text": "Aggregating with PARTITION BY\nThe Partition is an odd cousin of the Group that I became aware of only recently. In essence, it accomplishes the same thing as GROUP BY, but it doesn’t compress the data or otherwise change granularity. Instead, it returns a dataframe with a group’s aggregate result replicated at every row where that group exists. I can express this more clearly via example.\n\nSQLpandas\n\n\n\n\nSELECT\n    *,\n    SUM(accident_flag)\n        OVER (PARTITION BY highway, trip_date)\n    as n_accidents_date_highway\nFROM accidents\nORDER BY\n  highway,\n  trip_date;\n\n\n\n\n\n\n\n\n\nlicense_plate\nhighway\ntrip_date\naccident_flag\nn_accidents_date_highway\n\n\n\n\n0\n90021\nI280\n2024-11-13\n0\n19.0\n\n\n1\n90031\nI280\n2024-11-13\n0\n19.0\n\n\n2\n90019\nI280\n2024-11-13\n0\n19.0\n\n\n3\n90059\nI280\n2024-11-13\n1\n19.0\n\n\n4\n90054\nI280\n2024-11-13\n0\n19.0\n\n\n...\n...\n...\n...\n...\n...\n\n\n995\n90002\nI580\n2024-11-15\n0\n15.0\n\n\n996\n90000\nI580\n2024-11-15\n0\n15.0\n\n\n997\n90062\nI580\n2024-11-15\n0\n15.0\n\n\n998\n90037\nI580\n2024-11-15\n0\n15.0\n\n\n999\n90003\nI580\n2024-11-15\n0\n15.0\n\n\n\n\n1000 rows × 5 columns\n\n\n\n\n\n\n(\n    accidents\n    .assign(n_accidents_date_highway=\n            accidents\n            .groupby(['highway', 'trip_date'])['accident_flag']\n            .transform('sum')\n    )\n    .sort_values(by=['highway', 'trip_date'])\n)\n\n\n\n\n\n\n\n\nlicense_plate\nhighway\ntrip_date\naccident_flag\nn_accidents_date_highway\n\n\n\n\n4\n90093\nI280\n2024-11-13\n0\n19\n\n\n12\n90053\nI280\n2024-11-13\n0\n19\n\n\n23\n90086\nI280\n2024-11-13\n0\n19\n\n\n27\n90023\nI280\n2024-11-13\n0\n19\n\n\n29\n90057\nI280\n2024-11-13\n0\n19\n\n\n...\n...\n...\n...\n...\n...\n\n\n964\n90004\nI580\n2024-11-15\n0\n15\n\n\n969\n90070\nI580\n2024-11-15\n0\n15\n\n\n983\n90050\nI580\n2024-11-15\n0\n15\n\n\n988\n90000\nI580\n2024-11-15\n0\n15\n\n\n993\n90095\nI580\n2024-11-15\n0\n15\n\n\n\n\n1000 rows × 5 columns\n\n\n\n\n\n\nNote in the output that the number of accidents on I280 on 11/15/2024 is the same as it was in the grouped output above, but it repeats for every row from highway I280 on 11/15/2024 in the partitioned version. The pandas implementation for this task, available via the navbar, relies on the combination of groupby() and transform(), which I find unintuitive.\nThus, PARTITION BY preserves the long format of the data, while still allowing for the creation of aggregate columns. This can be useful behavior, though can introduce some confusion, since we now have a column of group-level results in a data frame that is not compressed to the group level. I try to use column naming to make that clear, but it’s a little difficult."
  },
  {
    "objectID": "posts/sql-count-cases/index.html#aggregation-across-common-table-expressions",
    "href": "posts/sql-count-cases/index.html#aggregation-across-common-table-expressions",
    "title": "Grouping Problems with SQL",
    "section": "Aggregation across common table expressions",
    "text": "Aggregation across common table expressions\nNow let’s use GROUP BY and PARTITION BY together in a complex query. At this point I’ll stop providing pandas translations because I view it as less analogous – were I doing queries like this in pandas, I would be stopping to define intermediary tables and things would generally look different.\nSuppose we want to know the number of accidents on each highway each day, but we also want to have some information about whether that number of accidents is typical for that highway, or if it’s an outlier. We can use a z-score to measure the position of an accident count within the highway’s historical distribution, but computing both the count of accidents at the highway-date level, then computing z-scores per highway is complicated.\nIn SQL, complex queries like this often involve Common Table Expressions (CTEs). I find working with CTEs more than a little difficult because it’s hard to determine what each CTE outputs. Below, I show a full query that uses both GROUP BY and PARTITION BY to complete my task. I also include a navigation bar that allows one to see what each of the three intermediary select statements output, thus expressing the query as a series of smaller queries and making the CTE content more transparent.\n\nFull QueryStep 1Step 2Step 3\n\n\n\n\nWITH grouped_table as (\n  SELECT\n    highway,\n    trip_date,\n    COUNT(*) as n_trips,\n    SUM(accident_flag) as n_accidents \n  FROM\n    accidents\n  GROUP BY\n    highway,\n    trip_date\n),\n          \npartitions_table as (\n  SELECT\n    *,\n    AVG(n_accidents) OVER (PARTITION BY highway) as highway_average,\n    STDDEV_POP(n_accidents) OVER (PARTITION BY highway) as highway_std,\n  FROM grouped_table\n)\n\nSELECT\n  highway,\n  trip_date,\n  n_trips,\n  n_accidents,\n  (n_accidents - highway_average) / highway_std as highway_z_score\nFROM partitions_table;\n\n\n\n\n\n\n\n\n\nhighway\ntrip_date\nn_trips\nn_accidents\nhighway_z_score\n\n\n\n\n0\nI280\n2024-11-13\n151\n19.0\n0.116248\n\n\n1\nI280\n2024-11-14\n181\n22.0\n1.162476\n\n\n2\nI280\n2024-11-15\n176\n15.0\n-1.278724\n\n\n3\nI580\n2024-11-15\n154\n15.0\n0.267261\n\n\n4\nI580\n2024-11-13\n171\n16.0\n1.069045\n\n\n5\nI580\n2024-11-14\n167\n13.0\n-1.336306\n\n\n\n\n\n\n\n\n\nWe start with the same basic grouped table, where each row is a unique highway-date combination with its total trips and total accidents.\n\n\nWITH grouped_table as (\n  SELECT\n    highway,\n    trip_date,\n    COUNT(*) as n_trips,\n    SUM(accident_flag) as n_accidents \n  FROM\n    accidents \n  GROUP BY\n    highway,\n    trip_date\n)\nSELECT * FROM grouped_table\n\n\n\n\n\n\n\n\n\nhighway\ntrip_date\nn_trips\nn_accidents\n\n\n\n\n0\nI280\n2024-11-15\n176\n15.0\n\n\n1\nI580\n2024-11-13\n171\n16.0\n\n\n2\nI280\n2024-11-14\n181\n22.0\n\n\n3\nI580\n2024-11-14\n167\n13.0\n\n\n4\nI280\n2024-11-13\n151\n19.0\n\n\n5\nI580\n2024-11-15\n154\n15.0\n\n\n\n\n\n\n\n\n\nWe proceed to use PARTITION BY statements on that grouped table, finding the average and standard deviation across dates for each highway.\n\n\nWITH partitions_table as (\n  SELECT\n    *,\n    AVG(n_accidents) OVER (PARTITION BY highway) as highway_average,\n    STDDEV_POP(n_accidents) OVER (PARTITION BY highway) as highway_std,\n  FROM grouped_table\n)\nSELECT * FROM partitions_table;\n\n\n\n\n\n\n\n\n\nhighway\ntrip_date\nn_trips\nn_accidents\nhighway_average\nhighway_std\n\n\n\n\n0\nI580\n2024-11-13\n171\n16.0\n14.666667\n1.247219\n\n\n1\nI580\n2024-11-14\n167\n13.0\n14.666667\n1.247219\n\n\n2\nI580\n2024-11-15\n154\n15.0\n14.666667\n1.247219\n\n\n3\nI280\n2024-11-15\n176\n15.0\n18.666667\n2.867442\n\n\n4\nI280\n2024-11-14\n181\n22.0\n18.666667\n2.867442\n\n\n5\nI280\n2024-11-13\n151\n19.0\n18.666667\n2.867442\n\n\n\n\n\n\n\n\n\nFinally, we can write a straightforward select statement that gets our desired final columns out of partitions_table.\n\n\nSELECT\n  highway,\n  trip_date,\n  n_trips,\n  n_accidents,\n  (n_accidents - highway_average) / highway_std as highway_z_score\nFROM partitions_table;\n\n\n\n\n\n\n\n\n\nhighway\ntrip_date\nn_trips\nn_accidents\nhighway_z_score\n\n\n\n\n0\nI580\n2024-11-13\n171\n16.0\n1.069045\n\n\n1\nI580\n2024-11-14\n167\n13.0\n-1.336306\n\n\n2\nI580\n2024-11-15\n154\n15.0\n0.267261\n\n\n3\nI280\n2024-11-15\n176\n15.0\n-1.278724\n\n\n4\nI280\n2024-11-14\n181\n22.0\n1.162476\n\n\n5\nI280\n2024-11-13\n151\n19.0\n0.116248"
  },
  {
    "objectID": "posts/sql-count-cases/index.html#multiple-aggregations-in-one-table",
    "href": "posts/sql-count-cases/index.html#multiple-aggregations-in-one-table",
    "title": "Grouping Problems with SQL",
    "section": "Multiple aggregations in one table",
    "text": "Multiple aggregations in one table\nLet’s say that, in addition to trips that were associated with accidents on highways, we were interested in counting cars that were associated with accidents on highways. This would give us a sense of if some cars are involved in multiple accidents on the same highway in the same day. Thus, we want to know:\n\nThe number of unique cars that traveled on each highway on each day\nThe number of unique cars that were associated with an accident on each highway on each day, alongside\nThe trip count and\nThe accident count, all in one table.\n\nThis implies two distinct levels of aggregation within a table, which can be complex in SQL. To start this, I’ll just look at how I would count up unique cars alone. We’ll lean on the COUNT(DISTINCT {column}) aggregation function in SQL, which counts unique rows in some column over a grouping.\n\nWhen the DISTINCT clause is provided, only distinct values are considered in the computation of the aggregate. This is typically used in combination with the count aggregate to get the number of distinct elements; but it can be used together with any aggregate function in the system.\n– DuckDB Documentation\n\n\n\nSELECT\n  highway,\n  trip_date,\n  COUNT(DISTINCT license_plate) AS n_cars\nFROM\n  accidents \nGROUP BY\n  highway,\n  trip_date\n\n\n\n\n\n\n\n\n\nhighway\ntrip_date\nn_cars\n\n\n\n\n0\nI280\n2024-11-14\n83\n\n\n1\nI280\n2024-11-15\n83\n\n\n2\nI580\n2024-11-13\n86\n\n\n3\nI580\n2024-11-14\n83\n\n\n4\nI580\n2024-11-15\n78\n\n\n5\nI280\n2024-11-13\n82\n\n\n\n\n\n\n\nThat’s fine and good, but we also want to know the count of those cars that were involved in accidents on each highway/day. This is a little complicated due to the structure of the data. Recall that the accident flag is defined at the trip level, not the car level.\n\nDate\n\nHighway\n\nA highway will have multiple cars that use it.\n\nCar (license plate)\n\nA car can make multiple trips.\n\nTrip (each row is a trip)\n\nAssociated with:\n\nAccident flag\n\n\n\n\n\n\n\n\nWe’ll need to move that flag up a level in the hierarchy, so that we have something like the following:\n\nDate\n\nHighway\n\nA highway will have multiple cars that use it.\n\nCar (license plate)\n\nAccident flag\n\n\n\n\n\nOne approach to this problem is to use GROUP BY, with grouping on highway, date, and car, then figuring out if that group (car-highway-date) had any accident flag via CASE WHEN SUM(accident_flag) &gt; 0 THEN 1 ELSE 0 END. Then, I can use GROUP BY again to return to the highway-date level and aggregate the number of cars and number of cars with accidents. After that, I can do a similar grouping exercise to count up the number of trips and trips resulting in accident, and then I can merge the two distinct tables. In other words:\n\ndefine the car with accident flag\ngroup by date-highway, aggregate cars\ngroup by date-highway, aggregate trips\nmerge (1) and (2)\n\nHere’s the whole process, executed.\n\nFull QueryStep 1Step 2Step 3Step 4\n\n\n\n\nWITH car_table as (SELECT\n  license_plate,\n  highway,\n  trip_date,\n  CASE WHEN SUM(accident_flag) &gt; 0 THEN 1 ELSE 0 END AS car_w_accident\nFROM\n  accidents \nGROUP BY\n  license_plate,\n  highway,\n  trip_date\n),\n\ncars_w_accident_count as (SELECT \n  highway,\n  trip_date,\n  COUNT(DISTINCT license_plate) as n_cars,\n  SUM(car_w_accident) as n_cars_w_accident\nFROM\n  car_table\nGROUP BY\n  highway,\n  trip_date),\n\naccident_count as (SELECT\n  highway,\n  trip_date,\n  COUNT(*) as n_trips,\n  SUM(accident_flag) as n_accidents\nFROM\n  accidents \nGROUP BY\n  highway,\n  trip_date\nORDER BY\n  highway,\n  trip_date)\n\nSELECT\na.highway,\na.trip_date,\na.n_trips,\na.n_accidents,\nb.n_cars,\nb.n_cars_w_accident\nFROM \n  accident_count a\n  JOIN cars_w_accident_count b\n    ON (a.highway = b.highway AND a.trip_date = b.trip_date)\n\n\n\n\n\n\n\n\n\nhighway\ntrip_date\nn_trips\nn_accidents\nn_cars\nn_cars_w_accident\n\n\n\n\n0\nI280\n2024-11-13\n151\n19.0\n82\n18.0\n\n\n1\nI280\n2024-11-14\n181\n22.0\n83\n20.0\n\n\n2\nI280\n2024-11-15\n176\n15.0\n83\n15.0\n\n\n3\nI580\n2024-11-13\n171\n16.0\n86\n16.0\n\n\n4\nI580\n2024-11-14\n167\n13.0\n83\n12.0\n\n\n5\nI580\n2024-11-15\n154\n15.0\n78\n15.0\n\n\n\n\n\n\n\n\n\nCreate an accident flag at the car/day/highway level. Note the use of CASE WHEN.\n\n\nWITH car_table as (SELECT\n  license_plate,\n  highway,\n  trip_date,\n  CASE WHEN SUM(accident_flag) &gt; 0 THEN 1 ELSE 0 END AS car_w_accident\nFROM\n  accidents \nGROUP BY\n  license_plate,\n  highway,\n  trip_date\n)\nSELECT * FROM car_table  \n\n\n\n\n\n\n\n\n\nlicense_plate\nhighway\ntrip_date\ncar_w_accident\n\n\n\n\n0\n90093\nI280\n2024-11-13\n0\n\n\n1\n90062\nI580\n2024-11-13\n0\n\n\n2\n90086\nI280\n2024-11-13\n0\n\n\n3\n90050\nI580\n2024-11-13\n0\n\n\n4\n90069\nI580\n2024-11-14\n0\n\n\n...\n...\n...\n...\n...\n\n\n490\n90006\nI580\n2024-11-13\n0\n\n\n491\n90021\nI280\n2024-11-13\n0\n\n\n492\n90009\nI580\n2024-11-14\n0\n\n\n493\n90024\nI280\n2024-11-14\n0\n\n\n494\n90068\nI580\n2024-11-13\n0\n\n\n\n\n495 rows × 4 columns\n\n\n\n\n\nAggregate cars.\n\n\nWITH cars_w_accident_count as (SELECT \n  highway,\n  trip_date,\n  COUNT(DISTINCT license_plate) as n_cars,\n  SUM(car_w_accident) as n_cars_w_accident\nFROM\n  car_table\nGROUP BY\n  highway,\n  trip_date)\nSELECT * FROM cars_w_accident_count;\n\n\n\n\n\n\n\n\n\nhighway\ntrip_date\nn_cars\nn_cars_w_accident\n\n\n\n\n0\nI580\n2024-11-13\n86\n16.0\n\n\n1\nI280\n2024-11-15\n83\n15.0\n\n\n2\nI280\n2024-11-14\n83\n20.0\n\n\n3\nI580\n2024-11-14\n83\n12.0\n\n\n4\nI280\n2024-11-13\n82\n18.0\n\n\n5\nI580\n2024-11-15\n78\n15.0\n\n\n\n\n\n\n\n\n\nAggregate trips (this is a table we’ve already created once before).\n\n\nWITH accident_count as (SELECT\n  highway,\n  trip_date,\n  COUNT(*) as n_trips,\n  SUM(accident_flag) as n_accidents \nFROM\n  accidents \nGROUP BY\n  highway,\n  trip_date\nORDER BY\n  highway,\n  trip_date)\nSELECT * FROM accident_count;\n\n\n\n\n\n\n\n\n\nhighway\ntrip_date\nn_cars\nn_cars_w_accident\n\n\n\n\n0\nI580\n2024-11-13\n86\n16.0\n\n\n1\nI280\n2024-11-15\n83\n15.0\n\n\n2\nI280\n2024-11-14\n83\n20.0\n\n\n3\nI580\n2024-11-14\n83\n12.0\n\n\n4\nI280\n2024-11-13\n82\n18.0\n\n\n5\nI580\n2024-11-15\n78\n15.0\n\n\n\n\n\n\n\n\n\nMerge the trip and car aggregates.\n\n\nSELECT\na.highway,\na.trip_date,\na.n_accidents,\na.n_accidents,\nb.n_cars,\nb.n_cars_w_accident\nFROM \n  accident_count a\n  JOIN cars_w_accident_count b\n    ON (a.highway = b.highway AND a.trip_date = b.trip_date)\n\n\n\n\n\n\n\n\n\nhighway\ntrip_date\nn_accidents\nn_accidents_1\nn_cars\nn_cars_w_accident\n\n\n\n\n0\nI580\n2024-11-13\n16.0\n16.0\n86\n16.0\n\n\n1\nI280\n2024-11-15\n15.0\n15.0\n83\n15.0\n\n\n2\nI280\n2024-11-14\n22.0\n22.0\n83\n20.0\n\n\n3\nI580\n2024-11-14\n13.0\n13.0\n83\n12.0\n\n\n4\nI280\n2024-11-13\n19.0\n19.0\n82\n18.0\n\n\n5\nI580\n2024-11-15\n15.0\n15.0\n78\n15.0\n\n\n\n\n\n\n\n\n\n\nSeems great, right? Maybe not – it’s a very long and overly complicated query because it goes down two separate paths of aggregation:\n\ngroup by date-highway, aggregate cars\ngroup by date-highway, aggregate trips\n\nIf we can avoid entering two distinct group/aggregation paths, the merge can be eliminated, we won’t need CTEs, and this can be simplified to the following:\n\ngroup by date-highway, aggregate cars, aggregate trips\n\nIt certainly isn’t immediately obvious (to me), but we can actually exploit a combination of CASE WHEN and COUNT (DISTINCT) to accomplish this simplification:\n\ngroup by date-highway.\n\naggregate cars with CASE WHEN and COUNT (DISTINCT)\naggregate trips with COUNT(*) and SUM\n\n\n\n\nSELECT\n    highway,\n    trip_date,\n    COUNT(*) as n_trips,\n    SUM(accident_flag) as n_accidents,\n    COUNT(DISTINCT license_plate) AS n_cars,\n    COUNT(DISTINCT\n        (CASE WHEN accident_flag = 1 THEN license_plate ELSE NULL END))\n        AS n_cars_w_accident\nFROM\n  accidents \nGROUP BY\n  highway,\n  trip_date\n\n\n\n\n\n\n\n\n\nhighway\ntrip_date\nn_trips\nn_accidents\nn_cars\nn_cars_w_accident\n\n\n\n\n0\nI280\n2024-11-13\n151\n19.0\n82\n18\n\n\n1\nI580\n2024-11-15\n154\n15.0\n78\n15\n\n\n2\nI280\n2024-11-14\n181\n22.0\n83\n20\n\n\n3\nI280\n2024-11-15\n176\n15.0\n83\n15\n\n\n4\nI580\n2024-11-13\n171\n16.0\n86\n16\n\n\n5\nI580\n2024-11-14\n167\n13.0\n83\n12\n\n\n\n\n\n\n\nThis exploits the fact that NULL values are ignored by most aggregate functions. Thus, we create a column that is essentially the same as the suspicious transaction flag, but instead of 1/0, it contains the license plate number for the car (if it was in a trip that resulted in an accident) or NULL. When we count distinct values for that column, grouped by highway and date, we then effectively count up how many unique cars associated with accidents there were in that highway-date group."
  },
  {
    "objectID": "posts/rent-vs-buy/index.html",
    "href": "posts/rent-vs-buy/index.html",
    "title": "Buy vs. Rent, A Financial Modeling Workflow in Python",
    "section": "",
    "text": "This post goes through the following exercises:\n\nUse numpy-financial to build a loan amortization calculator for a home mortgage\n\nUse said table as well as simulated home and stock equity returns over time to compare year-to-year wealth resulting from the following strategies:\n\nbuying a residential living space\n\nrenting one instead and investing the dollar amount that would have been your down-payment\n\n\n\n\nAt one point in time, numpy, the popular Python numerical analysis library, included 10 specialized functions for financial analysis. Given their specific nature, they were eventually removed from numpy, I think in 2019 (learn about why that is here) and are now available in the separate library, numpy-financial. The library still seems focused on the same 10 core functions, which handle tasks like calculating loan payment amounts given some inputs, and applied financial economics tasks like calculating time value of money. Cool… Anyways, I’ll use it to create an amortization schedule for a mortgage.\n\n\n\nI built this notebook in a Google Colab instance, which seems to include most major Python libraries (more info).\nYou’ll probably have to download numpy-financial (it’s not included in Anaconda as far as I know), which you can accomplish within any notebook-like environment using the following command:\n\n! pip install numpy-financial\n\n^C\n\n\nYou’ll want to load the usual suspects - pandas, numpy, seaborn, matplotlib. I also run from datetime import datetime since we will be working with ranges of dates, and I run sns.set_style() to get my seaborn plots looking a bit more aesthetically pleasing - read more on themes here.\n\nimport pandas as pd\nimport numpy as np\nimport numpy_financial as npf\nfrom datetime import datetime\nimport seaborn as sns\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/rent-vs-buy/index.html#summary",
    "href": "posts/rent-vs-buy/index.html#summary",
    "title": "Buy vs. Rent, A Financial Modeling Workflow in Python",
    "section": "",
    "text": "This post goes through the following exercises:\n\nUse numpy-financial to build a loan amortization calculator for a home mortgage\n\nUse said table as well as simulated home and stock equity returns over time to compare year-to-year wealth resulting from the following strategies:\n\nbuying a residential living space\n\nrenting one instead and investing the dollar amount that would have been your down-payment\n\n\n\n\nAt one point in time, numpy, the popular Python numerical analysis library, included 10 specialized functions for financial analysis. Given their specific nature, they were eventually removed from numpy, I think in 2019 (learn about why that is here) and are now available in the separate library, numpy-financial. The library still seems focused on the same 10 core functions, which handle tasks like calculating loan payment amounts given some inputs, and applied financial economics tasks like calculating time value of money. Cool… Anyways, I’ll use it to create an amortization schedule for a mortgage.\n\n\n\nI built this notebook in a Google Colab instance, which seems to include most major Python libraries (more info).\nYou’ll probably have to download numpy-financial (it’s not included in Anaconda as far as I know), which you can accomplish within any notebook-like environment using the following command:\n\n! pip install numpy-financial\n\n^C\n\n\nYou’ll want to load the usual suspects - pandas, numpy, seaborn, matplotlib. I also run from datetime import datetime since we will be working with ranges of dates, and I run sns.set_style() to get my seaborn plots looking a bit more aesthetically pleasing - read more on themes here.\n\nimport pandas as pd\nimport numpy as np\nimport numpy_financial as npf\nfrom datetime import datetime\nimport seaborn as sns\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/rent-vs-buy/index.html#problem-setup",
    "href": "posts/rent-vs-buy/index.html#problem-setup",
    "title": "Buy vs. Rent, A Financial Modeling Workflow in Python",
    "section": "Problem Setup",
    "text": "Problem Setup\n\nDefinining Constants\nI’ll run this as a comparison between buying an apartment that costs $700,000 with a 20% downpayment, versus renting a home for $2,600 a month. This is meant to approximate buying versus renting a two-bed one-bath apartment.\nBuying fees are defined at 4%, the homeowners association fees are defined as $700 monthly.\n\n# Buying Constants\ninterest_rate = 0.065\ncost = 700000\nhoa = 700\ndown_payment = cost * .2\nprincipal = cost - down_payment\nbuying_fees = principal*.04\n\n# Renting Constants\nrent = 2600\n\nnpf.pmt() can be used to generate a monthly mortgage payment given those buying constants:\n\nnpf.pmt(interest_rate/12, 12*30, principal)\n\n-3539.580931560606\n\n\nalternatively, we can use npf.ppt() to see how much of the payment goes towards the principal, and use npf.ipmt() to see how much goes towards interest (see below for applications of those functions).\n\n\nDefining Random Variables\nI’ll make the simplifying assumption that both “annual home appreciation” and “annual stock appreciation” are generated from normal distributions. This is a kind of strong assumption, but one that seems to be routinely made at least with regards to stock market returns, even if there might be better distribution choices out there (more here).\nHere’s a look at how we’ll draw from a normal distribution. Given an average annual return, \\(\\mu = 0.0572\\) (\\(\\mu\\), or, mu, is a common variable name for average) and a standard deviation \\(\\sigma = 0.1042\\) (\\(\\sigma\\), or, sigma, is the common variable name for standard deviation), we can draw one sample from a normal distribution as follows:\n\nfrom dotenv import load_dotenv\nload_dotenv()\n\nTrue\n\n\n\nimport yfinance as yf\n\n\n# Download S&P 500 data\nsp500 = yf.download(\"AAPL\", start=\"1950-01-01\")[\"Adj Close\"].dropna()\nsp500 = sp500.dropna()\n\n[*********************100%***********************]  1 of 1 completed\n\n\nERROR \n1 Failed download:\nERROR ['AAPL']: Exception('AAPL: No price data found, symbol may be delisted (1d 1950-01-01 -&gt; 2025-05-31)')\n\n\n\n\n\n\nfrom fredapi import Fred\nimport os\nfred = Fred(api_key=os.environ.get('fred_api'))\n\n\nsf_home = fred.get_series('SFXRSA')\nsf_home = sf_home.dropna()\n\n\nsf_home.plot()\n\n\n\n\n\n\n\n\n\nlog_prices = np.log(sf_home)\nlog_prices.plot()\n\n\n\n\n\n\n\n\n\nfrom statsmodels.tsa.seasonal import STL\n# -- STL DECOMPOSITION --\nstl = STL(log_prices, period=36)\nres = stl.fit()\ntrend = res.trend\nresid = res.resid\n\n# === 1. Plot log prices with trend ===\nplt.figure()\nplt.plot(log_prices, label='Log(Index)', alpha=0.7)\nplt.plot(trend, label='STL Trend', linewidth=2)\nplt.title('Log-transformed SF Housing Index with STL Trend')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# === 2. Residual diagnostics ===\nplt.figure()\nplt.plot(resid, label='STL Residuals', color='orange')\nplt.axhline(0, linestyle='--', color='gray')\nplt.title('STL Residuals (Detrended Log Prices)')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom datetime import timedelta\nfuture_months = 120\nblock_size = 12\nn_simulations = 100\nN_years = 10\n\n\n# === 3. Fit linear model to trend (last N years) ===\ntrend_recent = trend.last(f'{N_years}Y')\nX = np.arange(len(trend_recent)).reshape(-1, 1)\ny = trend_recent.values\nmodel = LinearRegression().fit(X, y)\nX_future = np.arange(len(X), len(X) + future_months).reshape(-1, 1)\ntrend_forecast = model.predict(X_future)\n\nC:\\Users\\peteramerkhanian\\AppData\\Local\\Temp\\ipykernel_66512\\1996583068.py:2: FutureWarning: last is deprecated and will be removed in a future version. Please create a mask and filter using `.loc` instead\n  trend_recent = trend.last(f'{N_years}Y')\nC:\\Users\\peteramerkhanian\\AppData\\Local\\Temp\\ipykernel_66512\\1996583068.py:2: FutureWarning: 'Y' is deprecated and will be removed in a future version, please use 'YE' instead.\n  trend_recent = trend.last(f'{N_years}Y')\n\n\n\n# Plot extrapolated trend\nplt.figure()\nplt.plot(trend, label='Observed Trend')\nplt.plot(\n    pd.date_range(trend.index[-1] + timedelta(days=30), periods=future_months, freq='MS'),\n    trend_forecast,\n    color='red', label='Forecasted Trend'\n)\nplt.title('STL Trend + Linear Extrapolation')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# === 4. Show example of block bootstrapped residuals ===\ndef block_bootstrap(residuals, n_periods, block_size=4):\n    blocks = []\n    n_blocks = int(np.ceil(n_periods / block_size))\n    for _ in range(n_blocks):\n        start = np.random.randint(0, len(residuals) - block_size)\n        blocks.extend(residuals[start:start + block_size])\n    return np.array(blocks[:n_periods])\n\nsample_boot = block_bootstrap(resid.dropna(), future_months, block_size)\n\n\nplt.figure()\nplt.plot(sample_boot, color='purple')\nplt.axhline(0, linestyle='--', color='gray')\nplt.title('Example: Block Bootstrapped Residuals (Simulated Noise)')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Plot extrapolated trend\nplt.figure()\nplt.plot(trend, label='Observed Trend')\nplt.plot(\n    pd.date_range(trend.index[-1] + timedelta(days=30), periods=future_months, freq='MS'),\n    (trend_forecast + sample_boot),\n    color='red', label='Forecasted Trend'\n)\nplt.title('STL Trend + Linear Extrapolation')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.regime_switching.markov_regression import MarkovRegression\n\n# Step 1: Convert log prices to returns\n # insert your data here\nreturns = np.diff(log_prices)  # log returns\n\n# Step 2: Fit Markov Switching model (e.g., 2 regimes, AR(1) in each)\nmodel = MarkovRegression(returns, k_regimes=2, trend='c', switching_variance=True)\nresult = model.fit()\n\n# Step 3: Summary and regime probabilities\nprint(result.summary())\n\n# Smoothed regime probabilities\nsmoothed_probs = result.smoothed_marginal_probabilities\nplt.plot(smoothed_probs)\nplt.show()\n\n                        Markov Switching Model Results                        \n==============================================================================\nDep. Variable:                      y   No. Observations:                  458\nModel:               MarkovRegression   Log Likelihood                1485.238\nDate:                Sat, 31 May 2025   AIC                          -2958.477\nTime:                        12:27:11   BIC                          -2933.715\nSample:                             0   HQIC                         -2948.724\n                                - 458                                         \nCovariance Type:               approx                                         \n                             Regime 0 parameters                              \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0009      0.001      1.472      0.141      -0.000       0.002\nsigma2      2.722e-05   3.31e-06      8.213      0.000    2.07e-05    3.37e-05\n                             Regime 1 parameters                              \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0082      0.001      7.744      0.000       0.006       0.010\nsigma2         0.0002   2.52e-05      8.680      0.000       0.000       0.000\n                         Regime transition parameters                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\np[0-&gt;0]        0.9607      0.014     68.720      0.000       0.933       0.988\np[1-&gt;0]        0.0410      0.016      2.640      0.008       0.011       0.071\n==============================================================================\n\nWarnings:\n[1] Covariance matrix calculated using numerical (complex-step) differentiation.\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n\n# 1. Define split point\nsplit_date = '2013-01-01'\nlog_prices_train = log_prices[:split_date]\nlog_prices_test = log_prices[split_date:]\nn_test = len(log_prices_test)\n\n# 2. STL on training data\nstl_train = STL(log_prices_train, period=12).fit()\ntrend_train = stl_train.trend\nresid_train = stl_train.resid.dropna()\n\n# Extrapolate trend linearly from last N years of training trend\ntrend_recent = trend_train.last('10Y')\nX = np.arange(len(trend_recent)).reshape(-1, 1)\ny = trend_recent.values\nmodel = LinearRegression().fit(X, y)\nX_future = np.arange(len(X), len(X) + n_test).reshape(-1, 1)\ntrend_forecast = model.predict(X_future)\n\n# Bootstrap residuals and add to trend forecast\nsim_resid_stl = block_bootstrap(resid_train, n_test, block_size)\nsim_log_stl = trend_forecast + sim_resid_stl\nsim_prices_stl = np.exp(sim_log_stl)\n\n# 3. Fit normal distribution to training log returns\nlog_returns_train = log_prices_train.diff().dropna()\nmu, sigma = log_returns_train.mean(), log_returns_train.std()\n\n# Simulate log returns and add to final value of training log prices\nsim_log_returns_normal = np.random.normal(mu, sigma, size=n_test)\nsim_log_normal = [log_prices_train.iloc[-1]]\nfor r in sim_log_returns_normal:\n    sim_log_normal.append(sim_log_normal[-1] + r)\nsim_log_normal = sim_log_normal[1:]\nsim_prices_normal = np.exp(sim_log_normal)\n\n# 4. True test log prices\ntrue_log_prices = log_prices_test[:n_test]\n\n# 5. Evaluation metrics\ndef evaluate(pred, truth, label):\n    r2 = r2_score(truth, pred)\n    rmse = mean_squared_error(truth, pred, squared=False)\n    mae = mean_absolute_error(truth, pred)\n    print(f\"📊 {label} Forecast Performance\")\n    print(f\"R²:   {r2:.4f}\")\n    print(f\"RMSE: {rmse:.4f}\")\n    print(f\"MAE:  {mae:.4f}\\n\")\n\nevaluate(sim_log_stl, true_log_prices, \"STL + Bootstrap\")\nevaluate(sim_log_normal, true_log_prices, \"Normal Sim\")\n\n# 6. Plot for visual comparison\nplt.figure(figsize=(12, 5))\nplt.plot(log_prices, label='True Log Prices')\nplt.plot(log_prices_test.index, sim_log_stl, label='STL + Bootstrap Forecast', color='red')\nplt.plot(log_prices_test.index, sim_log_normal, label='Normal Sim Forecast', color='green')\nplt.title('Comparison of Forecast Methods (Log Prices)')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\nC:\\Users\\peteramerkhanian\\AppData\\Local\\Temp\\ipykernel_66512\\2205870326.py:15: FutureWarning: last is deprecated and will be removed in a future version. Please create a mask and filter using `.loc` instead\n  trend_recent = trend_train.last('10Y')\nC:\\Users\\peteramerkhanian\\AppData\\Local\\Temp\\ipykernel_66512\\2205870326.py:15: FutureWarning: 'Y' is deprecated and will be removed in a future version, please use 'YE' instead.\n  trend_recent = trend_train.last('10Y')\n\n\n📊 STL + Bootstrap Forecast Performance\nR²:   -22.5005\nRMSE: 1.1821\nMAE:  1.0954\n\n📊 Normal Sim Forecast Performance\nR²:   -1.2241\nRMSE: 0.3636\nMAE:  0.3324\n\n\n\n\n\n\n\n\n\n\n\nfrom statsmodels.tsa.regime_switching.markov_regression import MarkovRegression\n\n# Markov switching model with 2 regimes: mean & variance change\nmodel = MarkovRegression(log_prices_train, k_regimes=2, trend='c', switching_variance=False)\nres = model.fit()\n\n# Forecast future values (in-sample first)\nfitted = res.fittedvalues\n\n# Plot in-sample fit\nplt.figure(figsize=(12, 4))\nplt.plot(log_prices_train, label='True Log Prices')\nplt.plot(fitted, label='Fitted (Markov)', linestyle='--')\nplt.title(\"Markov Switching Model Fit\")\nplt.legend()\nplt.show()\n\n# Show smoothed regime probabilities\nres.smoothed_marginal_probabilities[0].plot(title=\"Probability of Regime 0 (e.g. Boom)\")\n\nc:\\Users\\peteramerkhanian\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n  self._init_dates(dates, freq)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport yfinance as yf\nfrom fredapi import Fred\nimport os\n\n\n# Initialize FRED API (you need to get your own API key from https://fred.stlouisfed.org/)\nfred = Fred(api_key=os.environ.get('fred_api'))\n\n# Download S&P 500 data\nsp500 = yf.download('^GSPC', start='2000-01-01', interval='1mo')['Adj Close']\nsp500 = sp500.dropna()\n\n# Download San Francisco Home Price Index data\nsf_home = fred.get_series('SFXRSA')\nsf_home = sf_home.dropna()\n\n# Align dates\ndata = pd.DataFrame({'SP500': sp500, 'SF_Home': sf_home})\ndata = data.dropna()\n\n# Calculate monthly returns\nreturns = data.pct_change().dropna()\n\n\n# Set a random seed for stability of results\nnp.random.seed(10)\n\nmean = .0572\nstandard_deviation = .1042\nsamples = 1\n\n# Draw the sample\nnp.random.normal(mean, standard_deviation, samples)\n\narray([0.19595131])\n\n\nWe now simulate market returns for every month by supplying mean and standard deviation values for both home and stock market appreciation and drawing 360 samples (360 months in 30 years). For simplicity, we’ll just use world-wide aggregate values from “The Rate of Return on Everything, 1870-2015”.\n\nmu_stock = .1081\nsigma_stock = .2267\n\nmu_home = .0572\nsigma_home = .1042\n\nGiven that stock and home appreciation is probably correlated, I’ll have ti sample from a bivariate normal distribution using numpy.random.Generator.multivariate_normal - documentation here, rather than the univariate distribution draw shown above. I am going to assume a correlation coefficient, \\(\\rho_{stock,home}\\) of 0.5 - a fairly clear correlation.\nIn order to use that numpy function, I’ll need to translate my correlation statistic into a covariance statistic, and I’ll use the following formula (source):\n\\[ \\begin{align*}\ncov_{stock,home} &= \\rho_{stock,home} \\times \\sigma_{stock} \\sigma_{home} \\\\\\\ncov_{stock,home} &= 0.5 \\times .2267 \\times .1042 \\end{align*}\n\\]\nI calculate covariance and confirm that the covariance and correlations match up below:\n\ncov = 0.5 * sigma_stock * sigma_home\nprint(\"Covariance:\", cov)\nprint(\"Back to correlation:\", cov / (sigma_stock * sigma_home))\n\nCovariance: 0.01181107\nBack to correlation: 0.5\n\n\nNow that I have the covariance, I’ll be able to sample from a bivariate normal distribution of the form shown below (source).\n\\[\n\\begin{pmatrix} Stock \\\\\\\\ Home\\end{pmatrix} \\sim \\mathcal{N} \\left[ \\begin{pmatrix} \\mu_{s} \\\\\\ \\mu_{h}\\end{pmatrix}, \\begin{pmatrix} \\sigma_{s}^2 & cov_{s,h} \\\\\\ cov_{s,h} & \\sigma_{h}^2\\end{pmatrix} \\right]\n\\]\nNote, \\(s\\) is shorthand for stock and \\(h\\) is shorthand for home.\nNow I’ll code that in Python and confirm that the means and standard deviations of our samples match what we expect:\n\ncov_matrix = np.array([[sigma_stock**2, cov],\n              [cov, sigma_home**2]])\n\nreturns_df = pd.DataFrame(np.random\n                          .default_rng(30)\n                          .multivariate_normal([mu_stock, mu_home],\n                                               cov_matrix,\n                                               360),\n                          columns=[\"Stock_Appreciation\", \"Home_Appreciation\"])\nprint(\"Means:\", returns_df.mean(axis=0).values)\nprint(\"Std. Devs:\", returns_df.std(axis=0).values)\n\nreturns_df = (returns_df / 12)\n\nMeans: [0.10764063 0.05970695]\nStd. Devs: [0.22544095 0.10543034]\n\n\nPlotting the simulated values, we can see that stock market returns are typically higher than home value appreciation.\n\nreturns_df.add(1).cumprod().plot(figsize=(7,4))\nplt.xlabel(\"Months\")\nplt.ylabel(\"Money Multiplier\")\nplt.title(\"Simulated Home/Stock Returns\")\nsns.despine();\n\n\n\n\n\n\n\n\n\nhome_performance = returns_df.add(1).cumprod()['Home_Appreciation']\nstock_performance = returns_df.add(1).cumprod()['Stock_Appreciation']\n\nNow we can define two spread-sheet-like dataframes: - one that shows a mortgage amortization schedule for if you were to buy the $600,000 home, along with the home’s appreciation over time. - one that shows a table of rent payments and the stock market growth of what would have been your down payment (you can invest the down payment since you didn’t end up purchasing a house)."
  },
  {
    "objectID": "posts/rent-vs-buy/index.html#scenarios",
    "href": "posts/rent-vs-buy/index.html#scenarios",
    "title": "Buy vs. Rent, A Financial Modeling Workflow in Python",
    "section": "Scenarios",
    "text": "Scenarios\n\nOwnership Table\n\n# Buying Table\ndf_own = pd.DataFrame()\ndf_own[\"Period\"] =  pd.Series(range(12*30)) + 1\ndf_own[\"Date\"] = pd.date_range(start=datetime.today(),\n                           periods=12*30,\n                           freq='MS',\n                           name=\"Date\")\ndf_own[\"Principal_Paid\"] = npf.ppmt(interest_rate/12,\n                                    df_own[\"Period\"],\n                                    12*30,\n                                    principal)\ndf_own[\"Interest_Paid\"] = npf.ipmt(interest_rate/12,\n                                   df_own[\"Period\"],\n                                   12*30,\n                                   principal)\ndf_own[\"HOA_Paid\"] = hoa\ndf_own[\"HOA_Paid\"] = df_own[\"HOA_Paid\"].cumsum()\ndf_own[\"Balance_Remaining\"] = principal + df_own[\"Principal_Paid\"].cumsum()\ndf_own[\"Home_Value\"] = round(cost * home_performance, 2)\ndf_own[\"PropTax_Paid\"] = (df_own[\"Period\"]\n                          .apply(lambda x:\n                                 (cost * 1.02**((x-1)/12) * 0.01)\n                                 if (x-1) in list(range(0, 12*1000, 12))\n                                 else 0)\n                          .cumsum())\ndf_own[\"Sale_Fees\"] = df_own[\"Home_Value\"] * .07\ndf_own[\"Own_Profit\"] = (df_own[\"Home_Value\"] -\n                              df_own[\"HOA_Paid\"] -\n                              df_own[\"Balance_Remaining\"] -\n                              (buying_fees + df_own[\"Sale_Fees\"]) -\n                              df_own[\"PropTax_Paid\"])\ndf_own = round(df_own, 2)\n\nNote this code, which is a bit of a monster:\n\ndf_own[\"PropTax_Paid\"] = (df_own[\"Period\"]\n                          .apply(lambda x:\n                                 (cost * 1.02**((x-1)/12) * 0.01)\n                                 if (x-1) in list(range(0, 12*1000, 12))\n                                 else 0)\n                          .cumsum())\n\nWhat is happening here is a calculation of property assessed value and property tax according to California’s property assessment/tax regime (more here). We’ll look at this in two pieces, first, assessed value. In California, once you purchase a property, its assessed value is set at the purchase price, then increases annually by the lower of 2% or the rate of inflation according to the California Consumer Price Index (CCPI). You could write out an equation for this as follows:\n\\[\n\\begin{align*}\nAnnualFactor_y =\n\\begin{cases}\n        1 + CCPI_y, & \\text{if } CCPI_y &lt; 0.02 \\\\\\\n        1.02, & \\text{otherwise}\n\\end{cases}\n\\end{align*}\n\\]\n\\(AnnualFactor\\) is the amount that the assessed value of a home will appreciate (expressed as a multiplier) in a given year, \\(y\\). We define \\(y^*\\) as the year of initial purchase and \\(PurchasePrice\\) as the amount that the home was purchased for. Given that, \\(AssessedValue\\) is defined as follows:\n\\[ \\begin{align*}\nAssessedValue_y =\n    \\begin{cases}\n        PurchasePrice, & \\text{if } y = y^* \\\\\n        AssessedValue_{y-1} \\times AnnualFactor_y, & \\text{otherwise }\n    \\end{cases}\n\\end{align*}\n\\]\nIn our code, we will simplify this calculation by excluding the CCPI and just always using 1.02 as our annual factor. Therefore, we get:\n\\[\n  AssessedValue_y = PurchasePrice \\times 1.02^y\n\\]\nand once we factor in taxes (1%), we get:\n\\[\n  PropertyTax_y = 0.01(PurchasePrice \\times 1.02^y)\n\\]\nand finally we look at the the cumulative total property tax you’ve paid in a given year \\(y\\), which is df_own[\"PropTax_Paid\"]:\n\\[\n\\begin{equation*}\n  PropertyTaxPaid_y = \\sum_{y=1}^{30} 0.01(PurchasePrice \\times 1.02^y)\n\\end{equation*}\n\\]\nThere’s some elements added to the code to work between years and months, but that equation is the gist of it.\nWe end up with the following table for property ownership:\n\ndf_own\n\n\n\n\n\n\n\n\nPeriod\nDate\nPrincipal_Paid\nInterest_Paid\nHOA_Paid\nBalance_Remaining\nHome_Value\nPropTax_Paid\nSale_Fees\nOwn_Profit\n\n\n\n\n0\n1\n2025-06-01 08:48:43.587964\n-506.25\n-3033.33\n700\n559493.75\n701405.73\n7000.000000\n49098.40\n62713.58\n\n\n1\n2\n2025-07-01 08:48:43.587964\n-508.99\n-3030.59\n1400\n558984.76\n707155.41\n7000.000000\n49500.88\n67869.77\n\n\n2\n3\n2025-08-01 08:48:43.587964\n-511.75\n-3027.83\n2100\n558473.02\n723324.03\n7000.000000\n50632.68\n82718.33\n\n\n3\n4\n2025-09-01 08:48:43.587964\n-514.52\n-3025.06\n2800\n557958.50\n737245.96\n7000.000000\n51607.22\n95480.25\n\n\n4\n5\n2025-10-01 08:48:43.587964\n-517.31\n-3022.28\n3500\n557441.19\n745606.08\n7000.000000\n52192.43\n103072.46\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n355\n356\n2055-01-01 08:48:43.587964\n-3445.26\n-94.33\n249200\n13968.65\n4026844.31\n283976.554436\n281879.10\n3175420.00\n\n\n356\n357\n2055-02-01 08:48:43.587964\n-3463.92\n-75.66\n249900\n10504.74\n4044808.10\n283976.554436\n283136.57\n3194890.24\n\n\n357\n358\n2055-03-01 08:48:43.587964\n-3482.68\n-56.90\n250600\n7022.06\n4066823.89\n283976.554436\n284677.67\n3218147.61\n\n\n358\n359\n2055-04-01 08:48:43.587964\n-3501.54\n-38.04\n251300\n3520.51\n4096809.97\n283976.554436\n286776.70\n3248836.21\n\n\n359\n360\n2055-05-01 08:48:43.587964\n-3520.51\n-19.07\n252000\n-0.00\n4122136.18\n283976.554436\n288549.53\n3275210.09\n\n\n\n\n360 rows × 10 columns\n\n\n\n\n\nRental Table\nThis one is a but more simple, only examining the total rent you’ve paid in a given month and simulated stock returns at that point.\n\n# Rental Table\ndf_rent = pd.DataFrame()\ndf_rent[\"Period\"] =  pd.Series(range(12*30)) + 1\ndf_rent[\"Date\"] = pd.date_range(start=datetime.today(),\n                           periods=12*30,\n                           freq='MS',\n                           name=\"Date\")\ndf_rent[\"DownPayment_Invested\"] =  stock_performance * down_payment\ndf_rent[\"Rent_Paid\"] = rent * 1.02**(df_rent[\"Period\"].add(1) % 12 == 0).cumsum()\nmonthly_savings = df_own[[\"Principal_Paid\", \"Interest_Paid\"]].sum(axis=1).multiply(-1).add(hoa) - df_rent[\"Rent_Paid\"]\ndf_rent[\"Total_Rent_Paid\"] = df_rent[\"Rent_Paid\"].cumsum()\ndf_rent[\"Rent_Profit\"] = df_rent[\"DownPayment_Invested\"] - df_rent[\"Total_Rent_Paid\"] + monthly_savings.cumsum()\ndf_rent = round(df_rent, 2)"
  },
  {
    "objectID": "posts/rent-vs-buy/index.html#results",
    "href": "posts/rent-vs-buy/index.html#results",
    "title": "Buy vs. Rent, A Financial Modeling Workflow in Python",
    "section": "Results",
    "text": "Results\nAt this point, I’ll merge the ownership and rental tables and plot out what happened in this simulation\n\nmerged = pd.merge(df_own, df_rent, on=\"Period\")\nmerged = merged.melt(value_vars = [\"Rent_Profit\", \"Own_Profit\"], id_vars='Period')\n\n\nplt.figure(figsize=(14, 6))\nplt.title(\"Wealth Outcomes for Owning vs. Renting a 2b1br Apt\")\nsns.lineplot(data=merged, x=\"Period\", y=\"value\", hue=\"variable\")\n# for x in range(0, 350, 12):\n#     if x == 0:\n#         plt.axvline(x, color=\"grey\", linestyle=\":\", alpha=1, label=\"Year\")\n#     else:\n#         plt.axvline(x, color=\"grey\", linestyle=\":\", alpha=0.7)\n#     plt.text(x+1, -100000, str(int(x/12)), alpha=0.8)\nplt.axhline(0, color=\"red\", linestyle=\"--\", alpha=0.5, label=\"Zero\")\nplt.legend()\nsns.despine()\n\n\n\n\n\n\n\n\nWe can quickly see that ownership will clearly build more wealth in the medium and long run:\n\nyears = 5\nprint(f\"Owner after {years} years:\", df_own.loc[12*years-1, \"Own_Profit\"])\nprint(f\"Renter after {years} years:\", df_rent.loc[12*years-1, \"Rent_Profit\"])\n\nOwner after 5 years: 243148.71\nRenter after 5 years: 135926.53\n\n\nHowever, we can see that, in the unlikely case that the home is sold within the first year or so, the wealth of the renter and the owner are very similar, likely due to the owner contending with buying/selling fees:\n\nyears = 1\nprint(f\"Owner after {years} years:\", df_own.loc[12*years-1, \"Own_Profit\"])\nprint(f\"Renter after {years} years:\", df_rent.loc[12*years-1, \"Rent_Profit\"])\n\nOwner after 1 years: 115891.0\nRenter after 1 years: 135331.15\n\n\nA possible takeaway here is that, as long as you can be confident you’ll be able to hold onto the house for more than a year, it’s probably better to purchase it. Uncertainty estimates would be useful here, and could be obtained by running the simulation under a wide variety of randomly generated market conditions."
  },
  {
    "objectID": "posts/newtons-method/index.html",
    "href": "posts/newtons-method/index.html",
    "title": "Newton’s Method From The Ground Up",
    "section": "",
    "text": "Code\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport sympy\nimport imageio\nfrom typing import Callable\nimport seaborn as sns"
  },
  {
    "objectID": "posts/newtons-method/index.html#newtons-method-for-finding-roots-what-how-and-why",
    "href": "posts/newtons-method/index.html#newtons-method-for-finding-roots-what-how-and-why",
    "title": "Newton’s Method From The Ground Up",
    "section": "Newton’s Method for Finding Roots: What, How, and Why",
    "text": "Newton’s Method for Finding Roots: What, How, and Why\nIn the context of a Differential Calculus course, Newton’s Method, also referred to as The Newton-Raphson Method, seems to typically come up near the end of the semester, offering a brief look into the world of “numerical methods” and how we might solve complex problems in the real world. I think that it’s a cool topic and I wanted to write an extended blog post about it. The main purpose of this is to ensure that I always have personal reference materials for Newton’s Method, but perhaps it can be helpful to other readers.\nI draw on two key sources for thinking about Newton’s Method:\n\n“Newton’s Method.” 2023. In Wikipedia\nStrang, Gilbert, and Edwin Herman. 2016. Calculus Volume 1. OpenStax College.\n\n\nWhat is it\n\nIn many areas of pure and applied mathematics, we are interested in finding solutions to an equation of the form \\(f(x)=0\\)\n\n(Strang and Herman 2016)\nNewton’s Method is a numerical method that helps us solve \\(f(x)=0\\).\nThere are many cases where we need to solve equations like that, but the application area I work in involves statistical modeling, so I jump to the case where we want to “fit” a line as close as possible to some set of data points, thus creating a model of a data-generating-process. The fitting process rests on getting the line as close to the data points as possible, thus minimizing error. If we can formulate error as a function, then we can minimize it – we differentiate it and set the derivative to 0, and solve \\(f'(x)=0\\). Therein lies the opportunity to apply Newton’s Method to a real problem.\nHowever, we’ll take a step back from statistics and return to the domain of an introductory calculus course. Newton’s Method is useful for finding the root – the x-intercept – of a function. We’ll explore the method by walking through an example\n\n\nUsing Newton’s Method to Solve a Simple Problem\nSay we are given the function \\(4\\sin(x)\\) and we want to find the x-intercept of the function with the domain \\(-5\\leq x \\leq -1\\).\nWe’ll first set this function up in sympy, a python library for symbolic computation.\n\nx, y = sympy.symbols('x y')\ny = 4*sympy.sin(x)\ny\n\n\\(\\displaystyle 4 \\sin{\\left(x \\right)}\\)\n\n\nThis is a problem with a known answer (you can google x-intercepts of \\(\\sin(x)\\) for a table), so it’s not particularly useful to use Newton’s Method here, but for our purposes it will be helpful that we can check our answer with the “right” one. \\[\n\\begin{align*}\n4 \\sin(x) &= 0 \\quad \\text{where} \\quad -5\\leq x \\leq -1 \\\\\n\\sin(x) &= 0 \\\\\nx &= \\sin^{-1}(0) \\\\\nx &= -\\pi\n\\end{align*}\n\\]\n\n\nCode\ndef plot_f(\n        f: Callable[[float], float],\n        x_low: int,\n        x_high: int,\n        ax: matplotlib.axes.Axes) -&gt; None:\n    \"\"\"\n    Plots a given function f within a specified range on a provided axes.\n\n    Parameters:\n        f (Callable[[float], float]): The function to be plotted.\n        x_low (int): The lower bound of the x-axis.\n        x_high (int): The upper bound of the x-axis.\n        ax (matplotlib.axes.Axes): The matplotlib axes object on which the function will be plotted.\n\n    Returns:\n        None\n    \"\"\"\n    x_vec = np.linspace(x_low, x_high, 100)\n    ax.plot(x_vec, f(x_vec))\n\n\ndef base_plot(\n        y: sympy.core.mul.Mul,\n        x: sympy.core.mul.Mul,\n        x_low: int = -5,\n        x_high: int = 5) -&gt; None:\n    \"\"\"\n    Creates a base plot for a mathematical expression and its graph.\n\n    Parameters:\n        y (sympy.core.mul.Mul): The mathematical expression to be plotted.\n        x (sympy.core.mul.Mul): The symbol representing the independent variable in the expression.\n        x_low (int): The lower bound of the x-axis (default is -5).\n        x_high (int): The upper bound of the x-axis (default is 5).\n\n    Returns:\n        tuple: A tuple containing the matplotlib figure and axes used for plotting.\n\n    Note:\n        The mathematical expression is first converted to a Python function using sympy.lambdify.\n    The function is then plotted on the specified axes along with gridlines and labels.\n    \"\"\"\n    f = sympy.lambdify(x, y)\n    fig, ax = plt.subplots()\n    ax.grid(alpha=.5)\n    ax.axhline(0, color=\"black\", alpha=.5)\n    plot_f(f, x_low, x_high, ax)\n    ax.set(title=f\"$f(x)={sympy.latex(y)}$\", xlabel=\"$x$\", ylabel=\"$f(x)$\")\n    return fig, ax\n\n\ndef plot_truth(ax: matplotlib.axes.Axes) -&gt; None:\n    \"\"\"\n    Plots the true root of a function as a marker on the graph.\n\n    Parameters:\n        ax (matplotlib.axes.Axes): The matplotlib axes on which the marker will be plotted.\n\n    Returns:\n        None\n    \"\"\"\n    ax.plot(-np.pi,\n            0,\n            \"*\",\n            markersize=15,\n            color=\"darkblue\",\n            label=\"True root, $-\\pi$\")\n\n\ndef plot_guess(\n        ax: matplotlib.axes.Axes,\n        guess: int,\n        label: str) -&gt; None:\n    \"\"\"\n    Plots a guess or estimate as a marker on the graph.\n\n    Parameters:\n        ax (matplotlib.axes.Axes): The matplotlib axes on which the marker will be plotted.\n        guess (int): The estimated value to be marked on the graph.\n        label (str): Label for the marker.\n\n    Returns:\n        None\n    \"\"\"\n    ax.plot(guess,\n            0,\n            \"o\",\n            label=label)\n\n\ndef plot_guess_coords(\n        ax: matplotlib.axes.Axes,\n        guess: int,\n        label: str,\n        y: sympy.core.mul.Mul = y,\n        x: sympy.core.mul.Mul = x):\n    \"\"\"\n    Plots a guess or estimate with specific coordinates as a marker on the graph.\n\n    Parameters:\n        ax (matplotlib.axes.Axes): The matplotlib axes on which the marker will be plotted.\n        guess (int): The estimated x-coordinate where the marker will be placed.\n        label (str): Label for the marker.\n        y (sympy.core.mul.Mul): The mathematical expression corresponding to the y-coordinate.\n        x (sympy.core.mul.Mul): The symbol representing the independent variable (x).\n\n    Returns:\n        None\n    \"\"\"\n    ax.plot(guess,\n            y.subs(x, guess).evalf(),\n            \"s\",\n            label=label, color=\"black\")\n\n\ndef euclidean_dist_to_truth(x): return np.sqrt((-np.pi - float(x))**2)\n\n\nNow we’ll begin the process of using Newton’s Method to arrive at that same answer of \\(x=-\\pi\\).\n\nStep 1: Make a first guess and evaluate its (x, y) coordinates\nFor a first guess, it’s typical to start close to 0. In our case, we’ll try -2.\n\nx_0 = -2\n\nLet’s get a sense of how good of a guess this is by plotting it\n\\[\n\\begin{align*}\nf(x) &= 4 \\sin \\left( x \\right) \\\\\\\nf(x_0) &= 4 \\sin \\left( -2 \\right) \\\\\\\nf(x_0) &\\approx −3.6371897 \\\\\\\n(x_0, y_0) &= (x_0, f(x_0)) \\approx (-2, −3.6371897)\n\\end{align*}\n\\]\n\nfig, ax = base_plot(y, x, x_low=-5, x_high=-1)\nplot_truth(ax)\nplot_guess(ax, x_0, \"$x_0$ (initial guess)\")\nplot_guess_coords(ax, x_0, \"$(x_0, f(x_0))$\")\nax.legend();\n\n\n\n\n\n\n\n\nWe can assess the quality of that guess by calculating the distance from the guess to the right answer (\\(-\\pi\\)):\n\nprint(f\"error for guess x_0:\", euclidean_dist_to_truth(x_0))\n\nerror for guess x_0: 1.1415926535897931\n\n\n\n\nStep 2: Find the equation of the tangent line at those coordinates\nWe will proceed to improve upon that initial guess by computing the linear approximation of the function at that point, then retrieve its root to make a next-guess. Our first guess wasn’t based on any relevant information other than the domain of our search (between -5 and 0). Our next guess is going to be better-informed, as it is an estimate based on an approximation of the function.\n(Note that the linear approximation at this point is typically referred to as a tangent line, and I’ll use those two phrases interchangeably.)\nWe compute the tangent by first differentiating the function and plugging in our previous guess. This yields the slope of the tangent line: \\[\n\\begin{align*}\nf(x) &= 4 \\sin \\left( x \\right) \\\\\\\nf'(x) &= 4 \\cos \\left( x \\right) \\\\\\\nf'(x_0) &= 4 \\cos \\left( -2 \\right) \\\\\\\nf'(x_0) &\\approx −1.6645873\n\\end{align*}\n\\]\nI’ll note that sympy is capable of doing all of these routine calculations as well:\n\nprint(\"   f(x) = \", y)\nprint(\"  f'(x) = \", y.diff())\nprint(\"f'(x_0) = \", y.diff().subs(x, -2))\nprint(\"[note: sympy converted x_0=-2 to x_0=2 because cos(-x)=cos(x)]\")\nprint(\"f'(x_0) = \", y.diff().subs(x, x_0).evalf())\n\n   f(x) =  4*sin(x)\n  f'(x) =  4*cos(x)\nf'(x_0) =  4*cos(2)\n[note: sympy converted x_0=-2 to x_0=2 because cos(-x)=cos(x)]\nf'(x_0) =  -1.66458734618857\n\n\nNow, given each of these terms:\n\n\n\nTerm (math)\nValue\n\n\n\n\n\\(x_0\\) (X, Last Guess)\n\\(= -2\\)\n\n\n\\(f(x_0)\\) (Y at \\(x_0\\))\n\\(\\approx −3.63719\\)\n\n\n\\(f'(x_0)\\) (Slope of tangent at \\(x_0\\))\n\\(\\approx −1.66459\\)\n\n\n\nWe can proceed to find the full equation of the tangent line by writing out the point-slope form of a linear equation with slope \\(m=f'(x_0)\\). \\[\n\\begin{align*}\n(y - y_0) &= m(x - x_0) \\\\\n(y - f(x_0)) &= f'(x_0)(x - x_0) \\\\\ny &= f'(x_0)(x - x_0) + f(x_0)\n\\end{align*}\n\\] Plugging in our values, we get:\n\\[\n\\begin{align*}\ny &\\approx −1.66459x + (1.66459)(-2) - 3.63719 \\\\\ny &\\approx −1.66459x - 6.966364 \\\\\n\\end{align*}\n\\]\nWe’ll save that into a python function and plot it to make sure it does look like the tangent.\n\ndef f_1(x): return -1.6645873*x - 6.9663643\n\n\nfig, ax = base_plot(y, x, x_low=-5, x_high=-1)\nplot_truth(ax)\n\nplot_f(f_1, x_low=-5, x_high=-1, ax=ax)\n\nplot_guess(ax, x_0, \"$x_0$ (initial guess)\")\nplot_guess_coords(ax, x_0, \"$(x_0, f(x_0))$\")\n\nax.legend();\n\n\n\n\n\n\n\n\n\n\nStep 3: Find the x-intercept of the tangent line\nGiven that the tangent line is the best linear approximation of the original function, we can use its x-intercept as an approximation of the x-intercept of the original function. Thus, the root of the tangent line is the new “best-guess” of the original function’s root.\n\\[\n\\begin{align*}\n0 &\\approx −1.6645873x_1 - 6.9663643 \\\\\n\\frac{6.9663643}{−1.6645873} &\\approx x_1 \\\\\nx_1 &\\approx -4.1850\n\\end{align*}\n\\]\n\nx_1 = -4.1850\n\n\nfig, ax = base_plot(y, x, x_low=-5, x_high=-1)\nplot_truth(ax)\nplot_guess(ax, x_0, \"$x_0$ (initial guess)\")\nplot_f(f_1, x_low=-5, x_high=-1, ax=ax)\nplot_guess(ax, x_1, \"$x_1$ (next guess)\")\nax.legend();\n\n\n\n\n\n\n\n\nWhile this guess still isn’t particularly great, we can see that we have actually reduced the “error” of our guess:\n\nfor i, x_n in enumerate([x_0, x_1]):\n    print(f\"error for guess x_{i}:\", euclidean_dist_to_truth(x_n))\n\nerror for guess x_0: 1.1415926535897931\nerror for guess x_1: 1.0434073464102065\n\n\nThe big reveal of Newton-Raphson is that this error will continue to shrink as we repeat steps 1-3.\n\n\nStep 4: Repeat\nWe will again find the tangent line at this new point, \\((x_1, f(x_1))\\). We could take the old tangent line equation, \\(y = f'(x_0)(x - x_0) + f(x_0)\\) and simply update all of those \\(x_0\\) to \\(x_1\\), but at this point it will benefit us to move towards a more general equation: \\[\ny = f'(x_n)(x - x_n) + f(x_n)\n\\] This allows us to generate the tangent line at any given guess, \\(x_n\\). The following code leverages sympy to write that equation as a python function.\n\ndef y_n(x_n): return (\n    y.diff().subs(x, x_n) * (x - x_n) +  # f_1(x_1)(x-x_1) +\n    y.subs(x, x_n)  # f(x_1)\n)\n\n\ny_n(x_1)\n\n\\(\\displaystyle - 2.01311525745113 x - 4.96839101072835\\)\n\n\nWe’ll also use sympy to easily solve for the new tangent line’s x-intercept, \\(x_2\\)\n\nx_2 = sympy.solve(y_n(x_1), x)[0]\nx_2\n\n\\(\\displaystyle -2.46801120419652\\)\n\n\n\nfig, ax = base_plot(y, x, x_low=-5, x_high=-1)\nplot_truth(ax)\nplot_f(sympy.lambdify(x, y_n(x_1)), x_low=-5, x_high=-1, ax=ax)\n\nplot_guess_coords(ax, x_1, \"$(x_1, f(x_1))$\")\n\nplot_guess(ax, x_0, \"$x_0$ (initial guess)\")\nplot_guess(ax, x_1, \"$x_1$ (previous guess)\")\nplot_guess(ax, x_2, \"$x_2$ (next guess)\")\n\nax.legend();\n\n\n\n\n\n\n\n\nWe can verify that this new guess again reduces our “error,” which should encourage us to continue this process.\n\nfor i, x_n in enumerate([x_0, x_1, x_2]):\n    print(f\"error for guess x_{i}:\", euclidean_dist_to_truth(x_n))\n\nerror for guess x_0: 1.1415926535897931\nerror for guess x_1: 1.0434073464102065\nerror for guess x_2: 0.6735814493932737\n\n\n\n\nStep 5: Generalize the procedure\n\nA.) Make the equation more direct\nThus far we have used the general equation \\(y = f'(x_n)(x - x_n) + f(x_n)\\), where \\(x_n\\) is our current guess, and we solve for \\(x\\) to define our next guess. Given that we solve the equation for \\(x\\), we can rewrite it as follows:\n\\[\n\\begin{align*}\n0 &= f'(x_n)(x - x_n) + f(x_n) \\\\\n0 &= f'(x_n)(x - x_n) + f(x_n) \\\\\n0 - f(x_n) &= f'(x_n)(x - x_n) \\\\\n-\\frac{f(x_n)}{f'(x_n)} &= x - x_n \\\\\nx &= x_n -\\frac{f(x_n)}{f'(x_n)} \\\\\n\\end{align*}\n\\] This expresses one step of Newton’s Method – solving for the x-intercept of the tangent line at the point \\((x_n, f(x_n))\\).\n\n\nB.) Move from equation to algorithm\nWe now build on this single step and express the general process of Newton’s method. To start, it’s more accurate to label the left hand side \\(x_{n+1}\\) given that it represents the next guess: \\[\nx_{n+1} = x_n -\\frac{f(x_n)}{f'(x_n)}\n\\]\nGiven this equation, we can think of Newton’s Method as essentially searching for good guesses – defining new \\(x_{n+1}\\) – until it’s right. But how do we define “right”? Put in other words, when do we stop?\nIn our case, we are working with a simple function and we know the correct answer – we can just stop once we get close to \\(-\\pi\\) – but in any real application that won’t be the case. In those cases, it is common practice to define “right” as when the guesses stop changing much with each iteration. Stated semi-formally, we wait until \\(|x_{n+1} - x_{n}|\\) gets small.\nFor example: if the last guess was -3.14159 and the new guess is -3.141592, the guess only changed by .0000002, and we might conclude that we’ve gotten as close to the answer as is necessary. In this case, we set a stopping condition – when the next guess is less than or equal to .0000002 away from the previous one, we stop. We can write out the stopping condition as follows:\n\\[\n\\begin{align*}\n|x_{n+1} - x_{n}| &\\leq 2\\times 10^{-7} \\\\\n|(x_n -\\frac{f(x_n)}{f'(x_n)}) - x_{n}| &\\leq 2\\times 10^{-7} \\\\\n|-\\frac{f(x_n)}{f'(x_n)}| &\\leq 2\\times 10^{-7} \\\\\n|\\frac{f(x_n)}{f'(x_n)}| &\\leq 2\\times 10^{-7}\n\\end{align*}\n\\]\nWe can try writing out the recursive algorithm as a piece-wise equation:\n\\[\n\\begin{align*}\n\\text{Let } x_0 := \\text{initial guess, } \\\\\n\\text{For all natural numbers } n \\ge 0, \\\\\n\\text{Define }  x_{n+1} \\text{ as:}\n\\end{align*}\n\\]\n\\[\nx_{n+1} = \\begin{cases}\n        x_n & \\text{if }\\quad |\\frac{f(x_n)}{f'(x_n)}| \\leq 2\\times 10^{-7} \\\\\n        x_n -\\frac{f(x_n)}{f'(x_n)} & \\text{Otherwise}\n        \\end{cases}\n\\]\nHowever, now that we are moving into the realm of algorithms, I think it’s clearer to write this as code:\n\nx_n = -2\n\n# We'll use this to count which guess we are on\ncounter = 1\nwhile True:\n    # Disregard the following utility code\n    print(f\"Guess {counter}:\",\n          str(round(float(x_n), 5)).ljust(8, '0'),\n          \" --- Error:\",\n          euclidean_dist_to_truth(x_n))\n    ################################################\n    # The following is the code for newton's method\n    # 1.) Check for the stopping condition,\n    # |f(x_n)/f'(x_n)| &lt; 2 * 10^-7\n    stop_condition = (\n        np.abs(sympy.lambdify(x, y)(x_n) /\n               sympy.lambdify(x, y.diff())(x_n))\n        &lt; 2e-7\n    )\n    if stop_condition:\n        print(f\"Converged in {counter} steps.\")\n        break\n    # 2.) If stopping condition not met, make a new guess\n    x_n = x_n - (\n        # f(x_n) /\n        sympy.lambdify(x, y)(x_n) /\n        # f'(x_n)\n        sympy.lambdify(x, y.diff())(x_n)\n    )\n    ################################################\n    # Update the counter\n    counter += 1\n\nGuess 1: -2.00000  --- Error: 1.1415926535897931\nGuess 2: -4.18504  --- Error: 1.0434472096717258\nGuess 3: -2.46789  --- Error: 0.6736989790751275\nGuess 4: -3.26619  --- Error: 0.1245936239793135\nGuess 5: -3.14094  --- Error: 0.00064874127215786\nGuess 6: -3.14159  --- Error: 9.101119857746198e-11\nConverged in 6 steps.\n\n\nWe’ve converged at our best-guess after six steps, which we can see animated below.\n\n\nCode\n%%capture\nx_n = -2\nmax_iter = 10\ntolerance = 1e-6\n\nfig, ax = base_plot(y, x, x_low=-5, x_high=-1)\nplot_truth(ax)\nax.set_ylim(-4, 4)\nax.set_xlim(-4.5, -1.5)\n\nimages = []\nbreaking_condition = False\nfor j in range(max_iter):\n    error = euclidean_dist_to_truth(x_n)\n    if error &lt; tolerance:\n        breaking_condition = True\n\n    ax.set_title(\n        f\"Iteration {j+1}\\nGuess: {round(x_n, 6)}\\n Error: {round(error, 8)}\")\n\n    # Plot the current guess\n    plot_guess(ax, x_n, \"$x_n$\")\n    filename = f'newton_iteration_{j}_0.png'\n    fig.savefig(filename)\n    images.append(imageio.imread(filename))\n\n    if breaking_condition:\n        ax.text(x_n, 2, s=f\"CONVERGED IN {j} STEPS\", size=15)\n        filename = f'newton_iteration_{i}_3.png'\n        fig.savefig(filename)\n        images.append(imageio.imread(filename))\n        ax.text(x_n, 2, s=f\"CONVERGED IN {j} STEPS\", size=15)\n        filename = f'newton_iteration_{i}_4.png'\n        fig.savefig(filename)\n        images.append(imageio.imread(filename))\n        break\n    # Plot the coordinates of the current guess\n    plot_guess_coords(ax, x_n, \"$(x_n, f(x_n))$\")\n    filename = f'newton_iteration_{j}_1.png'\n    fig.savefig(filename)\n    images.append(imageio.imread(filename))\n    # Plot the tangent line of that coordinate to inform next guess\n    plot_f(sympy.lambdify(x, y_n(x_n)), x_low=-5, x_high=-1, ax=ax)\n    filename = f'newton_iteration_{j}_2.png'\n    fig.savefig(filename)\n    images.append(imageio.imread(filename))\n\n    # Reset plot\n    ax.clear()\n    fig, ax = base_plot(y, x, x_low=-5, x_high=-1)\n    plot_truth(ax)\n    ax.set_ylim(-4, 4)\n    ax.set_xlim(-4.5, -1.5)\n\n    x_n -= (\n        sympy.lambdify(x, y)(x_n) /\n        sympy.lambdify(x, y.diff())(x_n)\n    )\nimageio.mimsave('newton_iterations.gif', images, duration=1)\n\n# Clean out pngs\nwd = os.getcwd()\nfiles = os.listdir(wd)\nfor item in files:\n    if item.endswith(\".png\"):\n        os.remove(os.path.join(wd, item))\n\n\nUsageError: Line magic function `%%capture` not found.\n\n\n\n\n\n\n\nUsing Newton’s Method to Solve a Real Problem\nIn the previous example, we dealt with a function, \\(f(x) = 4\\sin(x)\\) with a well known analytical solution for its x-intercept. Other simple functions can typically be solved with known formulas – e.g. a second degree polynomial’s roots can be found using the quadratic formula. In cases of known analytical solutions or readily available root-finding formulas, there is no reason to use Newton’s Method beyond as a learning exercise.\nHowever, many functions do not have have readily available methods for finding the root. For example, if \\(f(x)\\) is a polynomial of degree 5 or greater, it is known that no formula for finding its roots exist (Strang and Herman 2016). Consider the following polynomial of degree 5: \\[\nf(x) =x^{5} + 8 x^{4} + 4 x^{3} - 2 x - 7\n\\] What if we are asked to solve the following: \\[\nf(x)=0 \\quad \\text{where} -5\\leq x \\leq 0\n\\] There is no formula that solves this. Even plugging the polynomial into sympy and running its solver, sympy.solveset, doesn’t give a clear answer.\n\nx, y = sympy.symbols('x y')\ny = x**5 + 8*x**4 + 4*x**3 - 2*x-7\ny\n\n\\(\\displaystyle x^{5} + 8 x^{4} + 4 x^{3} - 2 x - 7\\)\n\n\n\nprint(sympy.solveset(y, x, sympy.Interval(-5, 0)))\n\n{CRootOf(x**5 + 8*x**4 + 4*x**3 - 2*x - 7, 1)}\n\n\n(where CRootOf is an indexed complex root of the polynomial – not an analytical solution)\nWe might proceed to visually inspect the function on this domain – but it’s even pretty hard to visually spot a root here!\n\nx_vec = np.linspace(-5, 0, 1000000)\nfig, ax = plt.subplots()\nax.grid(alpha=.5)\nax.plot(x_vec, sympy.lambdify(x, y)(x_vec))\nax.set_title(f\"$y={sympy.latex(y)}$\");\n\n\n\n\n\n\n\n\nHere’s a good use-case for Newton’s Method. I set up the algorithm with \\(x_0=0\\) and a stopping condition that \\(|x_{n+1} - x_{n}| \\leq 10^{-12}\\).\nI quickly converge at an answer:\n\nx_n = 0\ntolerance = 1e-12\ncounter = 1\n\nlast = np.inf\nwhile True:\n    # Disregard the following utility code\n    print(\"Guess {0:3}:\".format(counter),\n          str(round(float(x_n), 5)).ljust(8, '0'),\n          \" --- Change:\",\n          round(np.abs(float(x_n) - last), 8))\n    last = float(x_n)\n    ################################################\n    # The following is the code for newton's method\n    # 1.) Check for the stopping condition,\n    # |f(x_n)/f'(x_n)| &lt; 2 * 10^-7\n    stop_condition = (\n        np.abs(sympy.lambdify(x, y)(x_n) /\n               sympy.lambdify(x, y.diff())(x_n))\n        &lt; tolerance\n    )\n    if stop_condition:\n        print(f\"Converged in {counter} steps.\")\n        break\n    # 2.) If stopping condition not met, make a new guess\n    x_n = x_n - (\n        # f(x_n) /\n        sympy.lambdify(x, y)(x_n) /\n        # f'(x_n)\n        sympy.lambdify(x, y.diff())(x_n)\n    )\n    ################################################\n    counter += 1\n\nGuess   1: 0.000000  --- Change: inf\nGuess   2: -3.50000  --- Change: 3.5\nGuess   3: -2.44316  --- Change: 1.05683755\nGuess   4: -1.81481  --- Change: 0.6283498\nGuess   5: -1.41471  --- Change: 0.40010602\nGuess   6: -1.19062  --- Change: 0.22409096\nGuess   7: -1.11070  --- Change: 0.07991323\nGuess   8: -1.10108  --- Change: 0.0096197\nGuess   9: -1.10095  --- Change: 0.00012977\nGuess  10: -1.10095  --- Change: 2e-08\nConverged in 10 steps.\n\n\nWhen we plot our best guess, \\(x_n\\), we see that it is indeed the root of the function.\n\nfig, ax = plt.subplots()\n\nax.axvline(x_n, linestyle=\"--\", color=\"tab:red\",\n           linewidth=2, label=\"Final Guess\")\nax.legend()\n\nx_vec = np.linspace(-1.5, 0, 1000000)\n\nax.grid(alpha=.5)\nax.plot(x_vec, sympy.lambdify(x, y)(x_vec))\nax.set_title(f\"$y={sympy.latex(y)}$\")\nax.axhline(0, color=\"black\", alpha=.5);\n\n\n\n\n\n\n\n\nWe can also compare our answer to what sympy gets from numerically solving for the function’s root:\n\nprint(\"Newton's Method:\", x_n)\n\nNewton's Method: -1.1009529619409872\n\n\n\nprint(\"Sympy's answer:\", sympy.solveset(y, x, sympy.Interval(-5, 0)).evalf())\n\nSympy's answer: {-1.10095296194099}\n\n\nWe see that we have basically the same answer as sympy."
  },
  {
    "objectID": "posts/newtons-method/index.html#newtons-method-as-optimization",
    "href": "posts/newtons-method/index.html#newtons-method-as-optimization",
    "title": "Newton’s Method From The Ground Up",
    "section": "Newton’s Method as Optimization",
    "text": "Newton’s Method as Optimization\nRecall that the derivative of a function is 0 at a critical point – its maximum or minimum. We have been using Newton’s Method to find the root of a function, but in the process we’ve also implicitly been finding the critical point of the anti-derivative, or, integral of that function.\nFor visual intuition of this fact, consider the following plot, in which we visualize the integral of our function, \\(\\int f(x) dx\\), and plot the root of \\(f(x)\\) that we just found using Newton’s Method:\n\nfig, ax = plt.subplots()\n\nax.axvline(x_n, linestyle=\"--\", color=\"tab:red\",\n           linewidth=2, label=\"Final Guess\")\n\nax.grid(alpha=.5)\nax.plot(x_vec, sympy.lambdify(x, y)(x_vec), label=\"$f(x)$\")\nax.plot(x_vec, sympy.lambdify(x, y.integrate())(x_vec), label=\"$\\int f(x) dx$\")\nax.legend()\nax.set_ylim(-5, 7)\nax.set_title(f\"$\\int ({sympy.latex(y)})dx$\");\n\n\n\n\n\n\n\n\nThe root of \\(f(x)\\) seems to also be the maximum of \\(\\int f(x) dx\\).\nWhy is this important? Based on this idea, we can extend Newton’s Method for general use in finding the critical value of a function, which means that we can use it for solving optimization problems. We can setup the optimization approach as follows:\nIf the following equation will converge to the root of \\(f(x)\\) and the critical point of \\(\\int f(x) dx\\): \\[\n\\begin{align*}\nx_{n+1} = x_n -\\frac{f(x_n)}{f'(x_n)}\n\\end{align*}\n\\] Then the following equation will converge to the root of \\(f'(x)\\) and thus the critical point of \\(f(x)\\) (equivalent to \\(\\int f'(x)dx\\)): \\[\n\\begin{align*}\nx_{n+1} = x_n -\\frac{f'(x_n)}{f''(x_n)}\n\\end{align*}\n\\]\n(“Newton’s Method” 2023, “Minimization and maximization problems”)"
  },
  {
    "objectID": "posts/iterated-expectations/index.html",
    "href": "posts/iterated-expectations/index.html",
    "title": "Iterated Expectations",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nI recently came across a list of 10 theorems/proofs that you “need to know” if you do econometrics. These were compiled by Jeffrey Wooldridge, an economist and textbook author whose introductory textbook has been fundamental to my interest in econometrics. As an exercise, I’m working through these 10 items, compiling resources, textbook passages, and data exercises that I think can make them easier to understand. The first item I’m trying to write my notes on is the Law of Iterated Expectations, but I’ll be prefacing/augmenting the notes with some discussion of basic probability for completeness.\nMy core reference is Introduction to Probability, Second Edition By Joseph K. Blitzstein, Jessica Hwang.\nTo start, I’ll simulate some data.\n# Set a random seed for reproducability\nnp.random.seed(42)\n# Define the number of people in the dataset\nnum_people = 100\n# Generate random ages - X ~ Uniform(min, max)\nages = np.random.randint(71, 79, num_people)\n# Create DataFrame\ndata = {'Person_ID': range(1, num_people + 1), 'Age': ages}\npeople_df = pd.DataFrame(data).set_index(\"Person_ID\")\n\npeople_df.head()\n\n\n\n\n\n\n\n\nAge\n\n\nPerson_ID\n\n\n\n\n\n1\n77\n\n\n2\n74\n\n\n3\n75\n\n\n4\n77\n\n\n5\n73\nLet’s say that these data represent life spans, thus \\(\\text{age}_i\\) is an individual’s lifespan, e.g. \\(\\text{age}_2=\\)\npeople_df.loc[2]\n\nAge    74\nName: 2, dtype: int32"
  },
  {
    "objectID": "posts/iterated-expectations/index.html#expectation",
    "href": "posts/iterated-expectations/index.html#expectation",
    "title": "Iterated Expectations",
    "section": "Expectation",
    "text": "Expectation\nThe mean of a random variable, like age above, is also referred to as its “expected value,” denoted \\(E(\\text{age})\\).\n\npeople_df['Age'].mean()\n\n74.6\n\n\nThe mean above is specifically called an arithmetic mean, defined as follows:\n\\[ \\bar{x} = \\frac{1}{n} \\sum_i^n x_i\\]\n\n(1/len(people_df)) * people_df['Age'].sum()\n\n74.60000000000001\n\n\nBut the arithmetic mean is just a special case of the more general weighted mean:\n\\[\\begin{align*}\n\\text{weighted-mean}(x) &= \\sum_i^n x_i p_i \\\\\n\\end{align*}\\]\nWhere the weights, \\(p_1, p_2, ...,p_n\\) are non-negative numbers that sum to 1. We can see that the arithmetic mean is the specific case of the weighted mean where all weights are equal\n\\[\\begin{align*}\n\\text{If } [p_1=p_2=...=p_n] &\\text{ And } [\\sum_i^n p_i =1]\\\\\n\\text{weighted-mean}(x) &= \\sum_i^n x_i \\frac{1}{n}\\\\\n\\text{weighted-mean}(x) &= \\frac{1}{n} \\sum_i^n x_i = \\bar{x}\\\\\n\\end{align*}\\]\nWe use the more general weighted mean when we define expectation.\n\nthe expected value of \\(X\\) is a weighted average of the possible values that \\(X\\) can take on, weighted by their probabilities\n– Blitzstein and Hwang (2019)\n\nMore formally, given a random variable, \\(X\\), with distinct possible values, \\(x_1, x_2, ... x_n\\), the expected value \\(E(X)\\) is defined as:\n\\[\\begin{align*}\nE(X) = & x_1P(X = x_1) + \\\\\n&x_2P(X = x_2) +\\\\\n&... + x_nP(X = x_n) \\\\\n= &\\sum_{i=1}^n x_iP(X = x_i)\n\\end{align*}\\]\nNow we’ll demonstrate this formula on our data. It’s useful here to move from our individual-level dataset, where each row is a person, to the following, where each row is a lifespan, which the probability that an individual has that lifespan.\n\nprob_table = people_df['Age'].value_counts(normalize=True)\nprob_table = prob_table.sort_index()\nprob_table\n\nAge\n71    0.08\n72    0.13\n73    0.11\n74    0.19\n75    0.12\n76    0.13\n77    0.13\n78    0.11\nName: proportion, dtype: float64\n\n\nAdapting the formula above to our data, we must solve the following:\n\\[\\begin{align*}\nE(\\text{Age}) = &\\text{Age}_1P(\\text{Age}=\\text{Age}_1) + \\\\\n&\\text{Age}_2P(\\text{Age}=\\text{Age}_2) + \\\\\n&... + \\text{Age}_nP(\\text{Age}=\\text{Age}_n) \\\\\n= &\\sum_{i=1}^n \\text{Age}_iP(\\text{Age}=\\text{Age}_i)\n\\end{align*}\\]\nWhich we can do transparently using a for-loop:\n\nsummation = 0\nfor i in range(len(prob_table)):\n  summation += prob_table.index[i] * prob_table.values[i]\nsummation\n\n74.60000000000001\n\n\nAs a quick aside – this can also be expressed as the dot product of two vectors, where the dot product is defined as follows:\n\\[\n\\begin{align*}\n\\vec{\\text{Age}}\\cdot P(\\vec{\\text{Age}}) = &\\text{Age}_1P(\\text{Age}=\\text{Age}_1) + \\\\\n&\\text{Age}_2P(\\text{Age}=\\text{Age}_2) + \\\\\n&... + \\text{Age}_3P(\\text{Age}=\\text{Age}_3)\n\\end{align*}\n\\]\n\nprob_table.index.values @ prob_table.values\n\n74.6\n\n\nThough we will stick to the summation notation paired with python for-loops for consistency"
  },
  {
    "objectID": "posts/iterated-expectations/index.html#conditional-expectation",
    "href": "posts/iterated-expectations/index.html#conditional-expectation",
    "title": "Iterated Expectations",
    "section": "Conditional Expectation",
    "text": "Conditional Expectation\nWe often have more than one variable available to us in an analysis. Below I simulate the variable gender:\n\nnp.random.seed(45)\npeople_df['Gender'] = np.random.choice(['Female', 'Male'], len(people_df))\npeople_df.head()\n\n\n\n\n\n\n\n\nAge\nGender\n\n\nPerson_ID\n\n\n\n\n\n\n1\n77\nMale\n\n\n2\n74\nFemale\n\n\n3\n75\nMale\n\n\n4\n77\nFemale\n\n\n5\n73\nFemale\n\n\n\n\n\n\n\nEach row in our dataset represents an individual person, and we now have access to both their gender and their life-span. It follows that we may be interested in how life-span varies across gender. In code, this entails a groupby operation, grouping on gender before calculting the mean age:\n\npeople_df.groupby('Gender')['Age'].mean()\n\nGender\nFemale    74.672727\nMale      74.511111\nName: Age, dtype: float64\n\n\nThe code in this case resembles the formal notation of a conditional expectation: \\(E(\\text{Age} \\mid \\text{Gender}=\\text{Gender}_j)\\), where each \\(\\text{Gender}=\\text{Gender}_j\\) is a distinct event.\nIf we are interested specifically in the mean life-span given the event that gender is equal to male (a roundabout way of saying the average life-span for males in the data), we could calculate the following\n\\(E(\\text{Age} \\mid \\text{Gender}=\\text{Male})\\)\n\npeople_df.groupby('Gender')['Age'].mean()['Male']\n\n74.5111111111111\n\n\nThese groupby operations in pandas obscure some of the conceptual stuff happening inside the conditional expectation, which we’ll delve deeper into now.\nSo what exactly is the conditional expectation, \\(E(X \\mid Y=y)\\)?\nBefore answering this, it will be useful to refresh the related concept of conditional probability:\n\nIf \\(X=x\\) and \\(Y=y\\) are events with \\(P(Y=y)&gt;0\\), then the conditional probability of \\(X=x\\) given \\(Y=y\\) is denoted by \\(P(X=x \\mid Y=y)\\), defined as\n\\[ P(X=x \\mid Y=y) = \\frac{P(X=x , Y=y)}{P(Y=y)} \\]\n– Blitzstein and Hwang (2019)\n\nThis formula specifically describes the probability of the event, \\(X=x\\), given the evidence, an observed event \\(Y=y\\).\nWe want to shift to describing a mean conditional on that evidence, and we include that information via the weights in the expectation.\n\nRecall that the expectation \\(E(X)\\) is a weighted average of the possible values of \\(X\\), where the weights are the PMF values \\(P(X = x)\\). After learning that an event \\(Y=y\\) occurred, we want to use weights that have been updated to reflect this new information.\n– Blitzstein and Hwang (2019)\n\nThe key point here is that just the weights that each \\(x_i\\) gets multiplied by will change, going from the probability \\(P(X=x)\\) to the conditional probability \\(P(X=x \\mid Y=y)\\).\nArmed with conditional probability formula above, we can define how to compute the conditional expected value \\[\n\\begin{align*}\nE(X \\mid Y=y) &= \\sum_{x} x P(X=x \\mid Y=y) \\\\\n&= \\sum_{x} x \\frac{P(X=x , Y=y)}{P(Y=y)}\n\\end{align*}\n\\]\nReturning to our example with data, we substitute terms to find the following: \\[\n\\begin{align*}\nE(\\text{Age} \\mid \\text{Gender}=\\text{Male}) &= \\sum_{i=1}^n \\text{Age}_iP(\\text{Age}=\\text{Age}_i \\mid \\text{Gender}=\\text{Male}) \\\\\n&= \\sum_{i=1}^n \\text{Age}_i \\frac{P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Male})}{P(\\text{Gender}=\\text{Male})}\n\\end{align*}\n\\]\nWe can explicitly compute this with a for-loop in python, as we did for \\(E(X)\\), but this time we will need to do a little up front work and define components we need for calculating the weights, \\(\\frac{P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Male})}{P(\\text{Gender}=\\text{Male})}\\)\n\nComponents\n\nThe joint probability distribution: \\(P(\\text{Age}=\\text{Age}_i, \\text{Gender} = \\text{Male})\\)\nThe probability of the event, \\(P(\\text{Gender}=\\text{Male})\\)\n\nWhere 1.) is the following:\n\nP_Age_Gender = pd.crosstab(people_df['Age'],\n                           people_df['Gender'],\n                           normalize='all')\nP_Age_Gender['Male']\n\nAge\n71    0.03\n72    0.07\n73    0.05\n74    0.08\n75    0.07\n76    0.05\n77    0.06\n78    0.04\nName: Male, dtype: float64\n\n\nand 2.) is:\n\nP_Gender = people_df['Gender'].value_counts(normalize=True)\nP_Gender.loc['Male']\n\n0.45\n\n\nWith those two pieces, we’ll convert the following into a for-loop: \\[\n\\sum_{i=1}^n \\text{Age}_i \\frac{P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Male})}{P(\\text{Gender}=\\text{Male})}\n\\]\n\nE_age_male = 0\nn = len(P_Age_Gender['Male'])\nfor i in range(n):\n  weight = P_Age_Gender['Male'].values[i] / P_Gender.loc['Male']\n  E_age_male += P_Age_Gender['Male'].index[i] * weight\nE_age_male\n\n74.51111111111112\n\n\nWe confirm that this is equal to the result of the more direct groupby:\n\npeople_df.groupby('Gender')['Age'].mean()['Male']\n\n74.5111111111111"
  },
  {
    "objectID": "posts/iterated-expectations/index.html#the-law-of-iterated-expectations",
    "href": "posts/iterated-expectations/index.html#the-law-of-iterated-expectations",
    "title": "Iterated Expectations",
    "section": "The Law of Iterated Expectations",
    "text": "The Law of Iterated Expectations\nThe law of iterated expectations, also referred to as the law of total expectation, the tower property, Adam’s law, or, my favorite, LIE, states the following: \\[E(X) = E(E(X \\mid Y))\\] Which is to say, the weighted average of \\(X\\) is equal to the weighted average of the weighted averages of \\(X\\) conditional on each value of \\(Y\\). This isn’t a particularly useful sentence, so let’s return to our example data. We plug in our values as follows: \\[E(\\text{Age}) = E(E(\\text{Age} \\mid \\text{Gender}))\\] Now it is useful to break this into some components that we’ve seen before. We previously found \\[\nE(\\text{Age} \\mid \\text{Gender}=\\text{Male}) =  \\sum_{i=1}^n \\text{Age}_i \\frac{P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Male})}{P(\\text{Gender}=\\text{Male})}\n\\]\nOver all \\(\\text{Gender}_j\\), we have the more generalizable expression: \\[\nE(\\text{Age} \\mid \\text{Gender}=\\text{Gender}_j)\n\\]\nWhich can tell us about any gender, not just \\(\\text{Gender}=\\text{Male}\\). This is equivalent to the expression:\n\\[\n\\begin{align*}\nE(\\text{Age} \\mid \\text{Gender} = \\text{Gender}_j) = \\sum_{i=1}^n \\text{Age}_i \\frac{P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Gender}_j) }{P(\\text{Gender}=\\text{Gender}_j)}\n\\end{align*}\n\\]\nGiven this, let’s return to the informal definition of the LIE, but break it into parts. The weighted average of \\(X\\) is equal to:\n1. The weighted average of 2. the weighted averages of \\(X\\) conditional on each value of \\(Y\\)“.\nThe expression above, \\(E(\\text{Age} \\mid \\text{Gender}=\\text{Gender}_j)\\) is equivalent to 2.) “the weighted averages of \\(X\\) conditional on each value of \\(Y\\).” So what we need to do now is find the weighted average of that expression. We’ll set up in the next few lines\n\\[\\begin{align*}\nE(\\text{Age}) &=E( \\underbrace{E(\\text{Age} \\mid \\text{Gender}=\\text{Gender}\\_j)}_{\\text{weighted averages conditional on each gender}} ) \\\\\n&=E(\\sum_{i} \\text{Age}_i \\frac{P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Gender}_j) }{P(\\text{Gender}=\\text{Gender}_j)}) \\\\\n\\end{align*}\\]\nWith that set up, we’ll now write out the last weighted average explicitly. Note that the variation in \\(\\text{Age}_i\\) has been accounted for – we are now averaging over gender, \\(\\text{Gender}_j\\).\n\\[\\begin{align*}\n&=\\sum_j (\\sum_{i} \\text{Age}_i \\frac{P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Gender}_j) }{P(\\text{Gender}=\\text{Gender}_j)}) P(\\text{Gender}=\\text{Gender}_j) \\\\\n&=\\sum_j \\sum_{i} \\text{Age}_i P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Gender}_j) \\\\\n\\end{align*}\\]\nSince \\(j\\) only appears in one of these two terms, we can rewrite this as follows:\n\\[\\begin{align*}\n&=  \\sum_{i} \\text{Age}_i \\sum_j P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Gender}_j)\n\\end{align*}\\]\nHere I’ll pause, because the next steps can be clarified with code. \\(P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Gender}_j)\\) is the joint probability distribution of age and gender, and it helps to take a look at exactly what it is in pandas:\n\nP_Age_Gender\n\n\n\n\n\n\n\nGender\nFemale\nMale\n\n\nAge\n\n\n\n\n\n\n71\n0.05\n0.03\n\n\n72\n0.06\n0.07\n\n\n73\n0.06\n0.05\n\n\n74\n0.11\n0.08\n\n\n75\n0.05\n0.07\n\n\n76\n0.08\n0.05\n\n\n77\n0.07\n0.06\n\n\n78\n0.07\n0.04\n\n\n\n\n\n\n\nLet’s compute the summation of \\(P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Gender}_j)\\) over \\(\\text{Gender}_j\\) and see what we get.\n\nP_Age_Gender[\"Male\"] + P_Age_Gender[\"Female\"]\n\nAge\n71    0.08\n72    0.13\n73    0.11\n74    0.19\n75    0.12\n76    0.13\n77    0.13\n78    0.11\ndtype: float64\n\n\nInterestingly, that is the exact same thing we get if we simply compute the probability of each age, \\(P(\\text{Age}=\\text{Age}_i)\\)\n\npeople_df['Age'].value_counts(normalize=True).sort_index()\n\nAge\n71    0.08\n72    0.13\n73    0.11\n74    0.19\n75    0.12\n76    0.13\n77    0.13\n78    0.11\nName: proportion, dtype: float64\n\n\nSo when you sum \\(P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Gender}_j)\\) only over \\(\\text{Gender}_j\\), you’re just left with \\(P(\\text{Age}=\\text{Age}_i)\\). This result stems from the definition of the Marginal PMF:\n\nFor the discrete random variables \\(X\\) and \\(Y\\), the marginal PMF of \\(X\\) is:\n\\[P(X=x) = \\sum_y P(X=x, Y=y)\\] – Blitzstein and Hwang (2019)\n\nand with this definition in mind we can finish the proof for the LIE: \\[\n\\begin{align*}\nE(\\text{Age})  &= \\sum_{i} \\text{Age}_i \\sum_j P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Gender}_j) \\\\\n&= \\sum_{i} \\text{Age}_i  P(\\text{Age}=\\text{Age}_i) \\\\\n&= E(\\text{Age})\n\\end{align*}\n\\]\nWe can directly show the last bit, \\(E(\\text{Age}) = \\sum_j \\sum_{i} \\text{Age}_i P(\\text{Age}=\\text{Age}_i, \\text{Gender}=\\text{Gender}_j)\\) using the joint probability distribution object from before:\n\nP_Age_Gender.head()\n\n\n\n\n\n\n\nGender\nFemale\nMale\n\n\nAge\n\n\n\n\n\n\n71\n0.05\n0.03\n\n\n72\n0.06\n0.07\n\n\n73\n0.06\n0.05\n\n\n74\n0.11\n0.08\n\n\n75\n0.05\n0.07\n\n\n\n\n\n\n\n\n(P_Age_Gender\n .sum(axis=1) # sum over j\n .reset_index() # bring out Age_i\n .product(axis=1) # Age_i * P(Age=Age_i)\n .sum() # sum over i\n )\n\n74.6\n\n\n\npeople_df['Age'].mean()\n\n74.6"
  },
  {
    "objectID": "posts/excel-wings/index.html",
    "href": "posts/excel-wings/index.html",
    "title": "Using The Excel Object Model in Python",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport xlwings as xw\nimport os\nIn this post, I’m going to overview how to use Python’s xlwings and pywin32 libraries to produce formatted, human-readable tables in Excel. Beyond basic uses of xlwings, I’ll overview how to leverage pywin32 and the .api attribute to interact directly with the Excel object model using VBA-like Python code. The approach allows one to produce highly customizable Excel tables that should satisfy any audience that prefers formatted spreadsheet output rather than, say, a quarto doc.\nAlongside this blog post, I’ve been developing an excel submodule in my swiss-code package, with various wrapper functions for xlwings and pywin32. I’ll also use this post to showcase that submodule, and specifically the ExcelDataFrame class that I’ve developed to integrate a given pandas dataframe with a corresponding table range in Excel."
  },
  {
    "objectID": "posts/excel-wings/index.html#problem-setup",
    "href": "posts/excel-wings/index.html#problem-setup",
    "title": "Using The Excel Object Model in Python",
    "section": "Problem Setup",
    "text": "Problem Setup\n\n\nCode for dataset generation\ndef simulate_df(num_transactions=1000):\n    np.random.seed(1)\n    customer_ids = np.random.randint(1000, 5000, num_transactions)\n    transaction_amounts = np.round(np.random.uniform(5, 500, num_transactions), 2)\n    payment_methods = np.random.choice([\"Credit Card\", \"Debit Card\", \"PayPal\", \"Cash\"], num_transactions)\n    categories = np.random.choice([\"Electronics\", \"Clothing\", \"Groceries\", \"Entertainment\"], num_transactions)\n    transaction_dates = pd.date_range(start=\"2024-01-01\", periods=num_transactions, freq=\"D\")\n    data = {\n        \"transaction_id\": range(1, num_transactions + 1),\n        \"customer_id\": customer_ids,\n        \"amount\": transaction_amounts,\n        \"payment_method\": payment_methods,\n        \"category\": categories,\n        \"date\": transaction_dates\n    }\n    df = pd.DataFrame(data)\n    df['year'] = df['date'].dt.year.astype(str)\n    df[\"class\"] = df[\"category\"].map(\n        {\n            \"Clothing\": \"Necessity\",\n            \"Groceries\": \"Necessity\",\n            \"Electronics\": \"Discretionary\",\n            \"Entertainment\": \"Discretionary\",\n        }\n    )\n    return df\n\n\nSay we have a dataset of customer transactions at a super store. I simulated this data arbitrarily.\n\ndf = simulate_df()\ndf.head()\n\n\n\n\n\n\n\n\ntransaction_id\ncustomer_id\namount\npayment_method\ncategory\ndate\nyear\nclass\n\n\n\n\n0\n1\n2061\n177.75\nDebit Card\nElectronics\n2024-01-01\n2024\nDiscretionary\n\n\n1\n2\n1235\n33.20\nCredit Card\nClothing\n2024-01-02\n2024\nNecessity\n\n\n2\n3\n4980\n118.26\nCredit Card\nGroceries\n2024-01-03\n2024\nNecessity\n\n\n3\n4\n2096\n333.73\nDebit Card\nEntertainment\n2024-01-04\n2024\nDiscretionary\n\n\n4\n5\n4839\n251.14\nCredit Card\nElectronics\n2024-01-05\n2024\nDiscretionary\n\n\n\n\n\n\n\nLet’s imagine the case where a stakeholder asks for a report on the following:\n\nTotal dollars transacted, broken down by key categories and payment method.\n\nIt’s fairly straightforward to use pandas to make a pivot table report, complete with human-readable, title-cased indices and columns. In this case, I’ve aggregated the total dollars spent, dissagregated by payment method and category of good. Some users prefer that the pivot table also include subtotals, and we flexibly add those with margins=True.\n\ndollars_by_method = df.pivot_table(\n    index=\"payment_method\",\n    columns=\"category\",\n    values=\"amount\",\n    aggfunc=\"sum\",\n    margins=True,\n    margins_name=\"Total\",\n)\ndollars_by_method.index.name = \"Payment Method\"\ndollars_by_method.columns.name = \"Category\"\ndollars_by_method\n\n\n\n\n\n\n\nCategory\nClothing\nElectronics\nEntertainment\nGroceries\nTotal\n\n\nPayment Method\n\n\n\n\n\n\n\n\n\nCash\n15016.00\n17026.47\n18208.16\n18242.88\n68493.51\n\n\nCredit Card\n16074.92\n10960.64\n16052.85\n20187.25\n63275.66\n\n\nDebit Card\n14032.23\n17325.96\n14363.99\n17188.67\n62910.85\n\n\nPayPal\n14143.84\n15108.84\n16852.17\n12065.14\n58169.99\n\n\nTotal\n59266.99\n60421.91\n65477.17\n67683.94\n252850.01\n\n\n\n\n\n\n\nThat’s a fine-enough pandas dataframe, but now the question arises of how to best serve this data to the end-user.\nThere are several options – lately I’ve been using Quarto reports or else Tableau dashboards for these sorts of analytics projects. However, those formats are very “finalized” and can be a little unsatisfying for end-users who want to further filter or modify table output. If that audience doesn’t use R/Python, access to underlying .ipynb or .qmd files will not be helpful. This is the case where I think that spreadsheet output, and specifically reproducible, formatted Excel worksheets, can be very potent."
  },
  {
    "objectID": "posts/excel-wings/index.html#getting-connected-to-excel",
    "href": "posts/excel-wings/index.html#getting-connected-to-excel",
    "title": "Using The Excel Object Model in Python",
    "section": "Getting connected to Excel",
    "text": "Getting connected to Excel\nTo write a pandas dataframe out to Excel, we’ll work through the Excel/xlwings object hierarchy:\n\nApplication1 \\(\\rightarrow\\) Work Book \\(\\rightarrow\\) Sheet \\(\\rightarrow\\) Range.\n\nIn xlwings we’ll establish a xw.Book object, which either creates or opens an existing excel work book, in this case test.xlsx.\n\nfilename = \"test.xlsx\"\nif os.path.exists(filename):\n        wb = xw.Book(filename)\nelse:\n    wb = xw.Book()\n    wb.save(filename)\nwb\n\n&lt;Book [test.xlsx]&gt;\n\n\nNext, we establish an xw.Sheet within that xw.Book object, in this case I’ll call the sheet first_test.\n\nsheet_name = 'first_test'\ntry:\n    sheet_new = wb.sheets.add(sheet_name) \nexcept ValueError:\n    sheet_new = wb.sheets[sheet_name]\nsheet_new\n\n&lt;Sheet [test.xlsx]first_test&gt;\n\n\nAt this point, we can select the sheet, sheet_new, select a start range, “A1”, then set it’s value with our dataframe object from the previous section, dollars_by_method. This will export the full dataframe into the spreadsheet.\n\nsheet_new.range(\"A1\").options(index=True, header=True).value = dollars_by_method\n\nThen, when we save the book, we can inspect the output by opening text.xlsx. Note that the data were entered into cell A1 and expanded as needed.\n\nwb.save()\n\n\nThis is an unattractive table, with no formatting and with poorly fitted columns. This is also equivalent to what we could get if we simply used the df.to_excel() method within pandas. We’ll proceed to format this table, that that is where we get the real value out of xlwings."
  },
  {
    "objectID": "posts/excel-wings/index.html#interacting-with-the-excel-object-model",
    "href": "posts/excel-wings/index.html#interacting-with-the-excel-object-model",
    "title": "Using The Excel Object Model in Python",
    "section": "Interacting with the Excel Object Model",
    "text": "Interacting with the Excel Object Model\nThe most powerful aspect of xlwings is that it allows for interacting with the Excel object model – an API for programmatically editing Excel spreadsheets. One would typically interact with Excel’s object model via VBA code, but xlwings and pywin32 together allow for us to use Python for a conceptually similar workflow.\nNote that Microsoft provides a few key pieces of documentation that augment xlwings’ own API reference.\n\nAn overview of the object model concept and typical VBA workflows in in the “VBA Programming 101” section of the Official VBA Reference, and,\nA complete reference for the Excel object model in the Official VBA Reference.\n\n\n\nExcel Object Model for VBA Reference\n\n\n\nWe can access the object model API from an xw.Sheet object by calling the .api attribute:\n\nsheet_new.api\n\n&lt;win32com.gen_py.Microsoft Excel 16.0 Object Library._Worksheet instance at 0x2714414253696&gt;\n\n\nThis is an instance of VBA’s Worksheet object, which has a documentation page in the VBA reference.\n\n\nThe Worksheet Object\n\n\n\n\nAutoFit\nPython objects have attributes and methods, whereas the VBA Worksheet object has properties and methods. One particularly useful property is UsedRange, which stores the range of cells that have data. We often access that property when we want to format a table.\n\nsheet_new.api.UsedRange\n\n&lt;win32com.gen_py.Microsoft Excel 16.0 Object Library.Range instance at 0x2712378707792&gt;\n\n\nFor very popular properties and methods from the Excel object model, such as UsedRange, the xlwings api has simplified python attributes and methods that allow one to avoid explicity accessing the object model api. E.g. we can call xw.Sheet.used_range for similar functionality to xw.Sheet.UsedRange.\n\nsheet_new.used_range\n\n&lt;Range [test.xlsx]first_test!$A$1:$F$6&gt;\n\n\nI’ll stick with the former approach for now.\nFrom the UsedRange attribute, which is a Range object, we can access the EntireColumn property, which selects the entire columns for a given range.\n\nsheet_new.api.UsedRange.EntireColumn\n\n&lt;win32com.gen_py.Microsoft Excel 16.0 Object Library.Range instance at 0x2712379336816&gt;\n\n\nThis is equivalent to the following selection:\n\nFor that Range object, we will now call the Autofit() method.\n\nsheet_new.api.UsedRange.EntireColumn.AutoFit()\n\nTrue\n\n\nWe get the following in Excel:\n\nLet’s autofit the rows and the columns where there is currently data.\n\nsheet_new.api.UsedRange.EntireColumn.AutoFit()\nsheet_new.api.UsedRange.EntireRow.AutoFit()\nwb.save()\n\n\n\nNumber Formatting\nNow we’ll move onto formatting the data, which represent dollar totals, as follows:\n\n11020 \\(\\rightarrow\\) $11,020\n\nI’ll be using a mix of xlwings wrapper methods/attributes, and explicit calls to the Excel object model via .api, with no real commitment as to when to use which. Given that we get equivalent results, I’m of the mind that it doesn’t particularly matter.\nWe can apply dollar formatting to any range in Excel via the NumberFormat property – number_format in xlwings. But to do that, we’ll first need to define a range that captures exactly where there is numerical data.\nI’ll first select the top row of the sheet and retrieve its values, which are the column headers:\n\nheaders = sheet_new.range(\"A1\").expand(\"right\").value\nheaders\n\n['Payment Method',\n 'Clothing',\n 'Electronics',\n 'Entertainment',\n 'Groceries',\n 'Total']\n\n\nWe now want to access those columns that actually contain data (Payment Method is the index title and doesn’t contain data). We’ll do this by taking the index of Clothing, the first column with data, adding one,2\n\ncol_index = headers.index('Clothing') + 1\ncol_index\n\n2\n\n\nthen retrieving the letter that corresponds to that index:\n\ncol_letter = xw.utils.col_name(col_index) \ncol_letter\n\n'B'\n\n\nGiven the column letter, we can now select all of the data in that column:\n\nselection = f\"{col_letter}2:{col_letter}1000000\"\nselection\n\n'B2:B1000000'\n\n\nThen we can set the number format (see supported Number format codes):\n\nsheet_new.range(selection).number_format = \"$#,###.##\"\nwb.save()\n\n\nTo format all of the data, we’ll just iterate through the columns and execute that same chunk of code:\n\nfor col in dollars_by_method.columns:\n    col_index = headers.index(col) + 1\n    col_letter = xw.utils.col_name(col_index)\n    selection = f\"{col_letter}2:{col_letter}1048576\"\n    sheet_new.range(selection).number_format = \"$#,###.##\"\nwb.save()\n\n\n\n\nText Formatting\nNow I’d like to bold-format the index to differentiate it from the data.\nWe’ll use a similar process to make the column and index headers bold. I’ll translate the dataframe’s headers into an Excel range as follows – I retrieve the numerical index of the last column (indexed from 1)\n\nlast_column_index = dollars_by_method.reset_index().shape[1]\nlast_column_index\n\n6\n\n\nI translate that into its column letter:\n\nend_col = xw.utils.col_name(last_column_index)\nend_col\n\n'F'\n\n\nThen I use that to define the range of the columns, which are all in row 1:\n\nselection = f\"A1:{end_col}1\"\nselection\n\n'A1:F1'\n\n\nWith that range selection, we can access the .font attribute in xlwings, which corresponds to the Font object in the Excel object model, and set bold = True.\n\nheader_range = sheet_new.range(selection)\nheader_range.font.bold = True\nwb.save()\n\n\nI’ll do the same with the index, capturing its range then setting bold = True\n\nend_row = dollars_by_method.reset_index().shape[0] + 1\nselection = f\"A1:A{end_row}\"\nselection\n\n'A1:A6'\n\n\n\nheader_range = sheet_new.range(selection)\nheader_range.font.bold = True\nwb.save()\n\n\n\n\nShading and Borders\nTo finish the formatting, I’d also like to shade in the grand total column/row so as to differentiate it from unaggregated data, and add basic black borders on all cells.\nWe’ll be doing much the same as before when it comes to shading in rows/columns – we define a range that captures an entire target row/column, then access the appropriate object to format that range. I’ll reuse the end_col and end_row values defined above to get the range of the last column\n\ntotal_col = f\"{end_col}1:{end_col}{end_row}\"\ntotal_col\n\n'F1:F6'\n\n\nIn excel, we set colors using RGB codes, which I like to get from https://htmlcolorcodes.com/, though any color-swatch site should suffice.\n\ntotal_col = f\"{end_col}1:{end_col}{end_row}\"\nsheet_new.range(total_col).color = (242, 242, 242)\ntotal_row = f\"A{end_row}:{end_col}{end_row}\"\nsheet_new.range(total_row).color = (242, 242, 242)\nprint(\"Total Column:\", total_col, \"\\nTotal Row:\", total_row)\nwb.save()\n\nTotal Column: F1:F6 \nTotal Row: A6:F6\n\n\n\nFor defining borders, we’ll just access the Borders object from the UsedRange, so that we apply the borders only where there is currently data. We set two properties for the Border:\n\nLineStyle, e.g. solid versus dashed.\nWeight, e.g. hairline versus thick.\nThere are several other properties we can change as needed in the Border object’s properties, but I typically just focus on style and weight.3\n\n\nsheet_new.used_range.api.Borders.LineStyle = 1\nsheet_new.used_range.api.Borders.Weight = 2\nwb.save()"
  },
  {
    "objectID": "posts/excel-wings/index.html#a-package-approach",
    "href": "posts/excel-wings/index.html#a-package-approach",
    "title": "Using The Excel Object Model in Python",
    "section": "A Package approach",
    "text": "A Package approach\nxlwings is great, but sometimes bridging the gap between working on a pandas dataframe and an Excel object can be a little involved. For example, formatting a column requires finding that column within Excel then programatically defining its range, being mindful of the different numerical indexing systems in Python vs. Excel, potential issues with pd.MultiIndex objects, etc.\nThis can require a lot of code, so I defined a set of wrapper functions, and eventually an entire class, ExcelDataFrame, to facilitate moving between pandas and Excel. This class is part of the submodule, swiss_code.excel, which is a part of my larger swiss_code package.\nYou can download as follows:\npip install git+https://github.com/peter-amerkhanian/swiss-code.git@main\n\n\nMy environment specific setup\nimport sys\nsys.path.append('../../../swiss-code/src')\n%load_ext autoreload\n%autoreload 2\n\n\nOnce you have the package installed, you can import the excel submodule.\n\nfrom swiss_code.excel import excel\n\nGiven a pandas dataframe and an Excel sheet, we can establish an ExcelDataFrame object via the excel.write_df_to_excel function. This will write the pandas dataframe into the Excel sheet, and establish an object that can be further manipulated.\n\nwb = excel.get_or_create_workbook(\"test.xlsx\")\nfunction_sheet = excel.select_sheet('function_sheet', wb)\nedf = excel.write_df_to_excel(df=dollars_by_method,\n                              sheet=function_sheet,\n                              cell_start=\"A1\")\nedf\n\nACTIVE ExcelDataFrame(sheet=function_sheet, range=$A$1:$F$6, df_shape=(5, 5))\n\n\nWe’ll now complete all the same formatting tasks as before via method calls:\n\nfor col in edf.df.columns:\n    edf.format_column_data(col, format=\"$#,###.##\")\nedf.make_borders()\nedf.format_indices(bold=True, color=None)\nedf.format_row('Total', bold=\"ignore\", color=\"light_grey\")\nedf.format_column('Total', bold=\"ignore\", color=\"light_grey\")\nexcel.close_out_book(wb)\n\nOnce excel.close_out_book(wb) has been called, the edf no longer has an active connection to any workbook.\n\nedf\n\n DISCONNECTED ExcelDataFrame(sheet=NA, range=NA, df_shape=(5, 5))"
  },
  {
    "objectID": "posts/excel-wings/index.html#complex-tables",
    "href": "posts/excel-wings/index.html#complex-tables",
    "title": "Using The Excel Object Model in Python",
    "section": "Complex Tables",
    "text": "Complex Tables\nI’ll show some further functionality of the ExcelDataFrame class in the case of a much more complicated pivot table. Let’s say that a stakeholder wants something like the previous report, but with more disaggregation:\n\nTotal dollars transacted, broken down by discretionary vs. essential, and broken down by key categories and payment method, for each year available. Include subtotals for each year.\n\nThis is a fairly complex pivot table, and requires a little bit for code beyond just pd.pivot_table. I’ll iterate over years and generate pivot tables with subtotals for each, then I’ll concatenate them together:\n\n# Year Pivots w/ subtotals\ndollars_by_method_yr = pd.concat(\n    [df[df['year'] == year]\n     .pivot_table(index=[\"payment_method\"], \n                  columns=[\"class\", \"category\"],\n                  values=\"amount\",\n                  aggfunc=\"sum\",\n                  margins=True,\n                  margins_name=\"Total\")\n                  .reset_index()\n                  .assign(Year=year)\n                  .set_index(['Year', 'payment_method'])\n                  for year in ['2024', '2025', '2026']],\n                  axis=0)\ndollars_by_method_yr = dollars_by_method_yr.fillna(0)\ndollars_by_method_yr\n\n\n\n\n\n\n\n\nclass\nDiscretionary\nNecessity\nTotal\n\n\n\ncategory\nElectronics\nEntertainment\nClothing\nGroceries\n\n\n\nYear\npayment_method\n\n\n\n\n\n\n\n\n\n2024\nCash\n5182.41\n8661.28\n5709.71\n6552.53\n26105.93\n\n\nCredit Card\n2976.99\n5334.29\n5846.53\n7983.28\n22141.09\n\n\nDebit Card\n6997.97\n4998.41\n4887.89\n5603.41\n22487.68\n\n\nPayPal\n5649.27\n7034.34\n5225.22\n3945.90\n21854.73\n\n\nTotal\n20806.64\n26028.32\n21669.35\n24085.12\n92589.43\n\n\n2025\nCash\n6148.02\n5110.80\n4651.67\n7717.81\n23628.30\n\n\nCredit Card\n4618.44\n5946.31\n6843.13\n5828.80\n23236.68\n\n\nDebit Card\n6606.99\n7087.51\n5801.20\n4554.53\n24050.23\n\n\nPayPal\n5220.53\n5592.94\n6203.22\n3442.25\n20458.94\n\n\nTotal\n22593.98\n23737.56\n23499.22\n21543.39\n91374.15\n\n\n2026\nCash\n5696.04\n4436.08\n4654.62\n3972.54\n18759.28\n\n\nCredit Card\n3365.21\n4772.25\n3385.26\n6375.17\n17897.89\n\n\nDebit Card\n3721.00\n2278.07\n3343.14\n7030.73\n16372.94\n\n\nPayPal\n4239.04\n4224.89\n2715.40\n4676.99\n15856.32\n\n\nTotal\n17021.29\n15711.29\n14098.42\n22055.43\n68886.43\n\n\n\n\n\n\n\nThen I’ll attach a grand total at the bottom:\n\n# Grand Total\ndollars_by_method_yr.loc[(\"Grand Total\", \"\"), :] = (\n    df.pivot_table(\n        index=[\"class\", \"category\"],\n        values=\"amount\",\n        aggfunc=\"sum\",\n        margins=True,\n        margins_name=\"Total\"\n        )[\"amount\"]\n        )\n# Table Formatting\ndollars_by_method_yr.index.names = [\"Year\", \"Payment Method\"]\ndollars_by_method_yr.columns.names = ['Class', 'Category']\ndollars_by_method_yr\n\n\n\n\n\n\n\n\nClass\nDiscretionary\nNecessity\nTotal\n\n\n\nCategory\nElectronics\nEntertainment\nClothing\nGroceries\n\n\n\nYear\nPayment Method\n\n\n\n\n\n\n\n\n\n2024\nCash\n5182.41\n8661.28\n5709.71\n6552.53\n26105.93\n\n\nCredit Card\n2976.99\n5334.29\n5846.53\n7983.28\n22141.09\n\n\nDebit Card\n6997.97\n4998.41\n4887.89\n5603.41\n22487.68\n\n\nPayPal\n5649.27\n7034.34\n5225.22\n3945.90\n21854.73\n\n\nTotal\n20806.64\n26028.32\n21669.35\n24085.12\n92589.43\n\n\n2025\nCash\n6148.02\n5110.80\n4651.67\n7717.81\n23628.30\n\n\nCredit Card\n4618.44\n5946.31\n6843.13\n5828.80\n23236.68\n\n\nDebit Card\n6606.99\n7087.51\n5801.20\n4554.53\n24050.23\n\n\nPayPal\n5220.53\n5592.94\n6203.22\n3442.25\n20458.94\n\n\nTotal\n22593.98\n23737.56\n23499.22\n21543.39\n91374.15\n\n\n2026\nCash\n5696.04\n4436.08\n4654.62\n3972.54\n18759.28\n\n\nCredit Card\n3365.21\n4772.25\n3385.26\n6375.17\n17897.89\n\n\nDebit Card\n3721.00\n2278.07\n3343.14\n7030.73\n16372.94\n\n\nPayPal\n4239.04\n4224.89\n2715.40\n4676.99\n15856.32\n\n\nTotal\n17021.29\n15711.29\n14098.42\n22055.43\n68886.43\n\n\nGrand Total\n\n60421.91\n65477.17\n59266.99\n67683.94\n252850.01\n\n\n\n\n\n\n\nGetting that complex table into a formatted Excel table presents a couple complications:\n\nThere is a multi-index, which pandas presents as merged. In Excel, these merged multi-indexes will just render with the value repeated over the merged range.\nWe will want to shade and bold all subtotals to match the previous table’s formatting. This will be a little more difficult given that they exist for each year.\n\n\nwb = excel.get_or_create_workbook(\"test.xlsx\")\ncomplex_sheet = excel.select_sheet('complex_sheet', wb)\nedf = excel.write_df_to_excel(dollars_by_method_yr,\n                              complex_sheet,\n                              cell_start=\"A1\")\n\n\nI’ve defined a suit of methods to deal with this – notably edf.merge_axis for dealing with any cell merging problems, and I can use simple for loops to format the “total” rows. I also added in some vertical and horizontal alignment options when formatting.\n\nedf.merge_axis(index=1, axis=1)\nedf.merge_axis(index=1, axis=0)\nedf.format_indices(bold=True, color=\"light_grey\", v_align='Center', h_align='Center')\nfor col in edf.df.columns:\n    edf.format_column_data(col, format=\"$#,###.##\")\nfor year in df['year'].unique():\n    edf.format_row((year, 'Total'), bold=True, color=\"light_grey\")\nedf.format_row(('Grand Total', ''), bold=True, color=\"light_grey\")\nedf.format_column(('Total', ''), bold=True, color=\"light_grey\")\nedf.make_borders()\nexcel.close_out_book(wb)\n\n\nThis package is a work in progress, so feel free to let me know if you have any ideas of how the design could be improved!"
  },
  {
    "objectID": "posts/excel-wings/index.html#footnotes",
    "href": "posts/excel-wings/index.html#footnotes",
    "title": "Using The Excel Object Model in Python",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDealing with the Application object in xlwings is optional, and the workflow outlined in this blog just ignores it.↩︎\nExcel indexes from 1 whereas python indexes from 0, so adding 1 will be common in a lot of these workflows↩︎\nI found out about these pages through this excellent stackoverflow answer↩︎"
  },
  {
    "objectID": "posts/covariance-matrices/index.html",
    "href": "posts/covariance-matrices/index.html",
    "title": "The Covariance Matrix from Scratch",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nimport ucimlrepo\nimport sympy as sp\nimport matplotlib.pyplot as plt\nThe following is an overview of how to construct a covariance matrix. The post is largely based on exercises in (Cohen 2022), an excellent overview of applied, numerical linear algebra in python. I overview how to construct a covariance matrix, but in the process I try to touch on a lot of important concepts in fundamental linear algebra: matrix multiplication, matrix transformations, the centering matrix, and getting from the idea of a matrix operation to the formal notation.\nI want to stress that the methods and procedures that I overview here are examples of analytical linear algebra. I’m looking at a lot of pen and paper methods for defining operations. In numerical linear algebra, where we actually compute this stuff on data, methodology is different. If you look into how numpy computes covariance, it’s not going to look anything like this. However, learning these analytical fundamentals has value – both in making it easier to read linear algebra heavy textbooks, and in being able to write out operations and ideas in parsimonious mathematical statements."
  },
  {
    "objectID": "posts/covariance-matrices/index.html#the-summation-formulation",
    "href": "posts/covariance-matrices/index.html#the-summation-formulation",
    "title": "The Covariance Matrix from Scratch",
    "section": "The summation formulation",
    "text": "The summation formulation\nSay we have two columns, population and householdsize, and we’d like to know to what degree they are associated. I’ll refer to these as x and y, respectively.\n\nx = example['population']\ny = example['householdsize']\n\nI can compute their covariance statistic via the following equation, where \\(x\\) is population, and \\(y\\) is householdsize\n\\[\nc_{x, y} = (n-1)^{-1} \\sum_{i=1} ^n (x_i - \\bar{x})(y_i - \\bar{y})\n\\tag{1}\\] (Cohen 2022, chap. 7)\nI’ll create a function that makes the equation operational for any two arrays, x and y:\n\ndef bivariate_cov(x: np.array, y: np.array) -&gt; float:\n    x_bar: float = x.mean()\n    y_bar: float = y.mean()\n    n = len(x)\n    summation = 0\n    for i in range(n):\n        summation += (x.loc[i] - x_bar) * (y.loc[i] - y_bar)\n    return summation / (n-1)\n\nFor population and householdsize the covariance statistic is:\n\nprint(\"cov(x, y):\", bivariate_cov(x, y))\n\ncov(x, y): -0.0020099999999999996\n\n\nI’ll check my answer with a built in function, numpy.cov():\n\nnp.cov(x, y)\n\narray([[ 0.00657, -0.00201],\n       [-0.00201,  0.05293]])\n\n\nThis is clearly different! Rather than a scalar, this function returned a matrix. However, the upper and lower triangles of the matrix are the same as the covariance value we computed. Indeed, what numpy returned for covariance is the following matrix: \\[\nC =\n\\left[\n\\begin{matrix}\n    var(x) & cov(x, y) \\\\\n    cov(y, x) & var(y) \\\\\n\\end{matrix}\n\\right]\n\\tag{2}\\]\nWe can compute the variances of \\(x\\) and \\(y\\) to confirm this (note that variance is a special case of covariance where the x and y input are equal)\n\nprint(\"var(x):\", bivariate_cov(x, x))\nprint(\"var(y):\", bivariate_cov(y, y))\n\nvar(x): 0.006570000000000002\nvar(y): 0.05293000000000001\n\n\nBut how do we get the linear algebra formula that numpy used to produce that matrix, \\(C\\)?"
  },
  {
    "objectID": "posts/covariance-matrices/index.html#the-vector-formulation",
    "href": "posts/covariance-matrices/index.html#the-vector-formulation",
    "title": "The Covariance Matrix from Scratch",
    "section": "The vector formulation",
    "text": "The vector formulation\nTo get our output to match numpy’s we’ll start by generalizing the covariance equation to input vectors. Recall the summation equation:\n\\[\n\\begin{align*}\nc_{x, y} &= (n-1)^{-1} \\sum_{i=1} ^n (x_i - \\bar{x})(y_i - \\bar{y}) \\\\\n\\end{align*}  \n\\]\nTo convert this into an equation that uses vectors, we’ll note that the \\(\\sum_{i=1}^n(\\cdots)_i(\\cdots)_i\\) term above corresponds to a “dot product” in linear algebra. We write this out as follows: \\[\n\\begin{align*}\nc_{x, y} &= (n-1)^{-1}(\\mathbf {x}_{n \\times 1} - \\bar{\\mathbf {x}})^\\intercal (\\mathbf {y}_{n \\times 1} - \\bar{\\mathbf {y}}) \\\\\n\\end{align*}  \n\\]\n(Cohen 2022, chap. 7)\nNote the following notational conventions:\n\nthe boldface x, \\(\\mathbf {x}_{n \\times 1}\\) refers to a vector with dimensions \\(n \\times 1\\) and all the elements \\(x_i\\), \\(i \\in [1,2, \\cdots, n]\\)\n\\(\\bar{\\mathbf {x}}\\) and \\(\\bar{x}\\) are equivalent – both represent the mean of the vector.\n\\(^\\intercal\\) represents the transpose. The short of this is that \\((\\mathbf {x}_{n \\times 1} - \\bar{\\mathbf {x}})\\) is an \\(n \\times 1\\) vector. When we transpose it, it becomes a \\(1 \\times n\\) vector, which is necessary for this multiplication to work. I’m going to go into detail on this a little later, but for now just know that transposing a vector switches its row and column dimensions. For example, here we convert a column, \\(n \\times 1\\) vector into a row, \\(1 \\times n\\) vector:\n\n\\[\n\\left[\\begin{matrix}x_1\\\\x_2\\\\x_3\\\\x_4\\\\x_5\\end{matrix}\\right]_{n \\times 1}^\\intercal = \\left[\\begin{matrix}x_1&x_2&x_3&x_4&x_5\\end{matrix}\\right]_{1 \\times n}\n\\]\nRather than overview every rule of linear algebra operations now, I’ll compute the covariance of the two variables in our dataset and we can directly examine how basic vector-scalar and vector-vector operations come out.\nI’ll denote the vectors and their means below:\n\nprint(\"x:\", x.values, \"\\nmean(x)=\", x.mean())\nprint(\"y:\", y.values,  \"\\nmean(y)=\", y.mean())\nprint(\"n = \", len(x))\n\nx: [0.19 0.   0.   0.04 0.01] \nmean(x)= 0.048\ny: [0.33 0.16 0.42 0.77 0.55] \nmean(y)= 0.446\nn =  5\n\n\nAnd we can now plug everything into that vector formula for covariance:\n\\[\n\\begin{align*}\nc_{x, y} &= (n-1)^{-1}(\\mathbf {x}_{n \\times 1} - \\bar{\\mathbf {x}})^\\intercal (\\mathbf {y}_{n \\times 1} - \\bar{\\mathbf {y}}) \\\\\n&= \\frac{1}{4} (\\left[\\begin{matrix}0.19\\\\0\\\\0\\\\0.04\\\\0.01\\end{matrix}\\right] - 0.048)^\\intercal (\\left[\\begin{matrix}0.33\\\\0.16\\\\0.42\\\\0.77\\\\0.55\\end{matrix}\\right] - 0.446) \\\\\n&= \\frac{1}{4} (\\left[\\begin{matrix}0.142\\\\-0.048\\\\-0.048\\\\-0.008\\\\-0.038\\end{matrix}\\right]^\\intercal \\left[\\begin{matrix}-0.116\\\\-0.286\\\\-0.026\\\\0.324\\\\0.104\\end{matrix}\\right]) \\\\\n&= \\frac{1}{4} ((0.142) (-0.116) + (-0.048) (-0.286) \\\\\n&\\quad \\quad + (-0.048) (-0.026) + (-0.008) (0.324) \\\\\n&\\quad \\quad + (-0.038) (0.104) )  \\\\\n&\\approx \\frac{1}{4} (-0.00804) \\\\\n&\\approx -0.00201\n\\end{align*}\n\\]\nIn python we can confirm our result, where @ computes the dot product in python’s numpy library:\n\nx_bar: float = x.mean()\ny_bar: float = y.mean()\nn = len(x)\n((x - x_bar).T @ (y - y_bar)) / (n - 1)\n\n-0.0020099999999999996"
  },
  {
    "objectID": "posts/covariance-matrices/index.html#deriving-the-matrix-formulation",
    "href": "posts/covariance-matrices/index.html#deriving-the-matrix-formulation",
    "title": "The Covariance Matrix from Scratch",
    "section": "Deriving the matrix formulation",
    "text": "Deriving the matrix formulation\nThe vector formulation is not particularly useful, so we will quickly move on to the matrix formulation, which will produce output equivalent to np.cov().\nBefore we get to the equation, we’ll start by defining a matrix, \\(X\\), made up of the vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\):\n\\[\nX_{n \\times 2} =\n\\left[\n\\begin{matrix}\n\\vert & \\vert \\\\\n    \\mathbf{x}   & \\mathbf{y}   \\\\\n    \\vert & \\vert\n\\end{matrix}\n\\right]_{n \\times 2} = \\left[\n\\begin{matrix}\n    x_1   & y_1   \\\\\n    x_2   & y_2   \\\\\n    \\vdots   & \\vdots   \\\\\n    x_n   & y_n   \\\\\n\\end{matrix}\n\\right]_{n \\times 2}\n\\]\nThis is really just our full dataset:\n\nX = example[['population', 'householdsize']]\nX\n\n\n\n\n\n\n\n\npopulation\nhouseholdsize\n\n\n\n\n0\n0.19\n0.33\n\n\n1\n0.00\n0.16\n\n\n2\n0.00\n0.42\n\n\n3\n0.04\n0.77\n\n\n4\n0.01\n0.55\n\n\n\n\n\n\n\nNow we want to create a currently unknown operation that will take in this matrix, \\(X\\), and output the covariance matrix, (Equation 2). In thinking about this, lets decompose what exactly that covariance matrix is, incorporating the simple summation formula, then the vector formula we just learned:\n\\[\n\\begin{align*}\n???(X) &=\n\\left[\n\\begin{matrix}\n    var(x) & cov(x, y) \\\\\n    cov(y, x) & var(y) \\\\\n\\end{matrix}\n\\right] \\\\\n&=\\left[\n\\begin{matrix}\n\\sum_{i=1}^{n} (x_i - \\bar{\\mathbf{x}})(x_i - \\bar{\\mathbf{x}}) & \\sum_{i=1}^{n} (x_i - \\bar{\\mathbf{x}})(y_i - \\bar{\\mathbf{y}}) \\\\\n\\sum_{i=1}^{n} (y_i - \\bar{\\mathbf{y}})(x_i - \\bar{\\mathbf{x}}) & \\sum_{i=1}^{n} (y_i - \\bar{\\mathbf{y}})(y_i - \\bar{\\mathbf{y}}) \\\\\n\\end{matrix}\n\\right] (n-1)^{-1}\\\\\n&= \\left[\n\\begin{matrix}\n(\\mathbf {x} - \\bar{\\mathbf {x}})^\\intercal (\\mathbf {x} - \\bar{\\mathbf {x}}) & (\\mathbf {x} - \\bar{\\mathbf {x}})^\\intercal (\\mathbf {y} - \\bar{\\mathbf {y}}) \\\\\n(\\mathbf {y} - \\bar{\\mathbf {y}})^\\intercal (\\mathbf {x} - \\bar{\\mathbf {x}}) & (\\mathbf {y} - \\bar{\\mathbf {y}})^\\intercal (\\mathbf {y} - \\bar{\\mathbf {y}}) \\\\\n\\end{matrix}\n\\right] (n-1)^{-1}\n\\end{align*}\n\\] Set aside the value, \\((n-1)^{-1}\\) for now, and just consider what this matrix of dot products is made of.\nI defer to the wikipedia entry on matrix-matrix multiplication to define the exact process, but consider that when you multiply two matrices, each entry in the output matrix is the dot product of the \\(i\\) th row of the left matrix and the \\(j\\) th column of the right matrix. Given that definition and the fact that our output above is a matrix of dot products, lets define it as the output of a multiplication operation between some currently unknown left and right matrix. Specifically:\n\\[\n\\begin{align*}\n\\left[\n\\begin{matrix}\n(\\mathbf {x} - \\bar{\\mathbf {x}})^\\intercal (\\mathbf {x} - \\bar{\\mathbf {x}}) & (\\mathbf {x} - \\bar{\\mathbf {x}})^\\intercal (\\mathbf {y} - \\bar{\\mathbf {y}}) \\\\\n(\\mathbf {y} - \\bar{\\mathbf {y}})^\\intercal (\\mathbf {x} - \\bar{\\mathbf {x}}) & (\\mathbf {y} - \\bar{\\mathbf {y}})^\\intercal (\\mathbf {y} - \\bar{\\mathbf {y}}) \\\\\n\\end{matrix}\n\\right] &= \\\\\n\\left[\n\\begin{matrix}\n(\\text{Row 1, Left Matrix}) \\cdot (\\text{Col 1, Right Matrix}) & (\\text{Row 1, Left Matrix}) \\cdot (\\text{Col 2, Right Matrix})  \\\\\n(\\text{Row 2, Left Matrix}) \\cdot (\\text{Col 1, Right Matrix}) & (\\text{Row 2, Left Matrix}) \\cdot (\\text{Col 2, Right Matrix})\\\\\n\\end{matrix}\n\\right]\n\\end{align*}\n\\]\nWe can use the equivalence of these two statements to define what the rows and columns are for the left and right matrix:\n\\[\n\\begin{align*}\nC &= \\left[\n\\begin{matrix}\n    —&(\\text{Row 1})  &—   \\\\\n    —& (\\text{Row 2}) &—\n\\end{matrix}\n\\right]\n\\left[\n\\begin{matrix}\n\\vert & \\vert \\\\\n    (\\text{Col 1})   & (\\text{Col 2})   \\\\\n    \\vert & \\vert\n\\end{matrix}\n\\right] (n-1)^{-1}\\\\\n&=\n\\left[\n\\begin{matrix}\n    —&(\\mathbf{x}-\\bar{\\mathbf{x}})  &—   \\\\\n    —& (\\mathbf{y}-\\bar{\\mathbf{y}}) &—\n\\end{matrix}\n\\right]\n\\left[\n\\begin{matrix}\n\\vert & \\vert \\\\\n    (\\mathbf{x}-\\bar{\\mathbf{x}})   & (\\mathbf{y}-\\bar{\\mathbf{y}})   \\\\\n    \\vert & \\vert\n\\end{matrix}\n\\right] (n-1)^{-1}\n\\end{align*}\n\\] This is a good time to explain how a matrix transpose works. We can see that the rows 1 and 2 of the left matrix are the same as the columns 1 and 2 of the right matrix. This means that the left matrix is a transpose of the right matrix (or vice versa).\n\nI think it’s […] easy to remember that transposing swaps rows and columns\n– (Cohen 2022, chap. 5)\n\n\\[\n\\left[\n\\begin{matrix}\n\\vert & \\vert \\\\\n    (\\mathbf{x}-\\bar{\\mathbf{x}})   & (\\mathbf{y}-\\bar{\\mathbf{y}})   \\\\\n    \\vert & \\vert\n\\end{matrix}\n\\right]^\\intercal =\n\\left[\n\\begin{matrix}\n    —&(\\mathbf{x}-\\bar{\\mathbf{x}})  &—   \\\\\n    —& (\\mathbf{y}-\\bar{\\mathbf{y}}) &—\n\\end{matrix}\n\\right]\n\\]\nWhen we plug that in, we get the typical formula for covariance.\n\\[\n\\begin{align*}\nC &=\n\\left[\n\\begin{matrix}\nx_1 - \\bar{\\mathbf{x}}   & y_1 - \\bar{\\mathbf{y}}   \\\\\n    x_2 - \\bar{\\mathbf{x}}   & y_2 - \\bar{\\mathbf{y}}   \\\\\n    \\vdots   & \\vdots   \\\\\n    x_n - \\bar{\\mathbf{x}}   & y_n - \\bar{\\mathbf{y}}   \\\\\n\\end{matrix}\n\\right]_{n \\times 2}^\\intercal\n\\left[\n\\begin{matrix}\nx_1 - \\bar{\\mathbf{x}}   & y_1 - \\bar{\\mathbf{y}}   \\\\\n    x_2 - \\bar{\\mathbf{x}}   & y_2 - \\bar{\\mathbf{y}}   \\\\\n    \\vdots   & \\vdots   \\\\\n    x_n - \\bar{\\mathbf{x}}   & y_n - \\bar{\\mathbf{y}}   \\\\\n\\end{matrix}\n\\right]_{n \\times 2} (n-1)^{-1}\n\\end{align*}\n\\tag{3}\\]\nSo now we know the matrix operation that will yield the covariance matrix, \\(C\\), from the matrix, \\(X\\) – we find the mean-centered representation of \\(X\\), multiply that with itself, and then multiply the result by the scalar, \\((n-1)^{-1}\\). This is a fairly straightforward operation, but it’s not really an equation yet. We will now formalize it into one.\n\nBuilding the centering matrix\nWe want to subtract the mean of x from the x elements, and the mean of y from the y elements, all in one matrix. In the vector setting above, we set this up as the difference between a vector and its scalar mean. Vector-scalar operations are intuitive in linear algebra:\n\\[\n\\left[\n\\begin{matrix}\n    x_1  \\\\\n    x_2   \\\\\n    \\vdots   \\\\\n    x_n\n\\end{matrix}\n\\right]_{n \\times 1} - \\bar{\\mathbf{x}} = \\left[\n\\begin{matrix}\nx_1 - \\bar{\\mathbf{x}}   \\\\\n    x_2 - \\bar{\\mathbf{x}}    \\\\\n    \\vdots   \\\\\n    x_n - \\bar{\\mathbf{x}}     \\\\\n\\end{matrix}\n\\right]_{n \\times 1}\n\\, \\text{ and } \\,\n\\left[\n\\begin{matrix}\n    y_1  \\\\\n    y_2   \\\\\n    \\vdots   \\\\\n    y_n\n\\end{matrix}\n\\right]_{n \\times 1} - \\bar{\\mathbf{y}} = \\left[\n\\begin{matrix}\ny_1 - \\bar{\\mathbf{y}}   \\\\\n    y_2 - \\bar{\\mathbf{y}}    \\\\\n    \\vdots   \\\\\n    y_n - \\bar{\\mathbf{y}}     \\\\\n\\end{matrix}\n\\right]_{n \\times 1}\n\\]\nIn a matrix formulation, this is slightly different.\n\nYou add [or subtract] two matrices by adding their corresponding elements. […]\nMatrix addition [and subtraction] is defined only between two matrices of the same size.\n– (Cohen 2022, chap. 5)\n\nTherefore, we would set \\(x_i - \\bar{x}\\) and \\(y_i - \\bar{y}\\) up in a matrix formulation as follows:\n\\[  \n\\left[\n\\begin{matrix}\n    x_1   & y_1   \\\\\n    x_2   & y_2   \\\\\n    \\vdots   & \\vdots   \\\\\n    x_n   & y_n   \\\\\n\\end{matrix}\n\\right]_{n \\times 2} -\n\\left[\n\\begin{matrix}\n    \\bar{\\mathbf{x}}   & \\bar{\\mathbf{y}}   \\\\\n    \\bar{\\mathbf{x}}   & \\bar{\\mathbf{y}}   \\\\\n    \\vdots   & \\vdots   \\\\\n    \\bar{\\mathbf{x}}   & \\bar{\\mathbf{y}}   \\\\\n\\end{matrix}\n\\right]_{n \\times 2} =\n\\left[\n\\begin{matrix}\nx_1 - \\bar{\\mathbf{x}}   & y_1 - \\bar{\\mathbf{y}}   \\\\\n    x_2 - \\bar{\\mathbf{x}}   & y_2 - \\bar{\\mathbf{y}}   \\\\\n    \\vdots   & \\vdots   \\\\\n    x_n - \\bar{\\mathbf{x}}   & y_n - \\bar{\\mathbf{y}}   \\\\\n\\end{matrix}\n\\right]_{n \\times 2}\n\\]\nBut how do we produce the \\(n \\times 2\\) matrix of column means that this equation requires?\n\\[\n??? = \\left[\n\\begin{matrix}\n    \\bar{\\mathbf{x}}   & \\bar{\\mathbf{y}}   \\\\\n    \\bar{\\mathbf{x}}   & \\bar{\\mathbf{y}}   \\\\\n    \\vdots   & \\vdots   \\\\\n    \\bar{\\mathbf{x}}   & \\bar{\\mathbf{y}}   \\\\\n\\end{matrix}\n\\right]_{n \\times 2}\n\\tag{4}\\]\nGiven that these means are functions of the elements of \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) respectively, we are going to try to create a function that takes in the matrix \\(X\\) and outputs the matrix of means above, (Equation 4).\n\nThe mean as a matrix transformation\nTo build that function, lets start by considering a standard definition of the average and translate it into a “matrix transformation”. The simple summation formulation for the mean is as follows:\n\\[\n\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i = \\sum_{i=1}^n \\frac{1}{n}x_i\n\\] The summation, \\(\\sum_{i=1}^n \\frac{1}{n}x_i\\), maps directly to a simple dot product:\n\\[\n\\begin{align*}\n\\bar{x} &= \\sum_{i=1}^n \\frac{1}{n}x_i \\\\\n&= \\frac{1}{n}x_1 + \\frac{1}{n}x_2 + \\cdots \\frac{1}{n}x_n \\\\\n&= \\left[\\begin{matrix}\\frac{1}{n} & \\frac{1}{n} & \\cdots & \\frac{1}{n}\\end{matrix}\\right]_{1 \\times n} \\left[\\begin{matrix}x_1\\\\x_2\\\\\\vdots\\\\x_n\\end{matrix}\\right]_{n \\times 1}\n\\end{align*}\n\\] This is interesting (really), because it looks like if we multiply \\(\\left[\\begin{matrix}\\frac{1}{n} & \\frac{1}{n} & \\cdots & \\frac{1}{n}\\end{matrix}\\right]_{1 \\times n}\\) by any \\(n \\times 1\\) vector, we get the mean. So it seems like the following is a function that takes in a vector, multiplies it with another vector, and outputs the average of the input vector: \\[\n\\bar{x} = f(\\mathbf{x}_{n \\times 1}) = \\left[\\begin{matrix}\\frac{1}{n} & \\frac{1}{n} & \\cdots & \\frac{1}{n}\\end{matrix}\\right]_{1 \\times n} \\mathbf{x}_{n \\times 1}\n\\tag{5}\\]\nLets confirm this behavior with code:\n\nprint(\"n =\", len(x))\nprint(\"[1/n, 1/n, ... 1/n] = \", 1/len(x) * np.ones(len(x)).T)\nprint(\"x = \", x.values)\nprint()\nprint(\n    \"[1/n, 1/n, ... 1/n] @ x = \", round(1/len(x) * np.ones(len(x)).T @ x, 5)\n    )\nprint(\"mean = \", x.mean())\nprint(\"^ these are equal\")\n\nn = 5\n[1/n, 1/n, ... 1/n] =  [0.2 0.2 0.2 0.2 0.2]\nx =  [0.19 0.   0.   0.04 0.01]\n\n[1/n, 1/n, ... 1/n] @ x =  0.048\nmean =  0.048\n^ these are equal\n\n\nNow that we’ve confirmed that the function works, lets create formal notation for it. Consider the vector of 1s, \\(\\mathbf{1}\\):\n\\[\n\\mathbf{1}_{n \\times 1} = \\left[\\begin{matrix}1\\\\1\\\\\\vdots\\\\1\\end{matrix}\\right]_{n \\times 1} \\\\\n\\] If we mutliply that by \\(1/n\\) and transpose the result, we get the vector from our function, (Equation 5): \\[\nn^{-1}\\mathbf{1}_{n \\times 1}^\\intercal = \\left[\\begin{matrix}\\frac{1}{n} & \\frac{1}{n} & \\cdots & \\frac{1}{n}\\end{matrix}\\right]_{1 \\times n}\n\\]\nWith that notation, we’ll now redefine the function as the matrix transformation, \\(T_\\text{mean}\\), that transforms a \\(n \\times 1\\) matrix (this is just a vector) into its scalar mean. \\[\nT_\\text{mean}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^1 \\quad \\text{defined by} \\quad T(\\mathbf{x}_{n \\times 1}) = n^{-1}\\mathbf{1}_{n \\times 1}^\\intercal \\mathbf{x}_{n \\times 1}\n\\]\nLooks weird and potentially useless, but we’ll use this same approach to define notation for the matrix of deviations from the mean from (Equation 3)\n\n\nDeviation from the mean as a matrix transformation\n\\(T_\\text{mean}\\) yields a scalar, \\(\\bar{x}\\), and it could also easily be used to find \\(\\bar{y}\\), but I want the matrix: \\[\n\\left[\n\\begin{matrix}\n    \\bar{\\mathbf{x}}   & \\bar{\\mathbf{y}}   \\\\\n    \\bar{\\mathbf{x}}   & \\bar{\\mathbf{y}}   \\\\\n    \\vdots   & \\vdots   \\\\\n    \\bar{\\mathbf{x}}   & \\bar{\\mathbf{y}}   \\\\\n\\end{matrix}\n\\right]_{n \\times 2}\n\\] To move towards that output, an \\(n \\times 2\\) matrix, lets consider some points about matrix multiplication.\n\n\nAn aside about matrix multiplication rules\nIn matrix multiplication, the following properties hold with regards to dimensions of the input and output:\n\nOperations are only valid if the column dimension of the left input matrix matches the row dimension of the right input matrix.\nThe output has the row dimension of the left matrix and the column dimension of the right matrix.\n\n– (Cohen 2022, chap. 5)\nFor example, in all of our previous vector \\(\\times\\) vector operations, our dimensionality was as follows:\n\\[\n\\begin{align*}\n(1 \\times \\fbox{n})(\\fbox{n} \\times 1) \\quad& (n = n) \\rightarrow \\text{Operation is valid} \\\\\n(\\fbox{1} \\times n)(n \\times \\fbox{1}) \\quad& (1 \\times 1) \\rightarrow \\text{Output is scalar}\n\\end{align*}\n\\] (Sometimes we had to transpose one of the vectors for this to be the case).\nIf we want to produce an \\(n \\times 2\\) matrix from a multiplication operation involving an \\(n \\times 2\\) input (our data matrix, \\(X\\)), we would need a matrix with the following, underlined dimensionality:\n\\[\n\\underline{(n \\times n)} (n \\times 2) \\rightarrow \\text{Valid with output: } (n \\times 2)\n\\] Upon some reflection, the \\(n \\times n\\) matrix must be the matrix where every element is \\(1/n\\). The following operation gives us the desired matrix with all elements set as the original column mean: \\[\n\\left[\n\\begin{matrix}\n    \\frac{1}{n}   & \\frac{1}{n}   & \\cdots & \\frac{1}{n}\\\\\n    \\frac{1}{n}   & \\frac{1}{n}   & \\cdots & \\frac{1}{n}\\\\\n    \\vdots   & \\vdots   & \\cdots & \\frac{1}{n}\\\\\n    \\frac{1}{n}   & \\frac{1}{n}   & \\cdots & \\frac{1}{n}\n\\end{matrix}\n\\right]_{n \\times n}\n\\left[\n\\begin{matrix}\n    x_1   & y_1   \\\\\n    x_2   & y_2   \\\\\n    \\vdots   & \\vdots   \\\\\n    x_n   & y_n   \\\\\n\\end{matrix}\n\\right]_{n \\times 2} =\n\\left[\n\\begin{matrix}\n    \\sum_{i=1}^n \\frac{x_i}{n}   & \\sum_{i=1}^n \\frac{y_i}{n}   \\\\\n    \\sum_{i=1}^n \\frac{x_i}{n}   & \\sum_{i=1}^n \\frac{y_i}{n}  \\\\\n    \\vdots   & \\vdots   \\\\\n    \\sum_{i=1}^n \\frac{x_i}{n}   & \\sum_{i=1}^n \\frac{y_i}{n}   \\\\\n\\end{matrix}\n\\right]_{n \\times 2}\n= \\left[\n\\begin{matrix}\n    \\bar{\\mathbf{x}}   & \\bar{\\mathbf{y}}   \\\\\n    \\bar{\\mathbf{x}}   & \\bar{\\mathbf{y}}   \\\\\n    \\vdots   & \\vdots   \\\\\n    \\bar{\\mathbf{x}}   & \\bar{\\mathbf{y}}   \\\\\n\\end{matrix}\n\\right]_{n \\times 2}\n\\]\n\n\nNotation of the transformation\nWe will need that matrix of \\(1/n\\) values in our covariance matrix equation. We’ll now formalize the construction of that matrix so that we can write it in an equation, much as we did for the scalar mean in the previous section.\nWe can construct a square matrix of 1s by multiplying the 1s vector with its transpose, \\((n \\times 1)(1 \\times n) \\rightarrow (n \\times n)\\). Then we can multiply the matrix by \\(1/n\\), which will apply to each element in the matrix:\n\\[\n\\begin{align*}\nn^{-1} \\mathbf{1}_{n \\times 1} \\mathbf{1}_{n \\times 1}^\\intercal &= n^{-1} \\left[\\begin{matrix}1\\\\1\\\\\\vdots\\\\1\\end{matrix}\\right]_{n \\times 1} \\left[\\begin{matrix}1 & 1 & \\cdots & 1\\end{matrix}\\right]_{1 \\times n} \\\\\n&= n^{-1} \\left[\n\\begin{matrix}\n    1   & 1   & \\cdots & 1\\\\\n    1   & 1   & \\cdots & 1\\\\\n    \\vdots   & \\vdots   & \\cdots & 1\\\\\n    1   & 1   & \\cdots & 1\n\\end{matrix}\n\\right]_{n \\times n} \\\\\n&= \\left[\n\\begin{matrix}\n    \\frac{1}{n}   & \\frac{1}{n}   & \\cdots & \\frac{1}{n}\\\\\n    \\frac{1}{n}   & \\frac{1}{n}   & \\cdots & \\frac{1}{n}\\\\\n    \\vdots   & \\vdots   & \\cdots & \\frac{1}{n}\\\\\n    \\frac{1}{n}   & \\frac{1}{n}   & \\cdots & \\frac{1}{n}\n\\end{matrix}\n\\right]_{n \\times n}\n\\end{align*}\n\\]\nThus the transformation that creates the “matrix of means,” (Equation 4) is \\(n^{-1} \\mathbf{1}_{n \\times 1} \\mathbf{1}_{n \\times 1}^\\intercal\\) applied to an input matrix, \\(X\\):\n\\[\n\\begin{align*}\nn^{-1} \\mathbf{1}_{n \\times 1} \\mathbf{1}_{n \\times 1}^\\intercal X_{n \\times 2} &= \\left[\n\\begin{matrix}\n    \\frac{1}{n}   & \\frac{1}{n}   & \\cdots & \\frac{1}{n}\\\\\n    \\frac{1}{n}   & \\frac{1}{n}   & \\cdots & \\frac{1}{n}\\\\\n    \\vdots   & \\vdots   & \\cdots & \\frac{1}{n}\\\\\n    \\frac{1}{n}   & \\frac{1}{n}   & \\cdots & \\frac{1}{n}\n\\end{matrix}\n\\right]_{n \\times n}\n\\left[\n\\begin{matrix}\n    x_1   & y_1   \\\\\n    x_2   & y_2   \\\\\n    \\vdots   & \\vdots   \\\\\n    x_n   & y_n   \\\\\n\\end{matrix}\n\\right]_{n \\times 2} \\\\\n&= \\left[\n\\begin{matrix}\n    \\sum_{i=1}^n \\frac{x_i}{n}   & \\sum_{i=1}^n \\frac{y_i}{n}   \\\\\n    \\sum_{i=1}^n \\frac{x_i}{n}   & \\sum_{i=1}^n \\frac{y_i}{n}  \\\\\n    \\vdots   & \\vdots   \\\\\n    \\sum_{i=1}^n \\frac{x_i}{n}   & \\sum_{i=1}^n \\frac{y_i}{n}   \\\\\n\\end{matrix}\n\\right]_{n \\times 2}  \\\\\n&= \\left[\n\\begin{matrix}\n    \\bar{\\mathbf{x}}   & \\bar{\\mathbf{y}}   \\\\\n    \\bar{\\mathbf{x}}   & \\bar{\\mathbf{y}}   \\\\\n    \\vdots   & \\vdots   \\\\\n    \\bar{\\mathbf{x}}   & \\bar{\\mathbf{y}}   \\\\\n\\end{matrix}\n\\right]_{n \\times 2}\n\\end{align*}\n\\]\nFinally, we’ll use our “matrix of means” to yield the original matrix of interest used in (Equation 3), where each element is the deviation from its column mean:\n\\[\n\\begin{align*}\n\\left[\n\\begin{matrix}\nx_1 - \\bar{\\mathbf{x}}   & y_1 - \\bar{\\mathbf{y}}   \\\\\n    x_2 - \\bar{\\mathbf{x}}   & y_2 - \\bar{\\mathbf{y}}   \\\\\n    \\vdots   & \\vdots   \\\\\n    x_n - \\bar{\\mathbf{x}}   & y_n - \\bar{\\mathbf{y}}   \\\\\n\\end{matrix}\n\\right]_{n \\times 2} &=\n\\left[\n\\begin{matrix}\n    x_1   & y_1   \\\\\n    x_2   & y_2   \\\\\n    \\vdots   & \\vdots   \\\\\n    x_n   & y_n   \\\\\n\\end{matrix}\n\\right]_{n \\times 2} -\n\\left[\n\\begin{matrix}\n    \\bar{\\mathbf{x}}   & \\bar{\\mathbf{y}}   \\\\\n    \\bar{\\mathbf{x}}   & \\bar{\\mathbf{y}}   \\\\\n    \\vdots   & \\vdots   \\\\\n    \\bar{\\mathbf{x}}   & \\bar{\\mathbf{y}}   \\\\\n\\end{matrix}\n\\right]_{n \\times 2}  \\\\\n&= X_{n \\times m} -  n^{-1} \\mathbf{1}_{n \\times 1} \\mathbf{1}_{n \\times 1}^\\intercal X_{n \\times m} \\\\\n&=  (I_{n \\times n} -  n^{-1} \\mathbf{1}_{n \\times 1} \\mathbf{1}_{n \\times 1}^\\intercal ) X_{n \\times m}\n\\end{align*}\n\\] (note the identity matrix, \\(I\\), which, in matrix multiplication, acts like a \\(1\\) would in scalar multiplication)\nAt this point, we will discard some of the dimension information about the matrices, which I’ve previously been using as guardrails, and simplify notation as follows: \\[\n\\begin{align*}\n  &= (I -  n^{-1} \\mathbf{1}_{n} \\mathbf{1}_{n}^\\intercal ) X\n\\end{align*}\n\\]\nThis yields the function for mean centering the matrix, commonly referred to as the centering matrix:\n\\[\nT_{\\text{center}}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n \\quad \\text{defined by} \\quad T(X) = (I -  n^{-1} \\mathbf{1}_{n} \\mathbf{1}_{n}^\\intercal ) X\n\\]\nIn code:\n\ndef T_center(X: np.array) -&gt; np.array:\n    n = len(X)\n    return (np.identity(n) - (1/n)*np.ones(shape=(n, n))) @ X\n\n\n\n\nA note on broadcasting and numerical methods\nnumpy and pandas support some invalid linear algebra operations, such as matrix addition/subtraction between different sized matrices (these packages use a method called broadcasting to make the operations valid):\n\n\nBroadcasting a times b\n\n\n\nThis makes the sort of centering operation we just covered in depth incredibly simple:\n\nX - X.mean()\n\n\n\n\n\n\n\n\npopulation\nhouseholdsize\n\n\n\n\n0\n0.142\n-0.116\n\n\n1\n-0.048\n-0.286\n\n\n2\n-0.048\n-0.026\n\n\n3\n-0.008\n0.324\n\n\n4\n-0.038\n0.104\n\n\n\n\n\n\n\nWe can confirm that our function produced that same centered matrix:\n\nT_center(X)\n\n\n\n\n\n\n\n\npopulation\nhouseholdsize\n\n\n\n\n0\n0.142\n-0.116\n\n\n1\n-0.048\n-0.286\n\n\n2\n-0.048\n-0.026\n\n\n3\n-0.008\n0.324\n\n\n4\n-0.038\n0.104\n\n\n\n\n\n\n\nAt this point we will return to the matrix equation for covariance, (Equation 3) that we defined earlier: \\[\n\\begin{align*}\nC = (n-1)^{-1} \\left[\n\\begin{matrix}\nx_1 - \\bar{\\mathbf{x}}   & y_1 - \\bar{\\mathbf{y}}   \\\\\n    x_2 - \\bar{\\mathbf{x}}   & y_2 - \\bar{\\mathbf{y}}   \\\\\n    \\vdots   & \\vdots   \\\\\n    x_n - \\bar{\\mathbf{x}}   & y_n - \\bar{\\mathbf{y}}   \\\\\n\\end{matrix}\n\\right]_{n \\times 2}^\\intercal\n\\left[\n\\begin{matrix}\nx_1 - \\bar{\\mathbf{x}}   & y_1 - \\bar{\\mathbf{y}}   \\\\\n    x_2 - \\bar{\\mathbf{x}}   & y_2 - \\bar{\\mathbf{y}}   \\\\\n    \\vdots   & \\vdots   \\\\\n    x_n - \\bar{\\mathbf{x}}   & y_n - \\bar{\\mathbf{y}}   \\\\\n\\end{matrix}\n\\right]_{n \\times 2}\n\\end{align*}\n\\]\nWe’ll now express it formally."
  },
  {
    "objectID": "posts/covariance-matrices/index.html#the-matrix-formulation",
    "href": "posts/covariance-matrices/index.html#the-matrix-formulation",
    "title": "The Covariance Matrix from Scratch",
    "section": "The matrix formulation",
    "text": "The matrix formulation\nCovariance is fairly simple to write out once you have the centering matrix, \\(T_{\\text{center}}\\). Our equation is as follows:\n\\[\n\\begin{align*}\nC &= (n - 1)^{-1} T_{\\text{center}}(X)^\\intercal T_{\\text{center}}(X)  \\\\\n&= (n - 1)^{-1}\\left[ (I -  n^{-1} \\mathbf{1}_{n} \\mathbf{1}_{n}^\\intercal ) X \\right]^\\intercal \\left[ (I -  n^{-1} \\mathbf{1}_{n} \\mathbf{1}_{n}^\\intercal ) X \\right]\n\\end{align*}\n\\] This is pretty verbose, and it’s common in textbooks to see some sort of abbreviated version, like the following, \\[\n\\begin{align*}\nC = (n - 1)^{-1} X_c^\\intercal X_c \\\\\n\\text{where }X_c = (I -  n^{-1} \\mathbf{1}_{n} \\mathbf{1}_{n}^\\intercal ) X\n\\end{align*}\n\\tag{6}\\]\n(Cohen 2022, chap. 7)\n\ndef covariance_matrix(X: np.array) -&gt; np.array:\n    n = len(X)\n    # Center matrix\n    X_c = T_center(X)\n    # Compute covariance\n    return 1/(n-1) * X_c.T @ X_c\n\n\nApplying it to data\nWe’ll test this formula out with our data:\n\nX\n\n\n\n\n\n\n\n\npopulation\nhouseholdsize\n\n\n\n\n0\n0.19\n0.33\n\n\n1\n0.00\n0.16\n\n\n2\n0.00\n0.42\n\n\n3\n0.04\n0.77\n\n\n4\n0.01\n0.55\n\n\n\n\n\n\n\nFirst we’ll define the centered matrix, \\(X_c = (I -  n^{-1} \\mathbf{1}_{n} \\mathbf{1}_{n}^\\intercal ) X\\):\n\n\nCode\nC, X_c = sp.symbols(\"C X_c\")\nexp = (sp.UnevaluatedExpr(sp.Matrix(np.identity(n).astype(int))) - sp.UnevaluatedExpr(1/n) * sp.UnevaluatedExpr(sp.Matrix(np.ones(shape=(n, 1)) @ np.ones(shape=(n, 1)).T))) * sp.UnevaluatedExpr(sp.Matrix(X))\nexp_output = (sp.Matrix(\n    np.round((np.identity(n) - (1/n) * np.ones(shape=(n, 1)) @ np.ones(shape=(n, 1)).T) @ X, 3)\n    ))\nsp.Eq(X_c, exp)\n\n\n\\(\\displaystyle X_{c} = \\left(- 0.2 \\left[\\begin{matrix}1.0 & 1.0 & 1.0 & 1.0 & 1.0\\\\1.0 & 1.0 & 1.0 & 1.0 & 1.0\\\\1.0 & 1.0 & 1.0 & 1.0 & 1.0\\\\1.0 & 1.0 & 1.0 & 1.0 & 1.0\\\\1.0 & 1.0 & 1.0 & 1.0 & 1.0\\end{matrix}\\right] + \\left[\\begin{matrix}1 & 0 & 0 & 0 & 0\\\\0 & 1 & 0 & 0 & 0\\\\0 & 0 & 1 & 0 & 0\\\\0 & 0 & 0 & 1 & 0\\\\0 & 0 & 0 & 0 & 1\\end{matrix}\\right]\\right) \\left[\\begin{matrix}0.19 & 0.33\\\\0 & 0.16\\\\0 & 0.42\\\\0.04 & 0.77\\\\0.01 & 0.55\\end{matrix}\\right]\\)\n\n\nWhich evaluates to:\n\n\nCode\nsp.Eq(X_c, sp.UnevaluatedExpr(exp_output))\n\n\n\\(\\displaystyle X_{c} = \\left[\\begin{matrix}0.142 & -0.116\\\\-0.048 & -0.286\\\\-0.048 & -0.026\\\\-0.008 & 0.324\\\\-0.038 & 0.104\\end{matrix}\\right]\\)\n\n\nAnd now the main equation: \\[\nC = (n - 1)^{-1} X_c^\\intercal X_c\n\\]\n\n\nCode\nout = (\n    sp.UnevaluatedExpr(exp_output.T) * sp.UnevaluatedExpr(exp_output.evalf())\n    * 1 / (sp.UnevaluatedExpr(len(x)) - sp.UnevaluatedExpr(1))\n    )\nsp.Eq(C, out)\n\n\n\\(\\displaystyle C = \\left[\\begin{matrix}0.142 & -0.048 & -0.048 & -0.008 & -0.038\\\\-0.116 & -0.286 & -0.026 & 0.324 & 0.104\\end{matrix}\\right] \\left[\\begin{matrix}0.142 & -0.116\\\\-0.048 & -0.286\\\\-0.048 & -0.026\\\\-0.008 & 0.324\\\\-0.038 & 0.104\\end{matrix}\\right] \\left(- 1 + 5\\right)^{-1}\\)\n\n\nAlready it should become apparent that this multiplication will be a sequential application of the vector formula for covariance. We can make that abundantly clear by visualizing the multiplication result:\n\n\nCode\nout = sp.Matrix([[sp.UnevaluatedExpr(sp.Matrix(x-x.mean()).T) * sp.UnevaluatedExpr(sp.Matrix(x-x.mean()))\n                  * 1 / (sp.UnevaluatedExpr(len(x)) - sp.UnevaluatedExpr(1)),\n                  sp.UnevaluatedExpr(sp.Matrix(x-x.mean()).T) * sp.UnevaluatedExpr(sp.Matrix(y-y.mean()))\n                  * 1 / (sp.UnevaluatedExpr(len(x)) - sp.UnevaluatedExpr(1))],\n                  [sp.UnevaluatedExpr(sp.Matrix(y-y.mean()).T) * sp.UnevaluatedExpr(sp.Matrix(x-x.mean()))\n                   * 1 / (sp.UnevaluatedExpr(len(x)) - sp.UnevaluatedExpr(1)),\n                   sp.UnevaluatedExpr(sp.Matrix(y-y.mean()).T) * sp.UnevaluatedExpr(sp.Matrix(y-y.mean()))\n                   * 1 / (sp.UnevaluatedExpr(len(x)) - sp.UnevaluatedExpr(1))]\n                   ])\nout\n\n\n\\(\\displaystyle \\left[\\begin{matrix}\\left[\\begin{matrix}0.142 & -0.048 & -0.048 & -0.008 & -0.038\\end{matrix}\\right] \\left[\\begin{matrix}0.142\\\\-0.048\\\\-0.048\\\\-0.008\\\\-0.038\\end{matrix}\\right] \\left(- 1 + 5\\right)^{-1} & \\left[\\begin{matrix}0.142 & -0.048 & -0.048 & -0.008 & -0.038\\end{matrix}\\right] \\left[\\begin{matrix}-0.116\\\\-0.286\\\\-0.026\\\\0.324\\\\0.104\\end{matrix}\\right] \\left(- 1 + 5\\right)^{-1}\\\\\\left[\\begin{matrix}-0.116 & -0.286 & -0.026 & 0.324 & 0.104\\end{matrix}\\right] \\left[\\begin{matrix}0.142\\\\-0.048\\\\-0.048\\\\-0.008\\\\-0.038\\end{matrix}\\right] \\left(- 1 + 5\\right)^{-1} & \\left[\\begin{matrix}-0.116 & -0.286 & -0.026 & 0.324 & 0.104\\end{matrix}\\right] \\left[\\begin{matrix}-0.116\\\\-0.286\\\\-0.026\\\\0.324\\\\0.104\\end{matrix}\\right] \\left(- 1 + 5\\right)^{-1}\\end{matrix}\\right]\\)\n\n\nThe result:\n\ncovariance_matrix(X.values)\n\narray([[ 0.00657, -0.00201],\n       [-0.00201,  0.05293]])\n\n\nFor a sanity check, we can compare our results with those in numpy and pandas.\nnumpy results:\n\nnp.cov(x, y)\n\narray([[ 0.00657, -0.00201],\n       [-0.00201,  0.05293]])\n\n\npandas results:\n\nX.cov()\n\n\n\n\n\n\n\n\npopulation\nhouseholdsize\n\n\n\n\npopulation\n0.00657\n-0.00201\n\n\nhouseholdsize\n-0.00201\n0.05293\n\n\n\n\n\n\n\n\nVisualizing the Covariance Matrix\nWe’ll now return to our original, full dataset from the very start of the exercise.\n\ndf_num.shape\n\n(1994, 99)\n\n\nand we’ll compute the covariance matrix across al 99 features.\n\ncov_matrix = covariance_matrix(df_num)\n\ncov_matrix.shape\n\n(99, 99)\n\n\nLooking at a \\(99 \\times 99\\) matrix is not particularly useful, so it’s common to use heatmaps to visualize matrices that show association between many variables.\n\nplt.imshow(cov_matrix.values)\nplt.colorbar();\n\n\n\n\n\n\n\n\nWhile this looks cool, I view it as more or less useless as a tool for doing any sort of exploration.\nThe covariances between variables are highly influenced by the scale of the original variables. This matrix visualization be much more useful if we convert it into a correlation matrix, \\(R\\), wherein the measures of association, \\(\\rho_{i,j}\\) are all normalized, \\(\\rho_{i,j} \\in [-1, 1]\\). This in effect adjusts our measures for scale and allows us to actually see which variables are the most associated.\nI’ll cover that matrix in a coming blog post…"
  },
  {
    "objectID": "posts/SF-Arrests/index.html",
    "href": "posts/SF-Arrests/index.html",
    "title": "Law Enforcement Activity in San Francisco, 2018-2024",
    "section": "",
    "text": "In this memo I analyze trends in law enforcement actions – police incident reports that result in arrest or citation – in San Francisco before, during, and after the COVID-19 Pandemic (2018-2024). Enforcement actions decreased before the pandemic, sharply dropped at the outset of the pandemic, and have been consistently low in the post-pandemic.\nEnforcement levels for almost every specific class of crime decreased during the analysis period, except for law enforcement actions related to drug sale, which substantially increased. The vast majority of those increased drug sale enforcement actions took place in the Tenderloin. Indeed, the Tenderloin was an exception to the trend of decreasing enforcement levels in San Francisco – law enforcement actions in the district increased in the post-pandemic, with drug sale enforcement actions driving the increase.\nAn important caveat of this analysis is that it examines enforcement actions, a combination of arrests and citations, not crime. Of all crime events, arrests and citations only include those events that are reported to law enforcement and go on to produce an arrest or citation. These trends do not necessarily point to equal trends in crime, rather they point to trends in how the City and Police Department are prioritizing law enforcement resources, specifically policing. Thus I conclude that in the post-pandemic, the City shifted policing resources towards the Tenderloin and prioritized making arrests and issuing citations for drug sale crime."
  },
  {
    "objectID": "posts/SF-Arrests/index.html#geographic-concentration-in-the-tenderloin",
    "href": "posts/SF-Arrests/index.html#geographic-concentration-in-the-tenderloin",
    "title": "Law Enforcement Activity in San Francisco, 2018-2024",
    "section": "Geographic concentration in the Tenderloin",
    "text": "Geographic concentration in the Tenderloin\nFigure 1 shows that law enforcement actions were geographically concentrated in the Tenderloin district over the six-year analysis period. The left-hand panel depicts the general concentration of enforcement actions in downtown police districts – Northern, Central, Southern, the Tenderloin, and the Mission. However, viewing the geographic trends by district obscures more specific patterns illustrated at a block-level in the right panel. Law enforcement actions along the Northern, Central, and Southern districts’ borders with the Tenderloin accounted for the high numbers in those districts.\n\n\n\n\n\nFigure 1: Geographic Trends in Law Enforcement Actions (2018-2024), by Police District (Left) and by Block Segment (Right)\n\n\n\n\n\n\n\n\nAt the block level, we can specifically classify “the greater Tenderloin,” the area inside of the Tenderloin plus the corners along its border streets,1 as having particularly high law enforcement activity. The greater Tenderloin represents 1.01% of the square miles of San Francisco, but made up 27% of the total law enforcement actions in the City during the analysis period. Of the top ten street corners in San Francisco ranked by total enforcement actions 2018-2024, shown in Figure 2, almost all of them were in the greater Tenderloin.\n\n\n\n\n\nFigure 2: Ten Corners with the Highest Law Enforcement Activity (2018-2024)"
  },
  {
    "objectID": "posts/SF-Arrests/index.html#changes-in-post-pandemic-arrest-patterns",
    "href": "posts/SF-Arrests/index.html#changes-in-post-pandemic-arrest-patterns",
    "title": "Law Enforcement Activity in San Francisco, 2018-2024",
    "section": "Changes in post-pandemic arrest patterns",
    "text": "Changes in post-pandemic arrest patterns\nThe onset of the pandemic and the accompanying city-wide shelter-in-place order were associated with a large drop in law enforcement activity at the city level. Between February and April 2020 – the immediate outset of the pandemic – monthly enforcement actions decreased by 43% (-441 arrests/citations) in San Francisco. This trend aligns with prior study of the pandemic’s effects on crime and policing. In a national analysis that included San Francisco and 11 other major cities, Abrams (2021) found that shelter-in-place orders and the pandemic itself caused substantial and immediate reductions in many types of criminal behavior and their accompanying arrest activity.\n\n\n\n\n\nFigure 3: Monthly Arrest Totals, City-wide, 2018-2024\n\n\n\n\n\n\n\n\nThat short-term drop in law enforcement activity in 2020 can be tied to the immediate disruptions of the Pandemic. However, it is not clear why activity remained depressed for years after the onset of the pandemic. Table 1 defines two distinct, two-year periods:\n\nthe “pre-pandemic,” 1/1/2018-1/1/2020,2 and\nthe “post-pandemic,” 1/1/2022-1/1/2024\n\nThe table shows that average weekly enforcement actions in the post-pandemic period were still down -36% from the pre-pandemic.\n\n\n\n\nTable 1: Weekly Arrest Totals in the pre/post pandemic\n\n\n\n\n\n\n\n\nTime Period\nAverage Weekly Arrest/Citation Total\nDifference\nPercent\n\n\n\n\nPre Pandemic (2018-20)\n269\n-\n-\n\n\nPost Pandemic (2022-24)\n171\n-98\n-36%\n\n\n\n\n\n\n\n\nThe general decrease in law enforcement activity in the post-pandemic was driven by a large decrease in enforcement actions for traffic violations, down -19.82 a week on average. Enforcement actions for larceny theft and some violent crimes – robbery and assault – also decreased.\nIn the post-pandemic, homicide, burglary, weapons, and drug-use3 enforcement actions all returned to or else never deviated from pre-pandemic levels. Their average weekly enforcement activity levels in the post-pandemic were not statistically distinguishable from the pre-pandemic.\nSale of drugs was the only incident category where enforcement activity significantly increased in the post-pandemic. Enforcement actions tied to the sale of drugs4 increased by 7.43 a week on average. It is notable that drug sale enforcement activity increased while drug-use enforcement activity seemingly did not. Figure 4 shows trends for each incident type, using a statistical approach detailed in the Methods Appendix Section 4.\n\n\n\n\n\nFigure 4: Change in Average Weekly Arrests/Citations, 2018-2020 to 2022-2024\n\n\n\n\n\n\n\n\nIn the post-pandemic, 73% of all drug sale enforcement actions in the City took place in the greater Tenderloin. The greater Tenderloin is also the only region in the city where enforcement activity returned to and in some months exceeded pre-pandemic levels. Other districts generally saw levels drop slightly, or, in the Mission’s case, drop significantly.\n\n\n\n\n\nFigure 5: Monthly Arrest Totals by District, 2018-2024\n\n\n\n\n\n\n\n\nThe greater Tenderloin also saw a meaningful change in the composition of enforcement activity in the post-pandemic, marked by a growing emphasis on drug sale enforcement actions. Figure 6 shows that for most incident types in the greater Tenderloin, average weekly levels were essentially equal in the pre and post-pandemic. Drug use enforcement activity rose on its face, but the increase was not statistically significant, suggesting that the weekly totals were highly variable and not consistently elevated.\nDrug sale enforcement activity saw a large and statistically significant increase, with 6.7 more actions a week on average in the post-pandemic. This increase is consistent with media coverage of the City’s aggressive approach to drug crime in the Tenderloin. Neilson (2023) also suggests that policing in the Tenderloin was more aggressive in the post-pandemic, with a particular focus on making arrests for drug sale.\n\n\n\n\n\nFigure 6: Change in Average Weekly Arrests in the Tenderloin, 2018-2020 to 2022-2024"
  },
  {
    "objectID": "posts/SF-Arrests/index.html#discussion",
    "href": "posts/SF-Arrests/index.html#discussion",
    "title": "Law Enforcement Activity in San Francisco, 2018-2024",
    "section": "Discussion",
    "text": "Discussion\nI conclude that in the post-pandemic, the City shifted law enforcement activity towards the Tenderloin and prioritized enforcement actions for drug sale in that district. To be clear, the political and administrative focus on the Tenderloin, and particularly on drug crime in the Tenderloin, is not novel (Hartlaub 2022). However, I newly document the degree to which the City, in the post-pandemic, emphasized law enforcement in the Tenderloin relative to 2018-2020 pre-pandemic trends. The gap in law enforcement activity levels between the greater Tenderloin and all other regions in the City grew considerably. This shift in policing resources could have been in response to rising drug sale crime in the Tenderloin, but further research using criminal incident data is necessary to confirm any trends in crime."
  },
  {
    "objectID": "posts/SF-Arrests/index.html#sec-appendix",
    "href": "posts/SF-Arrests/index.html#sec-appendix",
    "title": "Law Enforcement Activity in San Francisco, 2018-2024",
    "section": "Appendix",
    "text": "Appendix\n\n\nData Appendix\nI based all analysis on publicly available police incident data from the City of San Francisco (DataSF 2024). Of the 824,565 records in the incident data extract, many are duplicates or else incidents that did not lead to arrest or citation. I applied filtering to isolate the 119,119 records that represent unique arrests/citations between 01-01-2018 and 02-16-2024. I then filtered the dataset to isolate arrests where geographic point data is available – 118,506 unique arrests/citations. The vast majority of the 613 arrests/citations with missing point data took place outside of San Francisco. The negligible arrests/citations with missing point data within San Francisco were distributed across districts proportional to their totals, suggesting that dropping missing point data did not bias arrests/citations down in any particular region.\n\nDefining the greater Tenderloin\nI define the “greater Tenderloin” by placing a small buffer around the Tenderloin shape to encompass the corners along its immediate edges. See Figure 7 below. The buffered area is added to the Tenderloin and removed from the adjoining districts. The Tenderloin is the only district that I apply this buffering to because no other district has such intense spillover of enforcement actions along its borders.\n\n\n\n\n\nFigure 7: The Greater Tenderloin\n\n\n\n\n\n\n\n\n\n\nDefining “Drug use” enforcement actions\nIn the incident report data, there is a category, “Drug,” which contains all drug related enforcement actions. I split this category into two sub-groups – drug sale enforcement, and drug use/non sale enforcement. All enforcement actions in the broad “Drug” category are also members of one of these two subgroups. I define Drug use, or, non-drug sale enforcement actions, as those drug incidents that do not contain the phrase “sale” in their description.\n\n\n\n\n\n\n\nDrug Use Enforcement Description\nFrequency\n\n\n\n\nNarcotics Paraphernalia, Possession of\n4,349\n\n\nMethamphetamine Offense\n1,446\n\n\nControlled Substance Offense\n318\n\n\nHeroin Offense\n298\n\n\nOpiates Offense\n284\n\n\nLoitering Where Narcotics are Sold/Used\n249\n\n\nCocaine, Base/rock Offense\n231\n\n\nMarijuana Offense\n227\n\n\nFirearm, Armed While Possessing Controlled Substance\n207\n\n\nCocaine Offense\n161\n\n\nControlled Substance, Under the Influence of\n122\n\n\nMethamphetamine, Transportation\n39\n\n\nControlled Substance Violation, Loitering for\n24\n\n\nCocaine, Transportation\n21\n\n\nHallucinogenics Offense\n20\n\n\nMarijuana, Transporting\n20\n\n\nControlled Substance, Transportation\n12\n\n\nMethadone Offense\n11\n\n\nAmphetamines Offense\n11\n\n\nOpiates, Transportation\n10\n\n\nMaintain Premise Where Narcotics Are Sold/used\n10\n\n\nHeroin, Transportation\n9\n\n\nOpium Offense\n8\n\n\nOpium Derivative Offense\n6\n\n\nMarijuana, Cultivating/Planting\n4\n\n\nMarijuana, Furnishing\n4\n\n\nDrug Lab Apparatus, Possession\n3\n\n\nHypodermic Needle or Syringe, Possession\n2\n\n\nNarcotics Addict, Failure To Register\n2\n\n\nBarbiturates, Possession\n2\n\n\nPrescription, Forge Or Alter (11368 H&S)\n2\n\n\nBarbiturates Offense\n1\n\n\nControlled Substance, Presence Where Used\n1\n\n\nBarbiturates, Transportation\n1\n\n\n\n\n\n\n\nDefining “Drug sale” enforcement\nDrug sale enforcement actions are those incidents in the “Drugs” category that contain the word “sale” in their description.\n\n\n\n\n\n\n\nDrug Sale Enforcement Description\nFrequency\n\n\n\n\nMethamphetamine, Possession For Sale\n2,063\n\n\nCocaine, Base/rock, Possession For Sale\n1,862\n\n\nHeroin, Possession For Sale\n1,429\n\n\nOpiates, Possession For Sale\n898\n\n\nControlled Substance, Possession For Sale\n880\n\n\nCocaine, Base/rock, Sale\n523\n\n\nCocaine, Possession For Sale\n430\n\n\nMethamphetamine, Sale\n252\n\n\nControlled Substance, Sale\n232\n\n\nMarijuana, Possession For Sale\n182\n\n\nHeroin, Sales\n133\n\n\nOpiates, Sale\n108\n\n\nHallucinogenic, Possession For Sale\n62\n\n\nHallucinogenic, Sale\n45\n\n\nMethadone, Possession For Sale\n41\n\n\nMarijuana, Sales\n29\n\n\nCocaine, Sale\n18\n\n\nSales of Cocaine Base/Schoolyard Trafficking Act Violation\n11\n\n\nOpium, Possession For Sale\n7\n\n\nAmphetamine, Possession For Sale\n5\n\n\nOpium Derivative, Possession For Sale\n3\n\n\n\n\n\n\n\n\n\n\nMethods Appendix\nIn this memo, I used the following core statistical test for examining changes in law enforcement activity:\n\\[\nactions_t = \\hat{\\beta_0} + \\hat{\\beta_{1}}post_t + \\epsilon_t\n\\tag{1}\\]\nThe estimate of interest is \\(\\hat{\\beta_{1}}\\), which represents the average increase in weekly enforcement actions in the post-pandemic, 2022-2024, relative to the pre-pandemic, 2018-2020. \\(\\hat{\\beta_{0}}\\) is the average of weekly enforcement actions in the pre-pandemic and \\(\\hat{\\beta_{0}} + \\hat{\\beta_{1}}\\) is the average of weekly enforcement actions in the post-pandemic. I test the hypothesis that the average of weekly enforcement actions in the post-pandemic was different from the average of weekly enforcement actions in the pre-pandemic:\n\\[\nH_0: \\beta_{1} = 0, \\quad H_1: \\beta_{1} \\neq 0\n\\tag{2}\\]\nI repeat the estimation of Equation 1 and test Equation 2 for each arrest type, a total of 9 times, once at the city-level, and once in the greater Tenderloin. I obtain the standard errors presented in the memo via a block bootstrap procedure, with weeks clustered into four-unit groups to address autocorrelation in the week-level time series.\nRepeated hypothesis testing introduces the issue of multiple testing, where every additional test we conduct raises the probability that we made a false discovery of change (James et al. 2021, chap 13). To adjust for multiple testing, I apply a Bonferroni correction to the hypothesis tests. Specifically, for a 95% confidence interval, where one would typically test with a threshold for the probability of a false positive, \\(\\alpha = .05\\),5 the Bonferroni correction instead tests with \\(\\alpha = .05/m\\), where \\(m\\) is the number of hypotheses being tested (James et al. 2021, chap 13). In this case, \\(m=\\) 9, implying a threshold \\(.05/\\) 9 \\(=\\) 0.0056 and confidence intervals that cover 99.44%.\n\nRobustness checks\nOne potential issue with the modeling approach is that the Bonferroni correction can be overly conservative when the amount of hypotheses being tested is very large or sample sizes are very small. However, in this setting, where only 9 related hypotheses are being tested and the week-level time series is not especially small at \\(n=\\) 313, the Bonferroni correction can still be an appropriate adjustment (VanderWeele and Mathur 2019). Regardless, to test if the conservatism of the correction meaningfully changes the results of the policy analysis, I fit models with and without it.\nI also conduct robustness checks on the standard error specification. In the memo, I present the block-bootstrap standard errors because they rely on minimal assumptions about the structure of the data. However, I present results from two alternatives – Newey-West standard errors, and ordinary least squares (OLS) standard errors – to test robustness of the findings.\nEach of these specifications are theoretically inferior to the block-bootstrap approach, but the use of mis-specified OLS is so common in practice as to merit inclusion. Equation 1, when estimated using ordinary least squares (OLS), produces standard errors valid under the following assumptions:\n\nThe error term is normally distributed with constant variance and mean zero: \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\).\nWeekly arrests are continuous and distributed normally: \\(actions \\in \\mathbb{R}\\) and \\(actions \\sim \\mathcal{N}(\\mu, \\sigma^2)\\)\n\nIn the context of weekly enforcement action totals, which are a time-series that exhibit autocorrelation, the first assumption of constant variance may be problematic. Both Newey-West estimation and the block bootstrap are robust to this issue. Enforcement action count data, which are typically small, positive integers, don’t conform to the structure set forth in the second assumption. The block bootstrap is the only specification present that is robust to this issue. Thus, the bootstrap should be the best approach under the circumstances.\n\nResults\nTable 2 shows that changing testing specifications has no impact on any statistical or policy conclusions made at the City-level in the memo.\nTable 3 shows that testing specification does not affect core conclusions about drug sale enforcement in the greater Tenderloin. However, specification does affect the conclusion presented in the memo that larceny theft, robbery, traffic and drug use enforcement actions did not change in the greater Tenderloin in the post-pandemic. Much of this is due to the fact that OLS becomes an increasingly tenuous approach in the Tenderloin, where weekly enforcement action counts are often small and clustered near 0, a clear challenge to OLS modeling assumptions.\n\nDrug use: using OLS with no Bonferroni correction yields a statistically significant increase in drug use enforcement actions. This is a mis-specified test that does not take into account the structure of the data or possible multiple testing issues. All other tests concluded that there was no statistically significant change. Still, the effect is not trivially small, so I temper my conclusion that this enforcement action class did not increase and encourage further research into this arrest type in particular.\n\nLarceny theft: similarly, using OLS with no Bonferroni correction yields a statistically significant estimate, while all other tests conclude there was no change. In this case, the change is also practically negligible (less than one arrest a week). In the memo , I report no change in larceny theft enforcement actions in the greater Tenderloin.\nRobbery: every specification that does not utilize the Bonferroni correction yields a small, statistically significant decrease, and every specification with the correction yields no change. In this case, a next step would be using a less conservative correction for multiple testing (James et al. 2021, chap 13). However, given that the effect size is particularly small, I defer to the core specification and report no change in the memo.\nTraffic: tests present a mixed bag and a practically negligible effect size. In the memo I defer to conclusions of the core bootstrap specification.\n\n\n\n\n\nTable 2: Full City – robustness checks for changes in arrest activity\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nLower bound\nUpper bound\nSignificant\n\n\nOutcome\nModel\n\n\n\n\n\n\n\n\nAssault\nBlock Bootstrap (block=4)\n-7.876190\n-10.182692\n-5.634615\nTrue\n\n\nBlock Bootstrap, no Bonferroni\n-7.876190\n-9.509615\n-6.259615\nTrue\n\n\nNewey West (lags=3)\n-7.876200\n-11.169000\n-4.584000\nTrue\n\n\nNewey West, no Bonferroni\n-7.876200\n-10.193000\n-5.560000\nTrue\n\n\nOLS\n-7.876200\n-10.985000\n-4.768000\nTrue\n\n\nOLS, no Bonferroni\n-7.876200\n-10.063000\n-5.689000\nTrue\n\n\nBurglary\nBlock Bootstrap (block=4)\n-0.523810\n-1.540625\n0.569471\nFalse\n\n\nBlock Bootstrap, no Bonferroni\n-0.523810\n-1.250000\n0.288462\nFalse\n\n\nNewey West (lags=3)\n-0.523800\n-1.652000\n0.604000\nFalse\n\n\nNewey West, no Bonferroni\n-0.523800\n-1.317000\n0.270000\nFalse\n\n\nOLS\n-0.523800\n-1.708000\n0.661000\nFalse\n\n\nOLS, no Bonferroni\n-0.523800\n-1.357000\n0.310000\nFalse\n\n\nDrug Non Sale\nBlock Bootstrap (block=4)\n-3.066667\n-10.041827\n4.506490\nFalse\n\n\nBlock Bootstrap, no Bonferroni\n-3.066667\n-8.115625\n2.307692\nFalse\n\n\nNewey West (lags=3)\n-3.066700\n-10.623000\n4.490000\nFalse\n\n\nNewey West, no Bonferroni\n-3.066700\n-8.383000\n2.250000\nFalse\n\n\nOLS\n-3.066700\n-7.997000\n1.863000\nFalse\n\n\nOLS, no Bonferroni\n-3.066700\n-6.535000\n0.402000\nFalse\n\n\nDrug Sale\nBlock Bootstrap (block=4)\n7.428571\n1.889904\n13.209615\nTrue\n\n\nBlock Bootstrap, no Bonferroni\n7.428571\n3.432692\n11.740385\nTrue\n\n\nNewey West (lags=3)\n7.428600\n2.006000\n12.851000\nTrue\n\n\nNewey West, no Bonferroni\n7.428600\n3.614000\n11.243000\nTrue\n\n\nOLS\n7.428600\n2.813000\n12.044000\nTrue\n\n\nOLS, no Bonferroni\n7.428600\n4.181000\n10.676000\nTrue\n\n\nHomicide\nBlock Bootstrap (block=4)\n0.047619\n-0.059856\n0.153846\nFalse\n\n\nBlock Bootstrap, no Bonferroni\n0.047619\n-0.038462\n0.115385\nFalse\n\n\nNewey West (lags=3)\n0.047600\n-0.058000\n0.154000\nFalse\n\n\nNewey West, no Bonferroni\n0.047600\n-0.027000\n0.122000\nFalse\n\n\nOLS\n0.047600\n-0.053000\n0.148000\nFalse\n\n\nOLS, no Bonferroni\n0.047600\n-0.023000\n0.118000\nFalse\n\n\nLarceny Theft\nBlock Bootstrap (block=4)\n-9.571429\n-12.384615\n-6.211538\nTrue\n\n\nBlock Bootstrap, no Bonferroni\n-9.571429\n-11.740385\n-7.384615\nTrue\n\n\nNewey West (lags=3)\n-9.571400\n-12.820000\n-6.323000\nTrue\n\n\nNewey West, no Bonferroni\n-9.571400\n-11.857000\n-7.286000\nTrue\n\n\nOLS\n-9.571400\n-12.127000\n-7.016000\nTrue\n\n\nOLS, no Bonferroni\n-9.571400\n-11.369000\n-7.773000\nTrue\n\n\nRobbery\nBlock Bootstrap (block=4)\n-3.066667\n-4.396394\n-1.788462\nTrue\n\n\nBlock Bootstrap, no Bonferroni\n-3.066667\n-4.019231\n-2.163462\nTrue\n\n\nNewey West (lags=3)\n-3.066700\n-4.298000\n-1.835000\nTrue\n\n\nNewey West, no Bonferroni\n-3.066700\n-3.933000\n-2.200000\nTrue\n\n\nOLS\n-3.066700\n-4.181000\n-1.953000\nTrue\n\n\nOLS, no Bonferroni\n-3.066700\n-3.850000\n-2.283000\nTrue\n\n\nTraffic\nBlock Bootstrap (block=4)\n-19.819048\n-24.129327\n-15.685817\nTrue\n\n\nBlock Bootstrap, no Bonferroni\n-19.819048\n-22.884615\n-16.846154\nTrue\n\n\nNewey West (lags=3)\n-19.819000\n-23.714000\n-15.924000\nTrue\n\n\nNewey West, no Bonferroni\n-19.819000\n-22.559000\n-17.079000\nTrue\n\n\nOLS\n-19.819000\n-22.734000\n-16.905000\nTrue\n\n\nOLS, no Bonferroni\n-19.819000\n-21.870000\n-17.769000\nTrue\n\n\nWeapons\nBlock Bootstrap (block=4)\n-1.171429\n-3.127163\n1.240385\nFalse\n\n\nBlock Bootstrap, no Bonferroni\n-1.171429\n-2.461538\n0.586538\nFalse\n\n\nNewey West (lags=3)\n-1.171400\n-3.618000\n1.276000\nFalse\n\n\nNewey West, no Bonferroni\n-1.171400\n-2.893000\n0.550000\nFalse\n\n\nOLS\n-1.171400\n-3.401000\n1.058000\nFalse\n\n\nOLS, no Bonferroni\n-1.171400\n-2.740000\n0.397000\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 3: Greater Tenderloin – robustness checks for changes in arrest activity\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nLower bound\nUpper bound\nSignificant\n\n\nOutcome\nModel\n\n\n\n\n\n\n\n\nAssault\nBlock Bootstrap (block=4)\n-0.533333\n-1.403846\n0.367548\nFalse\n\n\nBlock Bootstrap, no Bonferroni\n-0.533333\n-1.144231\n0.096154\nFalse\n\n\nNewey West (lags=3)\n-0.533300\n-1.650000\n0.583000\nFalse\n\n\nNewey West, no Bonferroni\n-0.533300\n-1.319000\n0.252000\nFalse\n\n\nOLS\n-0.533300\n-1.626000\n0.560000\nFalse\n\n\nOLS, no Bonferroni\n-0.533300\n-1.302000\n0.236000\nFalse\n\n\nBurglary\nBlock Bootstrap (block=4)\n-0.076190\n-0.394231\n0.201923\nFalse\n\n\nBlock Bootstrap, no Bonferroni\n-0.076190\n-0.307692\n0.115385\nFalse\n\n\nNewey West (lags=3)\n-0.076200\n-0.397000\n0.245000\nFalse\n\n\nNewey West, no Bonferroni\n-0.076200\n-0.302000\n0.149000\nFalse\n\n\nOLS\n-0.076200\n-0.377000\n0.225000\nFalse\n\n\nOLS, no Bonferroni\n-0.076200\n-0.288000\n0.136000\nFalse\n\n\nDrug Non Sale\nBlock Bootstrap (block=4)\n3.561905\n-1.723317\n10.627163\nFalse\n\n\nBlock Bootstrap, no Bonferroni\n3.561905\n-0.326923\n8.557692\nFalse\n\n\nNewey West (lags=3)\n3.561900\n-2.601000\n9.725000\nFalse\n\n\nNewey West, no Bonferroni\n3.561900\n-0.774000\n7.898000\nFalse\n\n\nOLS\n3.561900\n-0.054000\n7.178000\nFalse\n\n\nOLS, no Bonferroni\n3.561900\n1.018000\n6.106000\nTrue\n\n\nDrug Sale\nBlock Bootstrap (block=4)\n6.704762\n1.521154\n11.954087\nTrue\n\n\nBlock Bootstrap, no Bonferroni\n6.704762\n3.115385\n10.442308\nTrue\n\n\nNewey West (lags=3)\n6.704800\n1.774000\n11.636000\nTrue\n\n\nNewey West, no Bonferroni\n6.704800\n3.236000\n10.174000\nTrue\n\n\nOLS\n6.704800\n2.671000\n10.738000\nTrue\n\n\nOLS, no Bonferroni\n6.704800\n3.867000\n9.542000\nTrue\n\n\nHomicide\nBlock Bootstrap (block=4)\n0.009524\n0.000000\n0.048077\nFalse\n\n\nBlock Bootstrap, no Bonferroni\n0.009524\n0.000000\n0.028846\nFalse\n\n\nNewey West (lags=3)\n0.009500\n-0.017000\n0.036000\nFalse\n\n\nNewey West, no Bonferroni\n0.009500\n-0.009000\n0.028000\nFalse\n\n\nOLS\n0.009500\n-0.017000\n0.036000\nFalse\n\n\nOLS, no Bonferroni\n0.009500\n-0.009000\n0.028000\nFalse\n\n\nLarceny Theft\nBlock Bootstrap (block=4)\n-0.885714\n-2.081250\n0.732933\nFalse\n\n\nBlock Bootstrap, no Bonferroni\n-0.885714\n-1.769231\n0.240385\nTrue\n\n\nNewey West (lags=3)\n-0.885700\n-2.504000\n0.732000\nFalse\n\n\nNewey West, no Bonferroni\n-0.885700\n-2.024000\n0.253000\nFalse\n\n\nOLS\n-0.885700\n-1.993000\n0.222000\nFalse\n\n\nOLS, no Bonferroni\n-0.885700\n-1.665000\n-0.106000\nTrue\n\n\nRobbery\nBlock Bootstrap (block=4)\n-0.361905\n-0.761779\n-0.019231\nFalse\n\n\nBlock Bootstrap, no Bonferroni\n-0.361905\n-0.644231\n-0.125000\nTrue\n\n\nNewey West (lags=3)\n-0.361900\n-0.750000\n0.027000\nFalse\n\n\nNewey West, no Bonferroni\n-0.361900\n-0.635000\n-0.089000\nTrue\n\n\nOLS\n-0.361900\n-0.748000\n0.024000\nFalse\n\n\nOLS, no Bonferroni\n-0.361900\n-0.634000\n-0.090000\nTrue\n\n\nTraffic\nBlock Bootstrap (block=4)\n-0.895238\n-2.221154\n0.403846\nFalse\n\n\nBlock Bootstrap, no Bonferroni\n-0.895238\n-1.807692\n-0.057692\nFalse\n\n\nNewey West (lags=3)\n-0.895200\n-1.959000\n0.169000\nFalse\n\n\nNewey West, no Bonferroni\n-0.895200\n-1.644000\n-0.147000\nTrue\n\n\nOLS\n-0.895200\n-1.716000\n-0.074000\nTrue\n\n\nOLS, no Bonferroni\n-0.895200\n-1.473000\n-0.318000\nTrue\n\n\nWeapons\nBlock Bootstrap (block=4)\n0.342857\n-0.569471\n1.434856\nFalse\n\n\nBlock Bootstrap, no Bonferroni\n0.342857\n-0.307933\n1.115385\nFalse\n\n\nNewey West (lags=3)\n0.342900\n-0.622000\n1.308000\nFalse\n\n\nNewey West, no Bonferroni\n0.342900\n-0.336000\n1.022000\nFalse\n\n\nOLS\n0.342900\n-0.548000\n1.233000\nFalse\n\n\nOLS, no Bonferroni\n0.342900\n-0.284000\n0.969000\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\n\nAbrams, David S. 2021. “COVID and Crime: An Early Empirical Look.” Journal of Public Economics 194 (February): 104344. https://doi.org/10.1016/j.jpubeco.2020.104344.\n\n\nDataSF. 2024. “SFPD Incident Report: 2018 to Present.” Dataset Explainers. https://datasf.gitbook.io/datasf-dataset-explainers/sfpd-incident-report-2018-to-present.\n\n\nHartlaub, Peter. 2022. “Culture, Protest and an Earthquake-Proof Bank: A Century of Stories from S.F.’s Tenderloin.” The San Francisco Chronicle, February. https://www.sfchronicle.com/projects/2022/san-francisco-tenderloin-history.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. 2nd ed. 2021 edition. New York, NY: Springer.\n\n\nNeilson, Aldo Toledo, Susie. 2023. “S.F. Drug Arrests Are the Highest They’ve Been in a Decade. Here’s Why.” San Francisco Chronicle, September. https://www.sfchronicle.com/sf/article/tenderloin-drug-arrests-18327055.php.\n\n\nVanderWeele, Tyler J, and Maya B Mathur. 2019. “SOME DESIRABLE PROPERTIES OF THE BONFERRONI CORRECTION: IS THE BONFERRONI CORRECTION REALLY SO BAD?” American Journal of Epidemiology 188 (3): 617–18. https://doi.org/10.1093/aje/kwy250."
  },
  {
    "objectID": "posts/SF-Arrests/index.html#footnotes",
    "href": "posts/SF-Arrests/index.html#footnotes",
    "title": "Law Enforcement Activity in San Francisco, 2018-2024",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA map defining the “greater Tenderloin” is available in Appendix Section 4.1.1↩︎\nThe U.S. reported its first COVID-19 case in January 2020↩︎\n“Drug-use enforcement actions” are defined in Appendix Section 4.1.2↩︎\n“Drug-sale enforcement actions” are defined in Appendix Section 4.1.3↩︎\n \\(CI = 1 - \\alpha = 1 - .05 = .95\\) ↩︎"
  },
  {
    "objectID": "posts/SIR/index.html",
    "href": "posts/SIR/index.html",
    "title": "Policy Responses to a Coronavirus Pandemic",
    "section": "",
    "text": "During the COVID-19 pandemic, policy makers around the world have developed a variety of tools for combating the virus: lockdowns and strategic re-openings, testing, contact-tracing, masking, and vaccination, to name a few (Brooks 2020). In this memo, I will examine the effectiveness of lockdowns and vaccination specifically, with an eye towards how they might be used during future pandemics. I find that lockdowns are an effective pandemic response in so far as they considerably reduce infections. I also find that a vaccine that provides temporary immunity cannot effectively end a pandemic with only one vaccination campaign. During future pandemics, policy makers should be ready to institute lockdowns, especially early in the pandemic, and prepare the public for yearly vaccine boosters once a vaccine has been developed."
  },
  {
    "objectID": "posts/SIR/index.html#groups-parameters-and-assumptions",
    "href": "posts/SIR/index.html#groups-parameters-and-assumptions",
    "title": "Policy Responses to a Coronavirus Pandemic",
    "section": "Groups, Parameters, and Assumptions",
    "text": "Groups, Parameters, and Assumptions\nThere are three population groups: there is a total population, to be expressed as \\(1\\), and on any given day, \\(t\\), there is a percentage of individuals in the population who are susceptible to the disease - \\(S_t\\) - a percentage of individuals currently infected - \\(I_t\\), and a percentage of individuals who have been infected, recovered, and cannot be infected again - \\(R_t\\). Those three groups represent the entire population: \\[ 1 = S_t + I_t + R_t \\]\nAlong with the groups, there are three key parameters that describe the behavior of the virus in our population:\n\n\\(\\beta\\) describes the growth in infections each day.1\n\\(\\nu\\) is the percentage of the infected population that recovers each day\n\\(\\lambda\\) is the percentage of the recovered population that loses their immunity each day (vaccinated or previously infected)\n\nUsing these population groups and the key parameters, \\(\\beta\\), \\(\\nu\\), and \\(\\lambda\\), I produce the following basic model."
  },
  {
    "objectID": "posts/SIR/index.html#the-basic-model",
    "href": "posts/SIR/index.html#the-basic-model",
    "title": "Policy Responses to a Coronavirus Pandemic",
    "section": "The Basic Model",
    "text": "The Basic Model\nOn a given day, the number of infected individuals is defined as follows: \\[\n\\begin{align*}\n\\text{Infected Today} &= \\text{Infected Yesterday} + \\text{New Infected Today} - \\text{New Recovered Today} \\\\\nI_t &= (I_{t-1}) + (I_{t-1} \\beta S_{t-1}) - (I_{t-1} \\nu)\n\\end{align*}\n\\]\nThe number of people who have recovered from the virus and thus have temporary immunity is: \\[\n\\begin{align*}\n\\text{Recovered Today} &= \\text{Recovered Yesterday} + \\text{New Recovered Today} - \\text{New Susceptible Today}\\\\\nR_t &= (R_{t-1}) + (I_{t-1} \\nu) - (R_{t-1} \\lambda)\n\\end{align*}\n\\]\nThe number of individuals susceptible to the virus is those who are not infected or recovered: \\[\n\\begin{align*}\nS_t &= 1 - I_t - R_t\n\\end{align*}\n\\]\n\nAssumptions\nThis model starts with 0.00001% of the population infected. I assume that individuals interact with an average of 10 other people each day, and 2% of those interactions result in virus transmission. Once infected, there is an average recovery time of 10 days. Those who have recovered from infection are expected to retain their immunity for 6 months on average."
  },
  {
    "objectID": "posts/SIR/index.html#possible-policy-responses",
    "href": "posts/SIR/index.html#possible-policy-responses",
    "title": "Policy Responses to a Coronavirus Pandemic",
    "section": "Possible Policy Responses",
    "text": "Possible Policy Responses\nLockdowns can be implemented by the government, but to have their costs reflected in the model, they can be sustained for no more than three weeks. They have the effect of reducing contacts between individuals by 75%, but have diminishing effectiveness. Each subsequent lockdown is half as effective as the previous one (the first lockdown reduces the contact rate from 10 to 2.5, the second from 10 to 5, the third from 10 to 7.5), though there is a minimum effectiveness, where no matter what number lockdown the sociey is on, there will at least be a 20% reduction in contact under lockdown. Lockdowns will be declared whenever the infected population grows beyond some threshold.\nI apply the model to two distinct scenarios under which rolling lockdowns are implemented: one where no vaccine is ever developed to combat the virus and one where a vaccine is developed after one year and rolled out incrementally."
  },
  {
    "objectID": "posts/SIR/index.html#footnotes",
    "href": "posts/SIR/index.html#footnotes",
    "title": "Policy Responses to a Coronavirus Pandemic",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe reproduction rate, \\(\\beta\\), is the product of the number of interactions between infected individuals and susceptible individuals each day and the percentage of those interactions that will result in disease transmission.↩︎"
  },
  {
    "objectID": "posts/dask-data-io/index.html",
    "href": "posts/dask-data-io/index.html",
    "title": "Benchmarking parquet and dask",
    "section": "",
    "text": "The following example involves a situation where I want to analyze historical data on Bay Area Rapid Transit (BART) ridership at the station/hour level. BART kindly makes such ridership information publicly available on their open data portal. This post will examine the following workflow:\nThis walkthrough largely serves to highlight the efficiency and the ease of use of dask and parquet. I find that:\n1.) Parquet is a relatively efficient format for storage (Figure 3)\n\n2.) dask+parquet is a relatively fast for data IO (Figure 4)\nI should note that my adoption of dask and parquet storage is due to the influence of two of my day-job coworkers, Batool Hasan and Konrad Franco, after we leveraged them for an internal project with a similar workflow.\nAs an aside, I highly recommend watching the “Background” section of this talk for some basic information on dask and how it differs from Spark, DuckDB, and polars. The talk also features some comprehensive benchmarks of dask and establishes the core differences between [dask and Spark] in one group, and [polars and DuckDB] in the other.\nimport pandas as pd\nimport numpy as np\nimport dask.dataframe as dd\n\n# Timers\nfrom dask.diagnostics import ProgressBar\nfrom tqdm import tqdm\nimport time\n\n# I/O Utilities\nimport requests\nfrom bs4 import BeautifulSoup\nimport re\nimport os\nfrom datetime import datetime\nimport gc\n\n# Display\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/dask-data-io/index.html#scraping-our-data",
    "href": "posts/dask-data-io/index.html#scraping-our-data",
    "title": "Benchmarking parquet and dask",
    "section": "Scraping our data",
    "text": "Scraping our data\nAnyways, to start, here is the homepage of the BART data portal:\n\n\n\nFigure 1: BART data portal\n\n\n\n\n\n\nI’ll set up a scraping script using requests and BeautifulSoup to build access to the BART hourly ridership data.\n\n# URL of the webpage to scrape\nurl = 'https://afcweb.bart.gov/ridership/origin-destination/'\nurl\n\n'https://afcweb.bart.gov/ridership/origin-destination/'\n\n\n\n# Send an HTTP GET request to the webpage\nresponse = requests.get(url)\n# Check if the request was successful\nif response.status_code == 200:\n    # Parse the content of the webpage with Beautiful Soup\n    soup = BeautifulSoup(response.text, 'html.parser')\nelse:\n    print(\"Failed to retrieve the webpage\")\n\nThe page is laid out as follows:\n\n\n\nFigure 2: Origin-destination pairing data\n\n\n\n\n\n\nMy target here is the set of .csv.gz (compressed .csv files) that contain hourly ridership totals between each station pairing. These are all links, thus are in &lt;a&gt; &lt;/a&gt; tags, and have an href that ends in .csv.gz. The following captures links to that specification:\n\nlinks = soup.find_all(\n    # final all &lt;a&gt;&lt;/a&gt; content\n    'a',\n    # filter to only those links with href ending in .csv.gz\n    href=lambda x: x and x.endswith(\".csv.gz\")\n    )\n# example output\nlinks[0]\n\n&lt;a href=\"date-hour-soo-dest-2018.csv.gz\"&gt; date-hour-soo-dest-2018.csv.gz&lt;/a&gt;\n\n\nI’m specifically interested in the file that this piece of html links to, which is contained in the href tag. I can capture the href for each of these pieces of html as follows\n\nfiles = [l.get('href') for l in links]\n# example output\nfiles[0]\n\n'date-hour-soo-dest-2018.csv.gz'\n\n\nI’ve now captured the filename, which is a relative url. To download this, I’ll need to convert this to a full url, by concatenating the base url to each of the files’ relative urls. This leaves us with direct links that each prompt the download of one year’s worth of hourly trip totals between the station pairings in the system:\n\nfile_urls = [url + f for f in files]\nfile_urls\n\n['https://afcweb.bart.gov/ridership/origin-destination/date-hour-soo-dest-2018.csv.gz',\n 'https://afcweb.bart.gov/ridership/origin-destination/date-hour-soo-dest-2019.csv.gz',\n 'https://afcweb.bart.gov/ridership/origin-destination/date-hour-soo-dest-2020.csv.gz',\n 'https://afcweb.bart.gov/ridership/origin-destination/date-hour-soo-dest-2021.csv.gz',\n 'https://afcweb.bart.gov/ridership/origin-destination/date-hour-soo-dest-2022.csv.gz',\n 'https://afcweb.bart.gov/ridership/origin-destination/date-hour-soo-dest-2023.csv.gz',\n 'https://afcweb.bart.gov/ridership/origin-destination/date-hour-soo-dest-2024.csv.gz']\n\n\nThis is our target data, so before proceeding to download all of it for local storage, we’ll profile the sizes of each file and the total download:\n\ncounter = 1\ntotal = 0\n\nfor f in file_urls:\n    response = requests.head(f)\n    # Retrieve the file size for each file\n    file_size = int(response.headers.get('Content-Length', 0))\n    # Keep track of the total file size\n    total += file_size\n    print(f\"File {counter} size: {file_size} bytes ({round(file_size*10e-7, 2)} mega-bytes)\")\n    counter += 1\nprint(f\"Total size of data: {total*10e-7} mega-bytes\")\n\nFile 1 size: 38627139 bytes (38.63 mega-bytes)\nFile 2 size: 38177159 bytes (38.18 mega-bytes)\nFile 3 size: 21415653 bytes (21.42 mega-bytes)\nFile 4 size: 24350926 bytes (24.35 mega-bytes)\nFile 5 size: 30546036 bytes (30.55 mega-bytes)\nFile 6 size: 32224174 bytes (32.22 mega-bytes)\nFile 7 size: 16468035 bytes (16.47 mega-bytes)\nTotal size of data: 201.809122 mega-bytes\n\n\nWe’ll proceed to download all of this into a folder, data. Here I take advantage of tqdm’s progress bar so that I can track the potentially large job’s progress. I also add logic for two conditions:\n\nmake sure that the ingest doesn’t re-read files that I already store locally.\nUNLESS, it the data is from the current year, in which case it likely contains more data than the present file for that year.\n\n\ncurrent_year: str = str(datetime.today().year)\n# Create the \"data\" folder if it doesn't exist\nif not os.path.exists('data'):\n    os.makedirs('data')\n# Download and save the files\nfor url in tqdm(file_urls):\n    filename: str = os.path.join('data', os.path.basename(url))\n    current_year_data: bool = re.search(\"\\d{4}\", url)[0] == current_year\n    file_exists: bool = os.path.exists(filename)\n    if file_exists and not current_year_data:\n        pass\n    else:\n        response = requests.get(url)\n        if response.status_code == 200:\n            with open(filename, 'wb') as file:\n                file.write(response.content)\n        else:\n            print(f\"Failed to download: {url}\")\n\n100%|██████████| 7/7 [00:00&lt;00:00,  7.79it/s]\n\n\nSince we are storing a large amount of data in the project directory, we will also set up a .gitignore to make sure that it doesn’t end up being tracked in version control.\n\n# Create a .gitignore file\ngitignore_content = \"data/\\n\"  # Content to exclude the \"data\" folder\nwith open('.gitignore', 'w') as gitignore_file:\n    gitignore_file.write(gitignore_content)\n\nI now have the raw data stored locally in the following paths.\n\ndata_paths = [\"data/\" + f for f in files]\ndata_paths\n\n['data/date-hour-soo-dest-2018.csv.gz',\n 'data/date-hour-soo-dest-2019.csv.gz',\n 'data/date-hour-soo-dest-2020.csv.gz',\n 'data/date-hour-soo-dest-2021.csv.gz',\n 'data/date-hour-soo-dest-2022.csv.gz',\n 'data/date-hour-soo-dest-2023.csv.gz',\n 'data/date-hour-soo-dest-2024.csv.gz']"
  },
  {
    "objectID": "posts/dask-data-io/index.html#storage-with-parquet",
    "href": "posts/dask-data-io/index.html#storage-with-parquet",
    "title": "Benchmarking parquet and dask",
    "section": "Storage with parquet",
    "text": "Storage with parquet\nSince these are in a usable form now, I decided against implementing any other processing steps to the data and preserve a raw form in storage, but I’ll proceed to batch convert this directory and set of files into a directory of parquet files for more efficient data I/O.\nWhen we convert to parquet we’ll get two core benefits:\n\nThe files will be smaller (see Figure 3 ).\nThe files can be read into memory much faster.\n\nTo do the batch transfer from .csv.gz into parquet, I’ll lazily read in the full directory of .csv.gz files using dask:\nNote that what I downloaded above is not all of the BART data I have – just what is currently on the website. I’ve previously downloaded extracts going back to 2011:\n\nall_csv_paths = [\"data/\" + f for f in os.listdir(\"data\") if f.endswith(\".gz\")]\nall_csv_paths\n\n['data/date-hour-soo-dest-2011.csv.gz',\n 'data/date-hour-soo-dest-2012.csv.gz',\n 'data/date-hour-soo-dest-2013.csv.gz',\n 'data/date-hour-soo-dest-2014.csv.gz',\n 'data/date-hour-soo-dest-2015.csv.gz',\n 'data/date-hour-soo-dest-2016.csv.gz',\n 'data/date-hour-soo-dest-2017.csv.gz',\n 'data/date-hour-soo-dest-2018.csv.gz',\n 'data/date-hour-soo-dest-2019.csv.gz',\n 'data/date-hour-soo-dest-2020.csv.gz',\n 'data/date-hour-soo-dest-2021.csv.gz',\n 'data/date-hour-soo-dest-2022.csv.gz',\n 'data/date-hour-soo-dest-2023.csv.gz',\n 'data/date-hour-soo-dest-2024.csv.gz']\n\n\n\ndata = dd.read_csv(all_csv_paths, blocksize=None, compression='gzip')\ndata.columns = ['Date', 'Hour', 'Start', 'End', 'Riders']\ndata\n\nDask DataFrame Structure:\n\n\n\n\n\nDate\nHour\nStart\nEnd\nRiders\n\n\nnpartitions=14\n\n\n\n\n\n\n\n\n\n\nstring\nint64\nstring\nstring\nint64\n\n\n\n...\n...\n...\n...\n...\n\n\n...\n...\n...\n...\n...\n...\n\n\n\n...\n...\n...\n...\n...\n\n\n\n...\n...\n...\n...\n...\n\n\n\n\nDask Name: operation, 2 expressions\n\n\n…\nNow I’ll establish a subdirectory in data that’s just for the parquet directory\n\nif not os.path.exists('data/parquet_data'):\n    os.makedirs('data/parquet_data')\n\nand I’ll write out to that folder using dask, with a progress bar to monitor how long this takes (note that these progress bars may not be visible in this document).\n\npbar = ProgressBar()\npbar.register()\n\nFor data of this size, the one-time conversion typically takes under two minutes\n\ndata.to_parquet('data/parquet_data', write_index=False)\n\n\nStorage Comparison\nNow that we have the files saved, we can conduct a quick check on how storage size differs. I’ll write a helper function to calculate the total bytes of all the files in a given directory.\n\ndef get_local_bytes(directory: str) -&gt; int:\n    contents: list[str] = os.listdir(directory)\n    contents_w_path: list[str] = [os.path.join(directory, f) for f in contents]\n    files_not_folders: list[str] = [f for f in contents_w_path\n                         if os.path.isfile(f)]\n    return sum(os.path.getsize(f) for f in files_not_folders)\n\n\n\nCode\nfig, ax = plt.subplots()\npd.DataFrame(\n    {\n        '.csv.gz': [round(get_local_bytes('data')*10e-7, 2)],\n        'parquet': [round(get_local_bytes('data/parquet_data')*10e-7, 2)]\n    }, index=['Bart Data Size']\n).T.plot.bar(ax=ax)\nax.bar_label(ax.containers[0], fmt=\"%g megabytes\")\nax.set(ylabel='Megabytes', xlabel='File Type')\nax.tick_params(axis='x', rotation=0)\nfig.tight_layout()\nfig.savefig('fig1.png', dpi=300)\n\n\n\n\n\nFigure 3: Storage Comparison: .csv.gz versus parquet\n\n\n\n\n\n\n\n\nparquet is pretty good here."
  },
  {
    "objectID": "posts/dask-data-io/index.html#reading-parquet",
    "href": "posts/dask-data-io/index.html#reading-parquet",
    "title": "Benchmarking parquet and dask",
    "section": "Reading parquet",
    "text": "Reading parquet\nWith the data in parquet, it’s now very easy to read in and analyze all of this ridership data, and we’ll see that it’s much faster that working with .csv.gz data (see Figure 4 for the efficiency conclusions).\n\ndask with parquet\nHere I’ll use dask to read in about 52 million rows of data in well under 30 seconds, in a single line of code.\nNote that the code for reading in the data, dd.read_parquet('data/parquet_data') executes lazily – it’s not reading data into memory yet! I’ve seen some guides online that benchmark dask and pandas based on un-computed dask commands, which completely misses how dask works. Here, I’m going to explicitly call the compute() method at the end of the code, which prompts the actual reading in of the data from local file to memory.\n\nstart = time.time()\ndf = dd.read_parquet('data/parquet_data').compute()\ndask_parquet_end = time.time() - start\n\nNote that I called start = time.time() and dask_parquet_endtime = time.time() to store the time that it takes to complete this task.\nAnyways, we just loaded in fairly large data:\n\ndf.shape\n\n(122851920, 5)\n\n\nIn what seems like a short amount of time:\n\nprint(round(dask_parquet_end, 2), \"seconds\")\n\n31.32 seconds\n\n\nand this data is now all in memory as a pandas dataframe, ready for typical use.\n\ndf.head()\n\n\n\n\n\n\n\n\nDate\nHour\nStart\nEnd\nRiders\n\n\n\n\n0\n2011-01-01\n0\n12TH\n16TH\n1\n\n\n1\n2011-01-01\n0\n12TH\n24TH\n3\n\n\n2\n2011-01-01\n0\n12TH\nASHB\n2\n\n\n3\n2011-01-01\n0\n12TH\nBAYF\n5\n\n\n4\n2011-01-01\n0\n12TH\nCIVC\n3"
  },
  {
    "objectID": "posts/dask-data-io/index.html#benchmarking-the-read-in-task",
    "href": "posts/dask-data-io/index.html#benchmarking-the-read-in-task",
    "title": "Benchmarking parquet and dask",
    "section": "Benchmarking the read-in task",
    "text": "Benchmarking the read-in task\nBefore we celebrate too much, lets compare that performance to our other possible cases. We’ll compare how the following read-in setups compare:\n\ndask + parquet\ndask + .csv.gz\npandas + parquet\npandas + .csv.gz\n\nBefore I start doing these benchmarks, I’ll need to do some memory cleaning so that I can repeatedly read these files in:\n\ndel df\ngc.collect()\n\n\ndask with .csv.gz\nHere we simply use the .csv.gz files with dask – does the parallel processing in dask make the conversion from .csv.gz to parquet not worth it?\n\nif not os.path.exists(\"times.csv\"):\n    start = time.time()\n    df_dask_csv = dd.read_csv(data_paths,\n                              blocksize=None,\n                              compression='gzip').compute()\n    dask_csv_end = time.time() - start\n    del df_dask_csv\n    gc.collect()\n\n\n\npandas with .csv.gz\nHere we simply use the .csv.gz files with pandas for read-in – how do things look if we use no new libraries and do no file conversions?\n\nif not os.path.exists(\"times.csv\"):\n    dfs = []\n    start = time.time()\n    for csv_path in tqdm(data_paths):\n        dfs.append(pd.read_csv(csv_path))\n    df_pandas = pd.concat(dfs)\n    pandas_csv_end = time.time() - start\n    del df_pandas\n    gc.collect()\n\n\n\npandas with parquet\nFinally, we’ll take advantage of the fact that pandas also supports the read-in of directories of parquet data and try the parquet files with pandas for read-in – can we avoid using dask?\n\nif not os.path.exists(\"times.csv\"):\n    start = time.time()\n    df_pandas_parquet = pd.read_parquet('data/parquet_data')\n    pandas_parquet_end = time.time() - start\n    del df_pandas_parquet\n    gc.collect()\n\nWe’ll answer all of those questions at once with a plot:\n\n\nCode\nif not os.path.exists(\"times.csv\"):\n    times = pd.DataFrame(\n        {\n            'dask + parquet': [round(dask_parquet_end, 2)],\n            'dask + .csv.gz': [round(dask_csv_end, 2)],\n            'pandas + parquet': [round(pandas_parquet_end, 2)],\n            'pandas + .csv.gz': [round(pandas_csv_end, 2)],\n        }, index=['Bart Data Read-In Time']\n    )\n    times.to_csv('times.csv')\nelse:\n    times = pd.read_csv('times.csv', index_col=0)\nfig, ax = plt.subplots()\ntimes.T.sort_values(by='Bart Data Read-In Time', ascending=False).plot.barh(ax=ax)\nax.bar_label(ax.containers[0], fmt=\" %g seconds\")\nax.set(xlabel='Seconds', ylabel='Package + Storage Method', xlim=(0, 85))\nax.tick_params(axis='x', rotation=0)\nfig.tight_layout()\nfig.savefig('fig2.png', dpi=300)\n\n\n\n\n\nFigure 4: Storage Comparison: .csv.gz versus parquet\n\n\n\n\n\n\n\n\nAgain, parquet, and especially parquet with dask, looks pretty good here.\n\n\nSome reflection\nThese differences in seconds aren’t so massive in the context of this 52 million row dataset. Indeed, in my experience with smaller row counts, just using .csv.gz/.csv with pandas is faster, as dask has to take time to set up parallel jobs at the start, and a parquet conversion has a fixed time cost up front. However, as the size of the data increases, one would expect the gap between the dask + parquet approach to start to diverge from the others more, and the efficiency gains become more important.\nEven beyond the time differences, I’m especially partial to dask+parquet due to the difference in how much code is required at read-in. I also love the fact that dask can read in an entire directory of data easily in one line of code:\ndf = dd.read_parquet('data/parquet_data').compute()"
  },
  {
    "objectID": "posts/linear-approximation-3d/index.html",
    "href": "posts/linear-approximation-3d/index.html",
    "title": "Linear Approximation in 3D",
    "section": "",
    "text": "Code\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import axes3d\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nThe following is a calculus problem that I think provides a good overview of linear approximation, a method for approximating a general function using a linear function. This is a simple application of taylor series approximation, and is used for optics, oscillation, and electric resistivity problems (“Linear Approximation” 2023). The following image from (Strang and Herman 2016, chap. 4.4) provides some good intuition of what this approximation looks like:"
  },
  {
    "objectID": "posts/linear-approximation-3d/index.html#the-question",
    "href": "posts/linear-approximation-3d/index.html#the-question",
    "title": "Linear Approximation in 3D",
    "section": "The Question",
    "text": "The Question\nUse a tangent plane to approximate the value of the following function at the point \\((2.1, -5.1)\\).\n\\[f(x,y) = \\sqrt{42-4x^2-y^2}\\]\nHere’s some code setup for the question:\n\nf = lambda x, y: np.sqrt(42-4*x**2 - y**2)\nrange_x = np.linspace(-8, 8, 500)\nrange_y = range_x.copy()\nX, Y = np.meshgrid(range_x, range_y)\nZ = f(X, Y)\n\n\nWhy do we need to approximate this?\nWe can’t just plug in and compute this output because the point (2.1, -5.1) is outside of the domain of this function – thus \\(f(2.1, -5.1)\\) does not exist.\n\nf(2.1, -5.1)\n\nnan\n\n\n\\[f(2.1, -5.1) \\approx \\sqrt{-1.65} = DNE\\]\n\n\nCode\ndef style_plot(ax, z=False):\n    ax.grid(alpha=.5)\n    ax.set(xlabel=\"$x$\", ylabel=\"$y$\")\n    if z:\n        ax.set(zlabel=\"$z$\")\n\nfig, ax = plt.subplots(figsize=(6, 4))\nax.contourf(X, Y, Z)\nax.scatter(x=2.2,\n           y=-5.1,\n           s=15,\n           color='tab:orange',\n           label=r\"$f(2.1, -5.1) = DNE$\")\nstyle_plot(ax)\nax.legend();\n\n\n\n\n\n\n\n\n\n\n\nDeveloping a linear approximation of the function\nTo approximate this quantity, we find a nearby point where the function is defined, and construct a tangent plane to extrapolate our quantity of interest. For a nearby point, I select \\(P_0 = (x_0=2, y_0=-5)\\), where the function still produces a real number output:\n\\[f(x_0, y_0) = f(2, -5) =\\sqrt{1} = 1\\]\n\n\nCode\nfig = plt.figure(figsize=(8, 3))\nax1 = plt.subplot(122)\nax2 = plt.subplot(121, projection='3d')\n# Countour plot\nax1.contourf(X, Y, Z)\nax1.scatter(2, -5, color='red', s=15, label=r\"$f(2, -5)$\")\nax1.grid(alpha=.5)\nax1.legend()\nstyle_plot(ax1)\n# 3d surface\nax2.plot_wireframe(X, Y, Z)\nstyle_plot(ax2, z=True)\nax2.set_box_aspect(None, zoom=0.9)\nax2.set_xlim(-5, 5)\nax2.set_ylim(-8, 8)\nax2.set_zlim(-5, 5)\nax2.scatter(2, -5, f(2, -5), color='red', s=15)\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nWe then derive a linear approximation of \\(f\\) at this point using the following equation:\n\\[\n\\begin{align}\n   L(x, y) = f\\left( {{x_0},{y_0}} \\right) + {f_x}\\left( {{x_0},{y_0}} \\right)\\left( {x - {x_0}} \\right) + {f_y}\\left( {{x_0},{y_0}} \\right)\\left( {y - {y_0}} \\right)\n\\end{align}\n\\tag{1}\\]\nThis is really just the first order taylor polynomial of a given function, which, in \\(\\mathbb{R}^3\\), represents a tangent plane approximation:\n\n[…] This equation […] represents the tangent plane to the surface defined by \\(z=f(x,y)\\) at the point \\((x_0,y_0)\\). The idea behind using a linear approximation is that, if there is a point \\((x_0,_0)\\) at which the precise value of \\(f(x,y)\\) is known, then for vales of \\((x,y)\\) reasonably close to \\((x_0,y_0)\\), the linear approximation (i.e., tangent plane) yields a value that is also reasonably close to the exact value of \\(f(x,y)\\).\n– (Strang and Herman 2016, chap. 4.4)\n\nRecall that \\(f_x\\) and \\(f_y\\) refer to components of the gradient vector, \\(\\nabla f\\). We calculate that gradient as follows: \\[  \n\\begin{align*}\n\\nabla f &= \\left&lt; f_x, f_y \\right&gt; \\\\\n\\nabla f &= \\left&lt; \\frac{df}{dx}, \\frac{df}{dy} \\right&gt; \\\\\n    \\nabla f &= \\left&lt; \\frac{-4x}{\\sqrt{42-4x^2-y^2}}, \\frac{-y}{\\sqrt{42-4x^2-y^2}} \\right&gt;\n\\end{align*}\n\\] Now we can plug in our point, \\(P_0\\):\n\\[\n\\begin{align*}\n\\nabla f(P_0) &= \\left&lt; {f_x}\\left( {{x_0},{y_0}} \\right), {f_y}\\left( {{x_0},{y_0}} \\right) \\right&gt; \\\\\n    \\nabla f(P_0) &= \\left&lt; \\frac{-4(2)}{1}, \\frac{-(-5)}{1} \\right&gt; = \\left&lt; -8, 5 \\right&gt;\n\\end{align*}\n\\] We now have all elements of the tangent plane/linear approximation, Equation 1, and we can simply plug-in and compute: \\[\n\\begin{align*}\n         L(x,y) &= f(2, -5) + -8 \\left( {x - 2} \\right) + 5\\left( y - (-5) \\right) \\\\\n         L(x,y) &= 1 + -8x + 16 + 5y + 25 \\\\\n         L(x,y) &= -8x + 5y + 42\n\\end{align*}\n\\]\n\nL = lambda x, y: -8*x + 5*y + 42\n\n\n\nCode\nfig = plt.figure(figsize=(8, 4))\nax1 = plt.subplot(121,)\nax2 = plt.subplot(122, projection='3d')\n# Countour plot\nax1.contour(X, Y, Z, label=r\"$f(x)$\", alpha=.6)\nax1.contour(X, Y, L(X, Y), levels= 25, alpha=.8, cmap=\"coolwarm\", label=r\"$L(x)$\")\nax1.scatter(2, -5, s=15, color='red', label=r\"$f(2, -5) = 1$\")\nax1.scatter(2.2, -5.1, s=15, color='tab:orange', label=f\"$L(2.1, -5.1) = {round(L(2.1, -5.1), 2)}$\")\nax1.grid(alpha=.5)\nax1.legend(loc=\"upper left\", framealpha=1)\nstyle_plot(ax1)\n# ax1.view_init(-130, -90, 0)\n# 3d surface\nZ = np.ma.masked_where(Z &lt;= 0, Z)\nX = np.ma.masked_where(Z &lt;= 0, X)\nY = np.ma.masked_where(Z &lt;= 0, Y)\nax2.contour3D(X, Y, Z, corner_mask=True)\nax2.plot_surface(X, Y, L(X, Y), alpha=1)\n# ax2.contour3D(X, Y, Z, 20, cmap='gray')\nstyle_plot(ax2, z=True)\nax2.set_xlim(-5, 5)\nax2.set_ylim(-8, 8)\nax2.set_zlim(-5, 5)\nax2.scatter(2, -5, f(2, -5), color='red', s=15)\nax2.scatter(2.2, -5.1, s=15, color='tab:orange', label=f\"$L(2.1, -5.1) = {round(L(2.1, -5.1), 2)}$\")\n\nax2.view_init(0, -133, 0)\nfig.suptitle(r\"$f(x, y)$ & $L(x, y)$\")\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nWith that linear approximation established, we can now estimate our original quantity of interest, \\(f(2.1, -5.1)\\). \\[f(2.1, -5.1) \\approx L(2.1, -5.1) = -8(2.1) + 5(-5.1) + 42 \\approx \\boxed{-0.3} \\]\n\nL(2.1, -5.1)\n\n-0.29999999999999716"
  },
  {
    "objectID": "posts/overfitting/index.html",
    "href": "posts/overfitting/index.html",
    "title": "Overfitting and The Train-Test Split",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\n\npd.set_option(\"display.float_format\", \"{:.2f}\".format)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.tree import DecisionTreeRegressor\n\n\ndef model_data_lm(degree, df):\n    X = df[[\"Education\"]].values\n    y = df[\"Income\"].values\n    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n    model.fit(X, y)\n    rmse = mean_squared_error(y, model.predict(X)) ** 0.5\n    r2 = r2_score(y, model.predict(X))\n    X_test = np.linspace(X.min() - 1.5 * X.std(), X.max() + 1.5 * X.std(), 100).reshape(\n        -1, 1\n    )\n    y_pred = model.predict(X_test)\n\n    return X_test, y_pred, rmse, r2, model"
  },
  {
    "objectID": "posts/overfitting/index.html#some-notes-on-modeling-data-and-resampling-methods",
    "href": "posts/overfitting/index.html#some-notes-on-modeling-data-and-resampling-methods",
    "title": "Overfitting and The Train-Test Split",
    "section": "Some Notes on Modeling Data and Resampling Methods",
    "text": "Some Notes on Modeling Data and Resampling Methods\nThe following is an exercise with synthetic data intended to memorialize some thoughts on models and resampling methods. I won’t reference a lot of specific books here, but generally this post is influenced by James et al. (2013) and Hastie, Tibshirani, and Friedman (2016).\nTo begin, say we are examining the relationship between income and years of education.\n\ndf = pd.DataFrame()\nnp.random.seed(46)\nn = 200\neducation_dgp = lambda n: np.random.lognormal(1.2, 0.6, n)\neducation = education_dgp(n)\ndf[\"Education\"] = education\n\nLet’s say that income is a function of years of education:\n\nnp.random.seed(49)\ndf[\"Income\"] = 33000 + 6000 * education\n\nThe relationship is governed by the deterministic function, which is a data generating process (DGP): \\[\nincome = 33000 + 6000 \\times education\n\\]\n\n\nCode\ng = sns.JointGrid(data=df, x=\"Education\", y=\"Income\", marginal_ticks=False, height=5)\ng.plot_joint(sns.scatterplot, alpha=0.6, marker=\"o\")\ng.plot_marginals(sns.kdeplot, fill=True, clip=(0, np.inf))\ng.fig.suptitle(f\"Income ~ Education\", y=1.05)\ng.ax_joint.grid(alpha=0.1)\n# Formatting the sides\ng.ax_joint.yaxis.set_major_formatter(\"${x:,.0f}\")\ng.ax_marg_x.tick_params(left=False, bottom=False)\ng.ax_marg_y.tick_params(left=False, bottom=False)\ng.ax_marg_y.set(xticklabels=[]);\n\n\n\n\n\n\n\n\n\nModeling this relationship is trivial. Given that it is a deterministic, linear function, we can use data to solve a linear system of equations and find the exact parameters of the relationship:\n\n\nCode\nfig, ax = plt.subplots(1, 2, sharey=True)\nsns.scatterplot(x=\"Education\", y=\"Income\", data=df, alpha=0.6, marker=\"o\", ax=ax[0])\nfig.suptitle(\"Income ~ Education\")\nax[0].grid(alpha=0.1)\nax[0].yaxis.set_major_formatter(\"${x:,.0f}\")\nsns.despine(ax=ax[0])\nax[0].set(\n    xlim=(0, df[\"Education\"].max() + df[\"Education\"].std()),\n    ylim=(0, df[\"Income\"].max() + df[\"Income\"].std()),\n)\nax[1].set(\n    xlim=(0, df[\"Education\"].max() + df[\"Education\"].std()),\n    ylim=(0, df[\"Income\"].max() + df[\"Income\"].std()),\n)\nX_test, y_pred, _, r2, _ = model_data_lm(1, df)\nax[1].grid(alpha=0.1)\nsns.scatterplot(x=\"Education\", y=\"Income\", data=df, alpha=0.3, marker=\"o\", ax=ax[1])\nax[1].plot(\n    X_test,\n    y_pred,\n    color=\"darkred\",\n    alpha=0.7,\n    label=r\"Model $\\rightarrow R^2=$\" + str(round(r2, 2)),\n)\nax[1].legend()\nsns.despine(ax=ax[1]);\n\n\n\n\n\n\n\n\n\n\\(R^2\\) is the percentage of variance in the outcome, income, that our model can describe (in this case 100%, a perfect fit). We’ve created a perfect model of the DGP."
  },
  {
    "objectID": "posts/overfitting/index.html#modeling-a-non-deterministic-process",
    "href": "posts/overfitting/index.html#modeling-a-non-deterministic-process",
    "title": "Overfitting and The Train-Test Split",
    "section": "Modeling a Non-Deterministic Process",
    "text": "Modeling a Non-Deterministic Process\nIn policy analysis settings, it’s rare that we would encounter deterministic processes that we have any interest in modeling. Instead, we typically encounter processes with some stochastic element. For example, the following is a new DGP, where income is a linear function of years of education, but with an added random variable, representing noise.\n\nnoise_std = 9000\nDGP = lambda x: 33000 + 6000 * x + np.random.normal(0, noise_std, size=len(x))\n\n\\[\nincome = 33000 + 6000 \\times education + \\mathcal{N}(0, 9000)\n\\]\n\ndf[\"Income\"] = DGP(education)\n\n\n\nCode\ng = sns.JointGrid(data=df, x=\"Education\", y=\"Income\", marginal_ticks=False, height=5)\ng.plot_joint(sns.scatterplot, alpha=0.6, marker=\"o\")\ng.plot_marginals(sns.kdeplot, fill=True, clip=(0, np.inf))\ng.fig.suptitle(f\"Income ~ Education + $\\mathcal{{N}}(0, {noise_std})$\", y=1.05)\ng.ax_joint.grid(alpha=0.1)\n\n# Formatting the sides\ng.ax_joint.yaxis.set_major_formatter(\"${x:,.0f}\")\ng.ax_marg_x.tick_params(left=False, bottom=False)\ng.ax_marg_y.tick_params(left=False, bottom=False)\ng.ax_marg_y.set(xticklabels=[]);\n\n\n\n\n\n\n\n\n\nModeling this process is no longer trivial. Rather than solve a simple system of equations algebraically, we must use an estimation method to find a “best” model. Estimation involves choices, and finding the best model for this data is more complex than it may seem at first glance.\n\nOverfitting\nWe will model this DGP using a linear regression fit via the ordinary least squares algorithm. We face a number of choices in using linear regression – for example, we can freely use polynomial transformations to make our model flexible to non-linear relationships. Here we’ll examine the behavior of a polynomial regression by setting up an infinite series as follows: \\[\ny_p=\\sum_{p=0}^{\\infty} \\beta_p x^p\n\\] As this series expands, it represents a polynomial regression of degree \\(p\\). When we expand, we can examine how this model performs as the degree increases.\n\n\nCode\ny_max = 3\nx_max = 3\nfig, ax = plt.subplots(x_max, y_max, sharey=True, sharex=True, figsize=(10, 6))\ndegree = 0\nfor i in range(x_max):\n    for j in range(y_max):\n        X_test, y_pred, _, r2, _ = model_data_lm(degree, df)\n        ax[i, j].set_ylim(\n            df[\"Income\"].min() - df[\"Income\"].std(),\n            df[\"Income\"].max() + df[\"Income\"].std(),\n        )\n        ax[i, j].grid(alpha=0.1)\n        sns.despine(ax=ax[i, j])\n        ax[i, j].plot(df[\"Education\"], df[\"Income\"], \".\", alpha=0.4, color=\"tab:blue\")\n        ax[i, j].plot(X_test, y_pred, color=\"darkred\", alpha=0.7)\n        ax[i, j].set(yticklabels=[], xticklabels=[])\n        ax[i, j].tick_params(left=False, bottom=False)\n        ax[i, j].set_title(\n            r\"$y_{{p={}}}=\\sum_{{p=0}}^{} \\beta_p x^p \\rightarrow$ \".format(\n                degree, degree\n            )\n            + r\"$R^2=$\"\n            + f\"{round(r2, 4)}\",\n            size=12,\n        )\n        degree += 1\n\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nRegardless of what you think of the shape of these curves, it’s clear that as the series expands and \\(p\\) increases, we see improving model accuracy, \\(R^2\\). We can expand this series until we find a seemingly “optimal” model.\n\nAn “Optimal” Model\nWe expand the series, or, in more typical language, we conduct a “grid-search,” to find the optimal model as defined by the maximum \\(R^2\\) score:\n\nr2s = []\nmax_root = 25\nroots = range(max_root)\nfor degree in roots:\n    _, _, _, r2, _ = model_data_lm(degree, df)\n    r2s.append(r2)\nr2s = np.array(r2s)\n\n\n\nCode\nfig, ax = plt.subplots(1, 2)\nax[0].plot(roots[1:], r2s[1:], \".-\", alpha=0.8)\nax[0].set_title(\"$R^2$ ~ $d$\")\nax[0].plot(\n    np.argmax(r2s),\n    np.max(r2s),\n    \"o\",\n    color=\"tab:red\",\n    alpha=0.6,\n    label=f\"$\\max \\; (R^2)=${round(np.max(r2s), 3)}\\n\"\n    + r\"$ \\underset{d}{\\arg\\max} \\; (R^2)=$\"\n    + f\"{np.argmax(r2s)}\",\n)\nax[0].set(xlabel=\"Polynomial Degree\", ylabel=\"$R^2$\")\nax[0].legend()\nax[0].grid(alpha=0.1)\nsns.despine(ax=ax[0])\n\ndegree = np.argmax(r2s)\nX_test, y_pred, _, r2, optimal_model = model_data_lm(degree, df)\nax[1].set_ylabel(r\"$\\downarrow$\", size=25)\nax[1].set_ylim(\n    df[\"Income\"].min() - df[\"Income\"].std(), df[\"Income\"].max() + df[\"Income\"].std()\n)\nax[1].grid(alpha=0.1)\nsns.despine(ax=ax[1])\nax[1].plot(df[\"Education\"], df[\"Income\"], \".\", alpha=0.4, color=\"tab:blue\")\nax[1].plot(X_test, y_pred, color=\"darkred\", alpha=0.7)\nax[1].set_xlabel('\"Optimal\" Model')\nax[1].tick_params(left=False, bottom=False)\nax[1].set_title(\n    r\"$y^*_{{p={}}}=\\sum_{{p=0}}^{{{}}} \\beta_p x^p$ \".format(degree, degree)\n)\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nWe see that the accuracy-maximizing model, \\(y^*\\), is a very high degree polynomial. Our simple, empirical investigation leads us to conclude that this is the “best” model.\nHowever, it’s clear that the model is overly complex and likely problematic given these visuals, where there are extreme swings in predictions. It would greatly benefit us to have a clear quantity that represents why this model may be problematic.\n\n\n\nOut of sample performance, or, what happens when we collect new data?\nOne simple way to further evaluate this model is to collect new data (from outside of our original sample) and then evaluate whether the model can describe variation in that new data. Given that we are working with synthetic data, it’s straightforward to simulate new observations from the underlying DGP, and thus “collect” new data. In the following plot, the right-hand pane is a fresh sample of individuals with observed education and income values:\n\neducation = education_dgp(1000)\ncorrect_income = DGP(education)\n\n\n\nCode\nom_residuals = np.abs(optimal_model.predict(education.reshape(-1, 1)) - correct_income)\nnew_data_df = pd.DataFrame()\nnew_data_df[\"Education\"] = education\nnew_data_df[\"Residuals\"] = om_residuals\nnew_data_df[\"Income\"] = correct_income\nnew_data_df[\"Data\"] = \"New\"\nnew_data_df = pd.concat(\n    [\n        df.assign(\n            Residuals=np.abs(\n                optimal_model.predict(df[[\"Education\"]].values) - df[\"Income\"]\n            )\n        ).assign(Data=\"Original\"),\n        new_data_df,\n    ],\n    axis=0,\n)\nax = sns.relplot(\n    height=4,\n    data=new_data_df,\n    x=\"Education\",\n    y=\"Income\",\n    col=\"Data\",\n    kind=\"scatter\",\n    legend=False,\n)\n\nax.axes[0][0].yaxis.set_major_formatter(\"${x:,.0f}\")\n# ax.axes[0][0].plot(X_test, y_pred, color='darkred', alpha=.7)\nax.axes[0][0].set_ylim(\n    df[\"Income\"].min() - df[\"Income\"].std(), df[\"Income\"].max() + df[\"Income\"].std()\n)\nax.axes[0][0].set_ylim(\n    df[\"Income\"].min() - df[\"Income\"].std(), df[\"Income\"].max() + df[\"Income\"].std()\n)\nax.tight_layout();\n\n\n\n\n\n\n\n\n\nHere we can see that, when exposed to a new sample from the same DGP, the model produces extreme error.\n\n\nCode\nnorm = (0, 1000000)\nnorm = plt.Normalize(*norm)\nsm = plt.cm.ScalarMappable(cmap=\"coolwarm\", norm=norm)\n\nax = sns.relplot(\n    height=4,\n    data=new_data_df,\n    x=\"Education\",\n    y=\"Income\",\n    col=\"Data\",\n    hue=\"Residuals\",\n    kind=\"scatter\",\n    palette=\"coolwarm\",\n    hue_norm=norm,\n    legend=False,\n)\nax.axes[0][0].yaxis.set_major_formatter(\"${x:,.0f}\")\nax.axes[0][1].figure.colorbar(sm, ax=ax.axes[0][1], format=\"${x:,.0f}\").set_label(\n    r\"$\\hat{y}-y$\"\n)\nax.axes[0][1].plot(X_test, y_pred, color=\"darkred\", alpha=0.7)\nax.axes[0][0].plot(X_test, y_pred, color=\"darkred\", alpha=0.7)\nax.axes[0][0].set_ylim(\n    df[\"Income\"].min() - df[\"Income\"].std(), df[\"Income\"].max() + df[\"Income\"].std()\n)\nax.axes[0][0].set_ylim(\n    df[\"Income\"].min() - df[\"Income\"].std(), df[\"Income\"].max() + df[\"Income\"].std()\n)\nax.tight_layout();\n\n\n\n\n\n\n\n\n\nWe quantify this error as follows:\n\n\nCode\nrmse = lambda y, y_hat: np.sqrt(np.mean((y_hat - y) ** 2))\noutput_1 = pd.DataFrame(\n    {\n        \"r2\": [\n            r2_score(correct_income, optimal_model.predict(education.reshape(-1, 1)))\n        ],\n        \"rmse\": [rmse(optimal_model.predict(education.reshape(-1, 1)), correct_income)],\n    },\n    index=[\"p=14\"],\n)\nround(output_1, 2)\n\n\n\n\n\n\n\n\n\nr2\nrmse\n\n\n\n\np=14\n-1975424250597210112.00\n26686561853746.36\n\n\n\n\n\n\n\nThis is clearly a bad model. What’s worrying is that it seemed like the best model when we simply examined model accuracy on one dataset. This raises the possibility that we might be duped into selecting models like this in practice.\nOut of curiosity, let’s see how an extremely simply model, a linear regression with \\(p=1\\) compares on this new data.\n\n\nCode\nX_test, y_pred, _, r2, sub_optimal_model = model_data_lm(1, df)\nnew_data_df.loc[new_data_df[\"Data\"] == \"New\", \"Degree 1 Residuals\"] = np.abs(\n    sub_optimal_model.predict(education.reshape(-1, 1)) - correct_income\n)\nnew_data_df.loc[new_data_df[\"Data\"] == \"Original\", \"Degree 1 Residuals\"] = np.abs(\n    sub_optimal_model.predict(df[[\"Education\"]].values) - df[\"Income\"]\n)\n\nsm = plt.cm.ScalarMappable(cmap=\"coolwarm\", norm=norm)\nax = sns.relplot(\n    height=4,\n    data=new_data_df,\n    x=\"Education\",\n    y=\"Income\",\n    col=\"Data\",\n    hue=\"Degree 1 Residuals\",\n    kind=\"scatter\",\n    palette=\"coolwarm\",\n    hue_norm=norm,\n    legend=False,\n)\n\nax.axes[0][0].yaxis.set_major_formatter(\"${x:,.0f}\")\nax.axes[0][1].figure.colorbar(sm, ax=ax.axes[0][1], format=\"${x:,.0f}\")\n\nax.axes[0][1].plot(X_test, y_pred, color=\"darkred\", alpha=0.7)\nax.axes[0][0].plot(X_test, y_pred, color=\"darkred\", alpha=0.7)\nax.axes[0][0].set_ylim(\n    df[\"Income\"].min() - df[\"Income\"].std(), df[\"Income\"].max() + df[\"Income\"].std()\n)\nax.axes[0][0].set_ylim(\n    df[\"Income\"].min() - df[\"Income\"].std(), df[\"Income\"].max() + df[\"Income\"].std()\n)\nax.tight_layout();\n\n\n\n\n\n\n\n\n\nThis is a very good fit, even though our previous empirical fitting process told us that the high \\(p\\) model was optimal.\n\n\nCode\noutput_2 = pd.DataFrame(\n    {\n        \"r2\": [\n            r2_score(\n                correct_income, sub_optimal_model.predict(education.reshape(-1, 1))\n            )\n        ],\n        \"rmse\": [\n            rmse(sub_optimal_model.predict(education.reshape(-1, 1)), correct_income)\n        ],\n    },\n    index=[\"p=1\"],\n)\nround(output_2, 2)\n\n\n\n\n\n\n\n\n\nr2\nrmse\n\n\n\n\np=1\n0.77\n9139.36\n\n\n\n\n\n\n\nWe have the benefit of already knowing that \\(p=1\\) matches the functional form of the true DGP, but even if we didn’t know that in advance, out-of-sample performance seems like a better way of evaluating whether or not our model gets at the underlying DGP of the observations."
  },
  {
    "objectID": "posts/simple-constrained-optimization/index.html",
    "href": "posts/simple-constrained-optimization/index.html",
    "title": "Simple Constrained Optimization in 2D",
    "section": "",
    "text": "Code\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nThe following are my notes on a basic calculus 1 homework question. I liked the question a lot, so decided to write out it all out for my future use."
  },
  {
    "objectID": "posts/simple-constrained-optimization/index.html#a-note-on-the-second-derivative-test",
    "href": "posts/simple-constrained-optimization/index.html#a-note-on-the-second-derivative-test",
    "title": "Simple Constrained Optimization in 2D",
    "section": "A note on the second derivative test",
    "text": "A note on the second derivative test\nWhen we determined \\(V''(x^\\star)&lt;0\\), we determined that coming off of this critical value, the slope of the function is decreasing – we are coming down from a maximum. In the figure below, the critical value is plotted as a black point. It’s clear that if we were to move slightly to the right of the critical value, the slope of the function would decrease, and we see this directly in the plots of the first and second derivatives.\n\n\nCode\nfig, ax = plt.subplots(1, 3)\nax[0].plot(x, V(x))\nax[0].grid(alpha=.5)\nax[0].set_title(r\"$V(x^*) = 8606.62$\")\nax[0].set_ylim(8500,8650)\nax[0].set_xlim(24, 28)\nax[0].plot(x_critical, V(x_critical), \"&gt;\", color=\"grey\", markeredgecolor=\"black\")\n\nax[1].axhline(0, color=\"grey\", linestyle=\"--\")\nax[1].plot(x, V_1(x), c=\"tab:blue\")\nax[1].grid(alpha=.5)\nax[1].set_xlabel(r\"$x$\")\nax[1].set_title(r\"$V'(x^*) = 0$\")\nax[1].set_xlim(20, 30)\nax[1].plot(x_critical, V_1(x_critical), \"&gt;\", color=\"grey\", markeredgecolor=\"black\")\n\nax[2].axhline(0, color=\"grey\", linestyle=\"--\")\nax[2].plot(x[V_2(x) &gt; 0], V_2(x)[V_2(x) &gt; 0], c=\"tab:green\")\nax[2].plot(x[V_2(x) &lt; 0], V_2(x)[V_2(x) &lt; 0], c=\"tab:red\")\nax[2].grid(alpha=.5)\nax[2].set_title(r\"$V''(x^*) \\approx -39$\")\nax[2].plot(x_critical, V_2(x_critical), \"o\", alpha=1, color=\"red\", markeredgecolor=\"black\")\n\nfig.tight_layout();\n\n\n\n\n\n\n\n\n\nExpanding on this point, consider another arbitrary function that has more than one critical point, \\(f(x)=2x^3 - 100x^2\\). In this case the function has one maximum and one minimum, so the second derivative test will be more important for analyzing each point. The function is plotted below, along with its first and second derivative.\n\n\nCode\nf = lambda x: 2*x**3 - 100*x**2\nf_1 = lambda x: 6*x**2 - 200*x\nf_2 = lambda x: 12*x - 200\n\nx = np.linspace(-25, 50, 100)\n\nfig, ax = plt.subplots(1, 3)\nax[0].plot(x, f(x))\nax[0].grid(alpha=.5)\nax[0].plot(100/3, f(100/3), \"&gt;\", color=\"grey\", markeredgecolor=\"black\")\nax[0].plot(0, f(0), \"&gt;\", color=\"grey\", markeredgecolor=\"black\")\n\nax[0].set_title(r\"$f(x)$\")\n\nax[1].axhline(0, color=\"grey\", linestyle=\"--\")\nax[1].plot(x, f_1(x), c=\"tab:blue\")\nax[1].grid(alpha=.5)\nax[1].plot(100/3, f_1(100/3), \"&gt;\", color=\"grey\", markeredgecolor=\"black\")\nax[1].plot(0, f_1(0), \"&gt;\", color=\"grey\", markeredgecolor=\"black\")\n\nax[1].set_title(r\"$f'(x)$\")\n\nax[2].axhline(0, color=\"grey\", linestyle=\"--\")\nax[2].plot(x[f_2(x) &gt; 0], f_2(x)[f_2(x) &gt; 0], c=\"tab:green\")\nax[2].plot(x[f_2(x) &lt; 0], f_2(x)[f_2(x) &lt; 0], c=\"tab:red\")\n\nax[2].grid(alpha=.5)\nax[2].plot(100/3, f_2(100/3), \"o\", color=\"green\", markeredgecolor=\"black\")\nax[2].plot(0, f_2(0), \"o\", color=\"red\", markeredgecolor=\"black\")\n\nax[2].set_title(r\"$f''(x)$\")\n\nfig.suptitle(r\"$f(x) = 2x^3 - 100x^2$\")\n\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nIf we were to want to maximize this function by taking the first derivative and solving for 0, we would find two critical values – a maximum and a minimum. In this case, the second derivative test would be used to conclude which of these points is the maximum and which is the minimum. We substitute each critical value into the second derivative- \\(f''(x^\\star)&lt;0\\) denotes the maximum (the slope of the function is decreasing off of this point), while \\(f''(x^\\star)&gt;0\\) denotes the maximum (the slope of the function is increasing off of this point)."
  },
  {
    "objectID": "posts/time-series-matplotlib/index.html",
    "href": "posts/time-series-matplotlib/index.html",
    "title": "Building Time-series Utilities for matplotlib",
    "section": "",
    "text": "In my day job I’ve recently been working on visualizing time series data with matplotlib. When I’ve wanted some sort of specific formatting or behavior, I’ve had to wade deep in the library documentation to figure out the code for the things I want to do. That’s fine, but what frustrates me is that in the back of my mind, I’m pretty sure these are all things I’ve actually done before. A few years ago in graduate school I took a course that was heavy on matplotlib plotting, and I spent many hours figuring out bizarre techniques for getting very specific results. Unfortunately I’ve forgotten almost all of that library-specific minutiae.\nThis has led me to a new project – I’m creating a personal data science utilities library with modules for matplotlib/seaborn, pandas, geopandas, and scikit-learn, each containing functions that do all the random things I’ve repeatedly built custom code solutions for in the past. The hope is that I don’t need to keep re-learning the same things and can call simple wrapper functions that I’ve written.\nIn this post, I’m going to cover a few matplotlib extensions that I often need. I’ll cover:\nimport pandas as pd\nimport numpy as np\nimport dask.dataframe as dd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport matplotlib.ticker as ticker\nplt.style.use('default')\nfrom datetime import datetime\nfrom typing import Tuple, Union\nfrom collections.abc import Iterable\nI’ll be using the BART dataset that I created in my blog post on dask.\ndf = dd.read_parquet(\"../dask-data-io/data/parquet_data\")\ndf.head()\n\n\n\n\n\n\n\n\nDate\nHour\nStart\nEnd\nRiders\n\n\n\n\n0\n2011-01-01\n0\n12TH\n16TH\n1\n\n\n1\n2011-01-01\n0\n12TH\n24TH\n3\n\n\n2\n2011-01-01\n0\n12TH\nASHB\n2\n\n\n3\n2011-01-01\n0\n12TH\nBAYF\n5\n\n\n4\n2011-01-01\n0\n12TH\nCIVC\n3\nLet’s say I’m interested specifically in an aggregate time series with the number of riders that used the system, for each hour from 2011-2024. I’ll do the appropriate grouping to build that dataset and move from dask to pandas.\ndf_rph = df.groupby(['Date', 'Hour'])['Riders'].sum().compute()\ndf_rph = df_rph.reset_index()\ndf_rph.head()\n\n\n\n\n\n\n\n\nDate\nHour\nRiders\n\n\n\n\n0\n2011-01-01\n0\n5174\n\n\n1\n2011-01-01\n1\n15479\n\n\n2\n2011-01-01\n2\n11055\n\n\n3\n2011-01-01\n3\n5592\n\n\n4\n2011-01-01\n4\n795\nI’m going to be doing a lot of slicing on the date, and I find that it’s easiest to do that when the date column is a datetime index. Also, since I’ll do some analysis of months of data, I’m going to truncate the time series to the last full month.\ndf_rph['Date'] = pd.to_datetime(df_rph['Date'], format=\"%Y-%m-%d\")\ndf_rph_d = df_rph.set_index(\"Date\")\ndf_rph_d = df_rph_d.loc[:\"2024-04-30\"]\ndf_rph_d.head()\n\n\n\n\n\n\n\n\nHour\nRiders\n\n\nDate\n\n\n\n\n\n\n2011-01-01\n0\n5174\n\n\n2011-01-01\n1\n15479\n\n\n2011-01-01\n2\n11055\n\n\n2011-01-01\n3\n5592\n\n\n2011-01-01\n4\n795\nFor now I’ll sum over the hours and just examine ride totals per day in a pd.Series\nmonth_time_series = df_rph_d.resample(\"ME\")['Riders'].sum()\nmonth_time_series.head()\n\nDate\n2011-01-31    8203562\n2011-02-28    7933264\n2011-03-31    9049039\n2011-04-30    8824840\n2011-05-31    8940380\nFreq: ME, Name: Riders, dtype: int64\nSimply plotting the series shows the impact of the pandemic, which is interesting:\nmonth_time_series.plot()\nplt.title(\"Monthly Total BART Riders\");"
  },
  {
    "objectID": "posts/time-series-matplotlib/index.html#pandemic-impact",
    "href": "posts/time-series-matplotlib/index.html#pandemic-impact",
    "title": "Building Time-series Utilities for matplotlib",
    "section": "Pandemic Impact",
    "text": "Pandemic Impact\nI might want to do a slightly more dynamic plot of the pandemic’s impact, where I more directly show how this drop in ridership compared to typical ridership.\nTo create this more complex visual, I’ll create a suite of functions:\n\ntime_overlay_plot, which creates an overlay plot where the x-axis represents months and each series corresponds to a year of data.\nshow_all_xticks, which overrides default matplotlib x-tick behavior and shows all possible ticks\ncustom_legend, which will make it easier to move a legend outside of the plot area\n\n\n\nCode\ndef time_overlay_plot(month_time_series: pd.Series,\n                      ax: matplotlib.axes.Axes,\n                      step_size: int,\n                      highlight_year: int,\n                      date_formatter: str,\n                      bg_alpha: float = .3) -&gt; Tuple[matplotlib.axes.Axes, pd.Index]:\n    \"\"\"\n    Create an overlay plot where the x-axis represents months and each series corresponds to a year of data.\n\n    This function reads in a month-level time series with data spanning multiple years. It overlays the data for each year \n    on the same plot, allowing for year-over-year comparison. One year can be highlighted for emphasis.\n\n    Args:\n        month_time_series (pd.Series): A time series with observations at one-month intervals.\n        ax (matplotlib.axes.Axes): A Matplotlib axis object to plot on.\n        step_size (int): The number of years to skip between plotted series.\n        highlight_year (int): The year to highlight in the overlay plot. Set to None for no highlight.\n        date_formatter (str): The format string for the date labels on the x-axis.\n        bg_alpha (float): The alpha transparency for the non-highlighted lines. Defaults to 0.3.\n\n    Returns:\n        matplotlib.axes.Axes: The updated axis object with the overlay plot.\n        pd.Index: The x-tick labels of the plot.\n\n    Raises:\n        AssertionError: If step_size is larger than the number of unique years in the time series.\n    \"\"\"\n    target_years = month_time_series.index.year.unique()[::step_size]\n    assert (len(target_years) &gt; step_size), \"Step size larger than index length\"\n    labs = month_time_series.loc[str(\n        target_years[-step_size]):str(target_years[-1])].index.strftime(date_formatter)\n    for i in range(len(target_years)-(step_size-1)):\n        a = 1 if (target_years[i + (step_size-1)]\n                  == highlight_year) else bg_alpha\n        subset_time_series = (\n            month_time_series\n            .loc[str(target_years[i]): str(target_years[i + (step_size-1)])]\n            .reset_index()\n            .set_index(labs)[month_time_series.name]\n        )\n        label = fr\"${target_years[i]} \\rightarrow {target_years[i + (step_size-1)]}$\" if step_size != 1 else fr\"${target_years[i]}$\"\n        subset_time_series.plot(linewidth=3,\n                                ax=ax,\n                                alpha=a,\n                                label=label)\n    return ax, labs\n\n\ndef show_all_xticks(ax: matplotlib.axes.Axes, labs: pd.Index) -&gt; matplotlib.axes.Axes:\n    \"\"\"\n    Sets all x-ticks on a Matplotlib axis object and labels them with the provided list of labels.\n\n    This function ensures that all x-ticks are displayed and labeled as specified, making it easier to read and interpret the x-axis of the plot. The labels are displayed horizontally.\n\n    Parameters:\n    ax (matplotlib.axes.Axes): The axis object on which to set the x-ticks and labels.\n    labs (list of str): A list of labels to set on the x-axis. The number of labels should correspond to the number of ticks.\n\n    Returns:\n    matplotlib.axes.Axes: The modified axis object with updated x-ticks and labels.\n    \"\"\"\n    ax.set_xticks(range(len(labs)))\n    ax.set_xticklabels(labs, rotation=0)\n    return ax\n\n\ndef comma_formatter() -&gt; ticker.FuncFormatter:\n    \"\"\"\n    Creates a custom Matplotlib tick formatter that formats axis ticks with commas.\n\n    This formatter converts axis tick values to integers and formats them with commas for thousands \n    separators. This can be useful for improving the readability of plots with large numerical values.\n\n    Returns:\n    --------\n    ticker.FuncFormatter:\n        A Matplotlib FuncFormatter object that applies the comma formatting to axis ticks.\n    \"\"\"\n    def comma(x: float, pos) -&gt; str:\n        \"\"\"Format the tick value `x` with commas. The parameter `pos` is not used.\"\"\"\n        return '{:,}'.format(int(x))\n\n    return ticker.FuncFormatter(comma)\n\ndef custom_legend(ax: matplotlib.axes.Axes,\n                  outside_loc: str = None,\n                  order: Union[str, list] = \"default\",\n                  title: str = \"\",\n                  linewidth: int = 2,\n                  **kwargs) -&gt; matplotlib.axes.Axes:\n    \"\"\"\n    Customize the legend location and order on a Matplotlib axis.\n\n    This function adjusts the position of the legend, optionally placing it outside the plot area,\n    and can reorder the legend entries based on specified criteria.\n\n    Args:\n        ax (matplotlib.axes.Axes): An existing Matplotlib axis object to which the legend belongs.\n        outside_loc (str, optional): Specifies the location of the legend outside the plot area.\n                                     Must be one of [\"lower\", \"center\", \"upper\", None]. \n                                     Defaults to None.\n        order (str, optional): Determines the order of the legend entries. \n                               Must be one of [\"default\", \"reverse\", \"desc\"]. \n                               \"default\" keeps the current order, \n                               \"reverse\" reverses the current order, \n                               \"desc\" orders entries by descending values. \n                               Defaults to \"default\".\n\n    Returns:\n        matplotlib.axes.Axes: The axis object with the customized legend.\n\n    Raises:\n        AssertionError: If `outside_loc` is not in [\"lower\", \"center\", \"upper\", None].\n    \"\"\"\n    handles, labels = ax.get_legend_handles_labels()\n    if order == 'default':\n        pass\n    elif order == 'reverse':\n        handles = handles[::-1]\n        labels = labels[::-1]\n    elif order == 'desc':\n        ordering = np.flip(np.argsort(np.array([line.get_ydata()[-1] for line in ax.lines if (len(line.get_ydata())&gt;0 and line.get_label() in labels)])))\n        handles = np.array(handles)[ordering].tolist()\n        labels = np.array(labels)[ordering].tolist()\n    elif isinstance(order, Iterable):\n        value_to_index = {value: idx for idx, value in enumerate(labels)}\n        indices = [value_to_index[value] for value in order]\n        labels = list(order)\n        handles = np.array(handles)[indices].tolist()\n    else:\n        raise Exception(\"Invalid Order\")\n    error_msg = \"legend_to_right loc must be None or in 'lower', 'center', or 'upper'\"\n    assert outside_loc in [\"lower\", \"center\", \"upper\", None], error_msg\n    if outside_loc == \"lower\":\n        ax.legend(handles, labels, loc='lower left', bbox_to_anchor=(1, 0), **kwargs)\n    elif outside_loc == \"center\":\n        ax.legend(handles, labels, loc='center left', bbox_to_anchor=(1, .5), **kwargs)\n    elif outside_loc == \"upper\":\n        ax.legend(handles, labels, loc='upper left', bbox_to_anchor=(1, 1), **kwargs)\n    else:\n        ax.legend(handles, labels, **kwargs)\n    legend = ax.get_legend()\n    legend.set_title(title)\n    for line in legend.get_lines():\n        line.set_linewidth(linewidth)\n    return ax\n\n\nUsing these functions, I’ll create a time series overlay that shows the impact of the pandemic on BART ridership in the context of previous three year trends, e.g. how does 2019-2021 compare to 2015-17? The effect is a plot that suggests that monthly BART ridership was on a trajectory similar to previous years leading up to the pandemic before sharply departing.\nNote that I’m able to flexibly define the date formatting for the x-axis using a date_formatter argument, and I can select to highlight a particular year with the highlight_year argument.\n\nfig, ax = plt.subplots(figsize=(8, 4))\ndate_formatter = '%b\\n\\'%y'\n_, labs = time_overlay_plot(month_time_series.loc[\"2011\":\"2021\"],\n                            ax=ax, step_size=2,\n                            highlight_year=2021,\n                            date_formatter=date_formatter)\nax.yaxis.set_major_formatter(comma_formatter())\nax.set(ylim=0, title=\"Three Year Trends in BART Ridership\",\n       ylabel=\"Total Monthly Riders\")\nax.axvline(np.where(labs == datetime(2020, 1, 1).strftime(date_formatter)),\n           linestyle=\":\",\n           label=\"First Covid Case in CA\",\n           color=\"black\",\n           linewidth=3,\n           alpha=.4)\nax.legend();\n\n\n\n\n\n\n\n\nI’ll use the same time_overlay_plot function to examine the recovery of the BART system in the post-pandemic period. This time, I’ll also use custom_legend to easily move the legend outside of the plot area.\n\nmonth_time_series_trunc = pd.concat(\n    [df_rph_d.resample(\"ME\")['Riders'].sum().loc[\"2019-01-01\": \"2020-01-01\"],\n     df_rph_d.resample(\"ME\")['Riders'].sum().loc[\"2021-01-01\": \"2024-01-01\"]])\n\nfig, ax = plt.subplots(figsize=(8, 4))\ndate_formatter = '%b'\n_, labs = time_overlay_plot(month_time_series_trunc, ax=ax, step_size=1,\n                            highlight_year=2019, date_formatter=date_formatter, bg_alpha=.5)\nshow_all_xticks(ax, labs)\nax.yaxis.set_major_formatter(comma_formatter())\nax.set(ylim=0, title=\"Post-Pandemic Recovery\", ylabel=\"Total Monthly Riders\")\nax.grid(alpha=.5)\ncustom_legend(ax, outside_loc=\"center\", order=\"desc\",\n              title=\"Year\", linewidth=3)\nhandles, labels = ax.get_legend_handles_labels()"
  },
  {
    "objectID": "posts/time-series-matplotlib/index.html#commute-patterns",
    "href": "posts/time-series-matplotlib/index.html#commute-patterns",
    "title": "Building Time-series Utilities for matplotlib",
    "section": "Commute Patterns",
    "text": "Commute Patterns\nI’ll now move onto examining some commute patterns. Here I’ll create some custom functions for building a colormap that I can use to consistently denote color-class relationships across multiple plots. Default behavior in matplotlib and seaborn typically leads to color-class relationships changing across charts of different types or involving different subsets of the data, e.g. in Chart 1, class A is Red, but in Chart 2, class A is Blue. My custom function, build_colormap, creates a simple color-class relationship dictionary that defines the color-class relationships once and can be input to various seaborn visuals.\nI’ll define color-class relationships for the days of the week, but first I’ll do some setup in pandas\n\ndf_rph['day_of_week'] = df_rph['Date'].dt.day_name()\ndf_rph['Weekend'] = df_rph['day_of_week'].isin(['Saturday', 'Sunday'])\ndf_rph['Hour_12'] = pd.to_datetime(\n    df_rph['Hour'], format='%H').dt.strftime('%I:%M %p')\ndf_rph['Hour_12'] = df_rph['Hour_12'].str.replace(\":00 \", \"\\n\").str.strip(\"0\")\ndf_rph['Rider_Thousands'] = df_rph['Riders'] / 1000\n\nNow I’ll define the build_colormap function and create my color mapping\n\n\nCode\ndef build_colormap(series: pd.Series) -&gt; dict:\n    \"\"\"\n    Build a colormap dictionary for a pandas Series.\n\n    This function creates a dictionary that maps each unique value in the input\n    Series to a unique color from the 'tab10' colormap provided by Seaborn. The \n    function ensures that the number of unique values in the Series is less than 10.\n\n    Parameters:\n    ----------\n    series : pd.Series\n        The pandas Series for which to build the colormap. Each unique value in the \n        Series will be assigned a unique color.\n\n    Returns:\n    -------\n    dict\n        A dictionary where the keys are the unique values from the input Series \n        and the values are the corresponding colors from the 'tab10' colormap.\n\n    Raises:\n    ------\n    AssertionError\n        If the number of unique values in the Series is 10 or more.\n    \"\"\"\n    unique_set = series.unique()\n    assert len(unique_set) &lt; 10\n    colors = sns.color_palette(\"tab10\")\n    colormap = {}\n    for d, c in zip(unique_set, colors[:len(unique_set)]):\n        colormap[d] = c\n    return colormap\n\n\nHere is the resulting color-mapping dictionary\n\ncolormap = build_colormap(df_rph['day_of_week'])\ncolormap\n\n{'Saturday': (0.12156862745098039, 0.4666666666666667, 0.7058823529411765),\n 'Sunday': (1.0, 0.4980392156862745, 0.054901960784313725),\n 'Monday': (0.17254901960784313, 0.6274509803921569, 0.17254901960784313),\n 'Tuesday': (0.8392156862745098, 0.15294117647058825, 0.1568627450980392),\n 'Wednesday': (0.5803921568627451, 0.403921568627451, 0.7411764705882353),\n 'Thursday': (0.5490196078431373, 0.33725490196078434, 0.29411764705882354),\n 'Friday': (0.8901960784313725, 0.4666666666666667, 0.7607843137254902)}\n\n\nand here’s a simple application of that colormap to examine the average riders-per-hour in the BART system.\n\nfig, ax = plt.subplots(figsize=(8, 4))\nsns.despine(top=True, ax=ax)\nsns.lineplot(\n    data=df_rph,\n    x='Hour_12',\n    y='Rider_Thousands',\n    hue='day_of_week',\n    palette=colormap,\n    linewidth=3,\n    estimator='mean',\n    errorbar=None,\n    ax=ax)\nax.set(\n    title=\"Average Hourly Riders\",\n    ylabel=\"Average Riders (1,000s)\",\n    xlabel=\"Hour\")\ncustom_legend(ax, title=\"Day of the week\", linewidth=2, order=list(colormap.keys()))\nax.grid(alpha=.6)\nfig.tight_layout()\n\n\n\n\n\n\n\n\nI can see the degree to which BART seems to have a lot of its ridership concentrated in commute times. Now lets say that I want to reproduce this plot repeatedly for different years, so see how this general shape changes before and after the pandemic. This is going to be a grid plot, where each individual plot in the grid represents one year of data. I’ll create a flexible function for grid-plotting, grid_plot, below, which allows for one to pass in a seaborn plot, and **kwargs for that plotting function, and repeat that plot across some grouping variable, group_var.\n\n\nCode\ndef grid_plot(grid_subset: pd.DataFrame,\n              seaborn_func: callable,\n              rows: int,\n              cols: int,\n              group_var: str,\n              figsize: tuple = (10, 5),\n              legend_loc: str = \"lower\",\n              **kwargs):\n    facet = grid_subset[group_var].unique()\n    facet = facet.reshape(rows, cols)\n    fig, axes = plt.subplots(\n        facet.shape[0], facet.shape[1], sharex=True, sharey=True, figsize=figsize)\n    if rows &gt; 1:\n        for m in range(facet.shape[0]):\n            for n in range(facet.shape[1]):\n                group = facet[m, n]\n                subset = grid_subset[grid_subset[group_var] == group]\n                if legend_loc == 'lower':\n                    legend = True if (m+1 == rows) and (n+1 == cols) else False\n                elif legend_loc == \"upper\":\n                    legend = True if (m+1 == 1) and (n+1 == cols) else False\n                else:\n                    legend = False\n                seaborn_func(\n                    data=subset,\n                    ax=axes[m, n],\n                    legend=legend,\n                    **kwargs)\n                axes[m, n].set(ylabel=None, xlabel=None, title=group)\n                axes[m, n].grid()\n    else:\n        for n in range(facet.shape[1]):\n            group = facet[n]\n            subset = grid_subset[grid_subset[group_var] == group]\n            if legend_loc == 'lower':\n                legend = True if (m+1 == rows) and (n+1 == cols) else False\n            elif legend_loc == \"upper\":\n                legend = True if (m+1 == 1) and (n+1 == cols) else False\n            else:\n                legend = False\n            seaborn_func(\n                data=subset,\n                ax=axes[m, n],\n                legend=legend,\n                **kwargs)\n            axes[n].set(ylabel=None, xlabel=None, title=group)\n            axes[n].grid()\n    return fig, axes\n\n\nFor an application, I’ll create a dataset from 2018-2024 where I want to display one chart per year in a grid pattern\n\ngrid_subset = df_rph.copy()[(df_rph['Date'] &gt; \"2018\")\n                            & (df_rph['Date'] &lt; \"2024\")]\ngrid_subset['Year'] = grid_subset['Date'].dt.year\n\nNow I can use my function, grid_plot, to specify a plot type, a grid shape, and pass seaborn **kwargs to the underlying plotting function.\n\nfig, axes = grid_plot(grid_subset,\n                      sns.lineplot,\n                      rows=2, cols=3,\n                      group_var='Year',\n                      legend_loc=\"lower\",\n                      x='Hour',\n                      y='Rider_Thousands',\n                      hue='day_of_week',\n                      figsize=(8, 5),\n                      palette=colormap,\n                      estimator='mean',\n                      linewidth=2,\n                      errorbar=None)\ncustom_legend(axes[1, 2], outside_loc=\"upper\",\n              title=\"Day of the week\", linewidth=2, order=list(colormap.keys()))\nfig.supxlabel(\"Hour of Day\")\nfig.supylabel(\"Average Riders (1,000s)\")\nfig.tight_layout()\nfig.suptitle(\"Hourly Ridership Trends Across Years\", y=1.05);\n\n\n\n\n\n\n\n\nThe grid presents an interesting pattern in so far as the general level of ridership drops across all hours, but the raw plot seems to suggest that the gap between peak commute ridership levels and off-peak ridership seems to have shrunken post-pandemic. In late 2023, BART instituted scheduling changes to reflect their belief that demand has shifted in the post-pandemic towards rides outside of peak commute times.\nWe can check that with a simple time series plot that relies on my custom_legend function to move the legend and resize the lines within it.\n\nis_commute = (df_rph['Hour'].isin([7, 8, 9, 16, 17, 18, 19]) &\n              ~df_rph['Weekend']).replace({\n                  True: 'Riders during Peak Commute times',\n                  False: 'Riders during Off-Peak times'\n              })\n\n\nfig, ax = plt.subplots()\n(df_rph[df_rph['Date'] &lt; \"2024-05-01\"]\n .pivot_table(index=pd.Grouper(key='Date', freq='ME'),\n              columns=is_commute,\n              values=\"Riders\",\n              aggfunc='sum')\n ).plot(linewidth=2, ax=ax)\nax.yaxis.set_major_formatter(comma_formatter())\nax.grid(alpha=.4)\nax.set(ylabel=\"Riders\", title=\"Trends in Commute versus Non-Commute Ridership\")\ncustom_legend(ax, title=\"Travel Time\", linewidth=3, loc=\"lower left\");\n\n\n\n\n\n\n\n\nIt does look like off peak and peak commute ridership had a growing gap in the pre-pandemic, but have been essentially equal in the post pandemic, interesting!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Peter Amerkhanian",
    "section": "",
    "text": "Researcher @ UC Office of the President\nUC Berkeley MPP ’23, BA ’16\n\n\nI’m a Data Scientist/Policy Analyst working in government. Check out my:\n\nTechnical blog\nProfessional reports and software"
  },
  {
    "objectID": "posts/pdf-scraping/index.html",
    "href": "posts/pdf-scraping/index.html",
    "title": "Reliable PDF Scraping with tabula-py",
    "section": "",
    "text": "Summary\n\nUse a combination of tabula‘s read_pdf() function and pandas’ various data manipulation functions in Python to accurately scrape .pdf files\n\n\n\nPrerequisites/Assumptions\n\nWindows 10 with administrator privileges (for setting environmental variables)\nJava SE Development Kit installed on your machine (download)\n\nset Java’s PATH environmental variable to point to the Java directory (see more here under “Get tabula-py working (Windows 10)”)\n\n\nPython version ~3.8 ish (I’m using Python 3.9.12 in Anaconda)\n\nAnaconda included packages - Pandas and NumPy\nLibraries maybe not included in Anaconda: requests, tabula-py\n\n\n\n\nProblem Narrative\nI’m interested in conducting a data analysis that involves the market value of single family homes in San Mateo County, California. This data can be hard to come by, but I’ve found a good county level resource – The San Mateo Association of Realtors’ “Market Data” page.\n\n\n\n\n\n\n\n\nFig 1: San Mateo Realtors Data Download Page\n\n\n\nHowever, to my dismay, I find that when I download one of these reports, I only get a .pdf containing a single table. It seems to be some sort of export of an Excel table, but the Association of Realtors has not made the actual spreadsheet available. Here is an example of one of their .pdf reports – in this case for April 2022:\n\n\n\n\n\n\n\n\nFig 2: Example PDF report :(\n\n\n\nThis is the exact data I want, but there are a few key issues: - The data are in .pdf files - You can only download monthly data files one at a time\n\n\nSolution\nI’ll solve this issue by writing a script to do the following: - Iterate through the urls of each of the monthly reports going back to 2011. For each report: - download its .pdf - parse and save the data from its .pdf\nStart with loading in the necessary libraries:\nimport pandas as pd\nimport numpy as np\nimport requests\nfrom tabula import read_pdf\nGet right into the script, which implements the pseudo-code I outlined above:\nagent = ['Mozilla/5.0 (Windows NT 10.0; Win64; x64)',\n         'AppleWebKit/537.36 (KHTML, like Gecko)',\n         'Chrome/91.0.4472.114 Safari/537.36']\nrows = []\nheaders = {'user-agent': \" \".join(agent)}\nfor year in range(2011, 2021):\n    for month in range(1, 12):\n        base = \"https://www.samcar.org/userfiles/file/salesstats/\"\n        url = base + f\"SF_{year}{str(month).zfill(2) }.pdf\"\n        print(url)\n        r = requests.get(url,\n                         stream=True,\n                         headers=headers)\n        open('holder.pdf', 'wb').write(r.content)\n        df = read_pdf(\"holder.pdf\", pages=\"all\")\n        table = df[0].iloc[-1, :]\n        table[\"date\"] = f\"{year}-{str(month).zfill(2)}\"\n        rows.append(table)\nNote: I’m defining agent in order to preempt being blocked by the site (read more).\nWhat’s remarkable about tabula.read_pdf() is that it just works. I didn’t have to really do any tinkering or iterating to get it going. Once it had access to the downloaded .pdf files, it easily and quickly parsed them.\nNow I run into something unique to this data – some of the .pdf tables had slightly different column names over the years. I implement a fix for that with the following code:\ncleaned_rows = []\nfor row in rows:\n    try:\n        new = row.rename(\n            {\"Sales\": \"Closed Sales\",\n             \"Sold\": \"Closed Sales\",\n             \"Avg Sales Price\": \"Average Sales Price\",\n             \"Avg SalePrice\": \"Average Sales Price\",\n             \"Unnamed: 3\": \"Closed Sales\",\n             \"Unnamed: 5\": \"Average Sales Price\"})[[\"date\",\n                                                    \"Closed Sales\",\n                                                    \"Average Sales Price\"]]\n        cleaned_rows.append(new.to_frame())\n    except KeyError:\n        print(\"******error\")\nWith the data retrieved and parsed, I perform some final cleaning and arrangement steps before exporting to a .csv\nall_years = pd.concat(cleaned_rows, axis=1)\n# Transpose the data and set `date` as the index\nfinal_df = all_years.T.set_index(\"date\")\n# Get the dollar signs and commas out. E.g. $1,658,900 -&gt; 1658900\nfinal_df[\"Average Sales Price\"] = (final_df[\"Average Sales Price\"]\n                                   .str.replace(\"[\\$,]\",\n                                                \"\",\n                                                regex=True)\n                                   .astype(int))\n# Closed Sales is discrete count data, so we convert to `int`\nfinal_df[\"Closed Sales\"] = final_df[\"Closed Sales\"].astype(int)\nfinal_df.to_csv(\"realtors_data_san_mateo.csv\")\nThe final product is a satisfying time series data set of the number of closed single family home sales and the average price of those sales over time.\n\n\n\n\n\n\n\n\nFig 3: Final .csv open in Excel\n\n\n\nThat’s surprisingly it.\n\n\nConclusion\nTabula-py is a very convenient and powerful .pdf parser (ported from Java) and easily handled basically all of the .pdfs I put through it.\n\n\n\n\n\n\nCitationBibTeX citation:@online{amerkhanian2022,\n  author = {Amerkhanian, Peter},\n  title = {Reliable {PDF} {Scraping} with Tabula-Py},\n  date = {2022-07-20},\n  url = {https://peter-amerkhanian.com/posts/pdf-scraping/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAmerkhanian, Peter. 2022. “Reliable PDF Scraping with\nTabula-Py.” July 20, 2022. https://peter-amerkhanian.com/posts/pdf-scraping/."
  },
  {
    "objectID": "posts/lagrange-cobb-douglas/index.html",
    "href": "posts/lagrange-cobb-douglas/index.html",
    "title": "Production Maximization with Lagrange Mutlipliers",
    "section": "",
    "text": "In “Simple Optimization in 3D”, I blogged about a basic optimization problem in three dimensional space. In this post, I’ll look at a more complex problem that deals with an equation constraint. I’ll utilize the method of lagrange multipliers optimization strategy to solve the problem.\n\n\nCode\nlibrary(plotly)\nlibrary(dplyr)\n\nscene &lt;- list(\n  camera = list(eye = list(\n    x = -2.2, y = 1.1, z = 1.2\n  )),\n  xaxis = list(title = \"L\"),\n  yaxis = list(title = \"K\"),\n  zaxis = list(title = \"$\")\n)\n\n\n\nThe Optimization Problem\n\nA company has determined that its production level is given by the Cobb-Douglas function \\(f(x,y)=2.5x^{0.45}y^{0.55}\\) where \\(x\\) represents the total number of labor hours in 1 year and \\(y\\) represents the total capital input for the company. Suppose 1 unit of labor costs $40 and 1 unit of capital costs $50. Use the method of Lagrange multipliers to find the maximum value of \\(f(x,y)=2.5x^{0.45}y^{0.55}\\) subject to a budgetary constraint of $500,000 per year.\n– (Strang and Herman 2016, chap. 4.8)\n\n\\[\n\\begin{align}\nf(x, y) \\rightarrow P(L, K) \\\\\nP(L, K) &= 2.5L^{0.45}K^{0.55} \\\\\ng(L, K) &= 40L + 50K - 500,000\n\\end{align}\n\\tag{1}\\]\nWe set the equation up in R so that we can inspect a plot and better understand the optimization problem.\n\nP_l_k &lt;- function(L, K) {\n  2.5 * L ^ (0.45) * K ^ (0.55)\n}\ng_l_k &lt;- function(L, K) {\n  40 * L + 50 * K - 500000\n}\nn &lt;- 100\nL &lt;- seq(0, 6000, length.out = n)\nK &lt;- seq(0, 6000, length.out = n)\nP &lt;- outer(L, K, P_l_k)\ng &lt;- outer(L, K, g_l_k)\n\n\n\nCode\nplot_ly(\n  x = L,\n  y = K,\n  z = P,\n  type = \"surface\",\n  name = \"P(L,K)\"\n) %&gt;%\n  colorbar(title = \"P(L,K)\") %&gt;%\n  add_trace(\n    x = L,\n    y = K,\n    z = g,\n    type = \"surface\",\n    colorscale = \"coolwarm\",\n    name = \"g(L,K)\",\n    colorbar = list(title = \"g(L,K)\")\n  ) %&gt;% layout(scene = scene)\n\n\n\n\n\n\nWe see that the production function \\(P\\) and the cost function \\(g\\) are surfaces that intersect. We are looking for the highest possible point in \\(P\\) that does not exceed the constraint \\(g\\), which will be somewhere around their intersection. Note that generally, all values below the intersection are possible, though not profit-maximizing, points. The points higher than the intersection are more profit-maximizing, but are not possible with this budget constraint.\n\n\nMaximizing using the Method of Lagrange Multipliers\nWe adapt the Lagrange multiplier problem-solving strategy from (Strang and Herman 2016, chap. 4.8) to our function input, and set up the following system of equations, which we will solve for \\(L_0\\) and \\(K_0\\):\n\\[\n\\begin{align*}\n\\nabla P(L_0, K_0) &= \\lambda \\nabla g(L_0, K_0) \\\\\ng(L_0, K_0) &= 0\n\\end{align*}\n\\tag{2}\\]\nAt this point, we will need to do some calculations to find each function in Equation 1’s gradient.\n\\[\n\\begin{align*}\n\\nabla P(L_0, K_0) &= \\left&lt;  \\frac{1.125K^{0.55}}{L^{0.55}} , \\frac{1.375L^{0.45}}{K^{0.45}}\\right&gt; \\\\\n\\nabla g(L_0, K_0) &= \\left&lt; 40, 50 \\right&gt; \\\\\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\n&\\begin{cases}\n\\left&lt;  \\frac{1.125K^{0.55}}{L^{0.55}} , \\frac{1.375L^{0.45}}{K^{0.45}}\\right&gt; &= \\lambda \\left&lt; 40, 50 \\right&gt; \\\\\n40L + 50K - 500,000 &= 0\n\\end{cases} \\\\\n&\\begin{cases}\n\\frac{1.125K^{0.55}}{L^{0.55}} &= 40 \\lambda\\\\\n\\frac{1.375L^{0.45}}{K^{0.45}} &= 50 \\lambda \\\\\n40L + 50K - 500,000 &= 0\n\\end{cases} \\\\\n&\\begin{cases}\n\\frac{1.125K^{0.55}}{40L^{0.55}} &= \\lambda\\\\\n\\frac{1.375L^{0.45}}{50K^{0.45}} &= \\lambda \\\\\n40L + 50K - 500,000 &= 0\n\\end{cases} \\\\\n&\\begin{cases}\n\\frac{1.125K^{0.55}}{40L^{0.55}} &= \\frac{1.375L^{0.45}}{50K^{0.45}} \\\\\n40L + 50K - 500,000 &= 0\n\\end{cases} \\\\\n&\\begin{cases}\n5.5L &= 5.625K \\\\\n40L + 50K - 500,000 &= 0\n\\end{cases} \\\\\n&\\begin{cases}\n5.5L- 5.625K &= 0 \\\\\n40L + 50K &= 500,000\n\\end{cases}\n\\end{align*}\n\\]\nWe now have a clear linear system of equations that we can solve via some substitution and algebraic manipulation:\n\\[\n\\begin{align*}\n&\\begin{cases}\nL &= \\frac{5.625K}{5.5} \\\\\n40 (\\frac{5.625K}{5.5}) + 50K &= 500,000 \\\\\n\\end{cases} \\\\\n&\\begin{cases}\nL &= \\frac{5.625K}{5.5} \\\\\nK(40 (\\frac{5.625}{5.5}) + 50) &= 500,000 \\\\\n\\end{cases} \\\\\n&\\begin{cases}\nL &= \\frac{5.625K}{5.5} \\\\\nK &= \\frac{500,000}{(40 (\\frac{5.625}{5.5}) + 50)} = 5,500\n\\end{cases} \\\\\n&\\begin{cases}\nL &= \\frac{5.625 (5,500)}{5.5} = 5,625 \\\\\nK &= 5,500\n\\end{cases} \\\\\n&\\begin{cases}\nL &= \\boxed{5,625 \\, \\text{labor hours}} \\\\\nK &= \\boxed{\\$ 5,500}\n\\end{cases}\n\\end{align*}\n\\]\nWe’ll now plug those values for capital and labor into our production function and see how much output this maximizing parameter combination produces (we’ll round given we are solving for whole output):\n\nP_l_k(5625, 5500) %&gt;% round()\n\n[1] 13890\n\n\nWhen we return to the plot of the product function and budget constraint, we can see that this point clearly is the highest possible output under the constraints.\n\n\nCode\nplot_ly(\n  x = L,\n  y = K,\n  z = P,\n  type = \"surface\",\n  name = \"P(L,K)\"\n) %&gt;%\n  colorbar(title = \"P(L,K)\") %&gt;%\n  add_trace(\n    x = L,\n    y = K,\n    z = g,\n    type = \"surface\",\n    colorscale = \"coolwarm\",\n    name = \"g(L,K)\",\n    colorbar = list(title = \"g(L,K)\")\n  ) %&gt;%\n  add_trace(\n    x = 5625,\n    y = 5500,\n    z = P_l_k(5625, 5500) %&gt;% round(),\n    type = \"scatter3d\",\n    mode = \"markers\",\n    marker = list(size = 5, color = \"black\")\n  ) %&gt;%\n  layout(scene = scene, legend=list(x=.5, y=0))\n\n\n\n\n\n\nHowever, in \\(R^3\\), contour plots offer a much clearer way of visualizing our solution.\n\n\nCode\nplot_ly(\n  x = L,\n  y = K,\n  z = P,\n  type = \"contour\",\n  name = \"P(L,K)\"\n) %&gt;%\n  colorbar(title = \"P(L,K)\") %&gt;%\n  add_trace(\n    x =  L,\n    y = 10000 - 4 * K / 5,\n    type = 'scatter',\n    mode = 'lines',\n    name = \"g(L, K)\",\n    color = \"red\"\n  ) %&gt;%\n  add_trace(\n    x = 5625,\n    y = 5500,\n    type = \"scatter\",\n    mode = \"markers\",\n    marker = list(\n      size = 10,\n      color = \"black\",\n      name = \"P(L*,K*)\"\n    )\n  ) %&gt;%\n  layout(xaxis = list(range = c(0, max(L))),\n         yaxis = list(range = c(0, max(K))))\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nStrang, Gilbert, and Edwin Herman. 2016. Calculus Volume 3. OpenStax.\n\nCitationBibTeX citation:@online{amerkhanian2024,\n  author = {Amerkhanian, Peter},\n  title = {Production {Maximization} with {Lagrange} {Mutlipliers}},\n  date = {2024-03-17},\n  url = {https://peter-amerkhanian.com/posts/lagrange-cobb-douglas/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAmerkhanian, Peter. 2024. “Production Maximization with Lagrange\nMutlipliers.” March 17, 2024. https://peter-amerkhanian.com/posts/lagrange-cobb-douglas/."
  },
  {
    "objectID": "posts/multiple-integration/index.html",
    "href": "posts/multiple-integration/index.html",
    "title": "Integrals in Probability",
    "section": "",
    "text": "Code\nlibrary(plotly)\nlibrary(dplyr)\n\nx_vec &lt;- seq(0, 25)\ny_vec &lt;- seq(0, 25)\n\nvline &lt;- function(x = 0, color = \"black\") {\n  list(\n    type = \"line\",\n    y0 = 0,\n    y1 = 1,\n    yref = \"paper\",\n    x0 = x,\n    x1 = x,\n    line = list(color = color, dash=\"dot\")\n  )\n}\nThe following are notes on using integration for finding cumulative probability distribution functions in the univariate and joint settings. The exercises and citations are all coming out of a textbook for a course in vector calculus (Strang and Herman 2016)."
  },
  {
    "objectID": "posts/multiple-integration/index.html#modeling-a-single-waiting-time",
    "href": "posts/multiple-integration/index.html#modeling-a-single-waiting-time",
    "title": "Integrals in Probability",
    "section": "Modeling a single waiting time",
    "text": "Modeling a single waiting time\nConsider the following question:\n\nAt a drive-thru restaurant, customers spend, on average, 3 minutes placing their orders […] find the probability that customers spend 6 minutes or less placing their order. […] Waiting times are mathematically modeled by exponential density functions.\n– (Strang and Herman 2016, chap. 5.2)\n\nTo model this and calculate our target quantity, we’ll start by referring to the process of placing one’s order as a random variable, \\(X\\). We will model that random variable using an exponential probability density function (pdf): \\[\nf(x) = \\begin{cases}\n\\frac{1}{\\beta} e^{-\\frac{x}{\\beta}} \\quad &x \\geq 0 \\\\\n0 \\quad &x &lt; 0\n\\end{cases}\n\\tag{1}\\]\nWhere \\(\\beta\\) is the average waiting time (Strang and Herman 2016, chap. 5.2).\nWe set the equation up in R and plot it so that we can understand what it represents.\n\nP_x &lt;- function(x, beta=3) {\n  case_when(x &lt; 0 ~ 0,\n            x &gt;= 0 ~ (1/beta) * exp(-x/beta)\n            )\n}\n\n\n\nCode\ndata &lt;- data.frame(x = x_vec, y = P_x(x_vec))\nggplot(data, aes(x = x, y = y)) +\n  geom_point(color = \"royalblue\", size=3.1, alpha=.8) +\n  geom_line(color = \"royalblue\", size=1.2, alpha=.7) +\n  labs(x = \"x (Minutes)\", y = \"f(x)\", title = \"An exponential probability density function (pdf)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nAt each \\(x\\), this function outputs the probability that it takes \\(x\\) minutes placing an order in the drive-thru. For example, the probability that it takes exactly six minutes to place an order is as follows:\n\nP_x(6)\n\n[1] 0.04511176\n\n\nThat’s what a pdf gives us and that’s cool. However, our question asks for the probability that it takes less than or equal to six minutes to order. That involves adding up all of the probabilities for times \\([0,6]\\) over this continuous function – sounds like an integral. Indeed, we’ll need to integrate over the following region:\n\n\nCode\nplot_ly(\n  x = x_vec,\n  y = P_x(x_vec),\n  type = \"scatter\",\n  mode=\"line\",\n  name = \"P(x)\") %&gt;% add_trace(\n    x = 0:6,\n    y = P_x(0:6, 3),\n    type = \"scatter\",\n    mode=\"none\",\n    fill = \"tozeroy\",\n    name = \"P(x) &lt;= 6\"\n) %&gt;% \n  layout(plot_bgcolor = \"#e5ecf6\", shapes = list(vline(6))\n         )\n\n\n\n\n\n\nWe can formally define that integral as follows:\n\\[\n\\begin{align*}\nP(X \\leq 6) &= \\int_0 ^6 f(x)dx \\\\\n&= \\int_0 ^6 \\frac{1}{\\beta} e^{-\\frac{x}{\\beta}}dx\n\\end{align*}\n\\] Now that we’ve defined the problem, we’ll solve it by-hand, then using symbolic computation software.\n\nSolving by hand\nAfter setting the problem up, we can solve it step-by-step using \\(u\\) substitution to compute the integral: \\[\n\\begin{align*}\nP(X \\leq 6) &= \\int_0 ^6 \\frac{1}{\\beta} e^{-\\frac{x}{\\beta}}dx \\\\\n\\text{Substitute}\\, u &= -\\frac{x}{\\beta} \\\\\n\\text{And substitute}\\, dx \\rightarrow \\, \\frac{du}{dx} &= -\\frac{1}{\\beta} \\rightarrow dx = -\\beta du \\\\\nP(X \\leq 6) &= \\int_0 ^6 \\frac{1}{\\beta} e^u (-\\beta du) \\\\\n&= -1\\int_0 ^6  e^u du \\\\\n&= - e^u \\Big|_0^6 \\\\\nP(X \\leq 6) &= - e^{-\\frac{x}{\\beta}} \\Big|_0^6 \\\\\n\\end{align*}\n\\]\nThe last line gives us the following algebra: \\(- e^{-\\frac{x}{\\beta}} \\Big|_0^6 = - e^{-\\frac{6}{3}} - (- e^{-\\frac{0}{3}}) = 1 - e^{-\\frac{6}{3}}\\) Leading us to our answer:\n\n(-exp(-6/3)) + 1\n\n[1] 0.8646647\n\n\nWhich is the probability that it takes six minutes or less to order in this drive-thru.\n\n\nSolving with sympy\nNow we’ll overview a more practically useful way of solving the integral using python’s sympy library. To switch to python, I’ll first load the reticulate library in R and specify that I want to use my conda install of python, which comes pre-loaded with sympy.\n\nlibrary(reticulate)\nuse_condaenv('base')\n\nNow I’ll switch to python code and set up some preliminaries – I define a custom render function so that I can output the sympy equations I create in LaTeX:\n\n\n\npython\n\nimport sympy as sp\nfrom IPython.display import Markdown\nrender = lambda x: Markdown(sp.latex(x, mode='equation'))\n\n\nNow I’ll define the variables, the constant \\(e\\),1 and Equation 1 in sympy. sympy will automatically apply some simplification to the expression, so don’t worry if it doesn’t look exactly like Equation 1.\n\n\n\npython\n\n# Define the variables\nx, beta = sp.symbols('x beta')\n# Define the constant, e\ne = sp.exp(1)\n# Define the equation\npdf = (1/beta) * e**(-x/beta)\n# Render the equation\nrender(pdf)\n\n\n\\[\\begin{equation}\\frac{e^{- \\frac{x}{\\beta}}}{\\beta}\\end{equation}\\]\n\n\nIn our specific problem, we know that the average time it takes to place an order is 3, so we’ll substitute in \\(\\beta=3\\).\n\n\n\npython\n\nf_x = pdf.subs({beta: 3})\nrender(f_x)\n\n\n\\[\\begin{equation}\\frac{e^{- \\frac{x}{3}}}{3}\\end{equation}\\]\n\n\nIntegration using sympy is trivially easy – to get the indefinite integral, we can just call .integrate() and supply the variable of integration.\n\n\n\npython\n\nrender(f_x.integrate(x)) \n\n\n\\[\\begin{equation}- e^{- \\frac{x}{3}}\\end{equation}\\]\n\n\nThe problem requires the definite integrate, which is just as straightforward – we supply the variable to integrate over, and the upper and lower limits of integration.\n\n\n\npython\n\nrender(f_x.integrate( (x, 0, 6) )) \n\n\n\\[\\begin{equation}1 - e^{-2}\\end{equation}\\]\n\n\nWe’ll retrieve the decimal output by running .evalf() on the integral.\n\n\n\npython\n\nf_x.integrate( (x, 0, 6) ).evalf()\n\n\n0.864664716763387\n\n\nMuch less painful than \\(u\\)-substitution!"
  },
  {
    "objectID": "posts/multiple-integration/index.html#cumulative-probability",
    "href": "posts/multiple-integration/index.html#cumulative-probability",
    "title": "Integrals in Probability",
    "section": "Cumulative Probability",
    "text": "Cumulative Probability\nWhen we derived the indefinite integral, \\(\\int \\frac{1}{\\beta} e^{-\\frac{x}{\\beta}}dx = - e^{-\\frac{x}{\\beta}}\\), in the process of solving our definite integral problem, we actually derived another useful function – the cumulative distribution function (cdf). The cumulative distribution function describes the probability that \\(x\\) is some number, say \\(a\\), or less, and it is generally the integral of the probability density function (“Cumulative Distribution Function” 2024). In the case of the exponential function, it is as follows:\n\\[\nF(a) = P(x \\leq a) = \\int_0^a P(x)dx =- e^{-\\frac{x}{\\beta}} \\Big|_0^a\n\\]\nOr, in code:\n\ncdf_x &lt;- function(x, beta=3){\n  -1 * exp(-x/beta)\n}\n\nWe’ll confirm that \\(F(6) = e^{-\\frac{x}{\\beta}} \\Big|_0^6\\)\n\n1 + cdf_x(6)\n\n[1] 0.8646647\n\n\nHere are the pdf and cdf functions plotted together for the exponential distribution with an average of 3 (\\(\\beta=3\\)). At each point on the orange line (the pdf), you can hover over and find out the probability that the drive-thru takes exactly \\(x\\) minutes, whereas on the blue line (the cdf), you’ll find the probability that the drive-thru takes less than or equal to \\(x\\) minutes.\n\n\nCode\nplot_ly(\n  x = x_vec,\n  y = 1 + cdf_x(x_vec),\n  type = \"scatter\",\n  mode=\"lines+markers\",\n  name = \"CDF: F(a)\") %&gt;% \n  add_trace(\n    x = x_vec,\n    y = P_x(x_vec),\n    type = \"scatter\",\n    mode=\"lines+markers\",\n    name = \"PDF: P(x)\"\n)"
  },
  {
    "objectID": "posts/multiple-integration/index.html#modeling-multiple-waiting-times",
    "href": "posts/multiple-integration/index.html#modeling-multiple-waiting-times",
    "title": "Integrals in Probability",
    "section": "Modeling multiple waiting times",
    "text": "Modeling multiple waiting times\n\nAt Sydney’s Restaurant, customers must wait an average of 15 minutes for a table. From the time they are seated until they have finished their meal requires an additional 40 minutes, on average. What is the probability that a customer spends less than an hour and a half at the diner, assuming that waiting for a table and completing the meal are independent events?\n– (Strang and Herman 2016, chap. 5.2)\n\nSince we are given two random variables and asked to find the probability of an event where something specific happens to each one, we can conceive of this problem using a joint density function. We are given that these two waiting times are independent (not correlated), so we can model joint density as follows:\n\nThe variables \\(X\\) and \\(Y\\) are said to be independent random variables if their joint density function is the product of their individual density functions: \\[\nf(x, y) = f_1(x)f_2(y)\n\\]\n– (Strang and Herman 2016, chap. 5.2)\n\nThe two functions given in the problem are:\n\nan exponential density function with mean 15 (waiting for the table)\n\n\\[\nf_1(x) = \\begin{cases}\n\\frac{1}{15} e^{-\\frac{x}{15}} \\quad &x \\geq 0 \\\\\n0 \\quad &x &lt; 0\n\\end{cases}\n\\]\n\nan exponential density function with mean 40 (eating at the table)\n\n\\[\nf_2(y) = \\begin{cases}\n\\frac{1}{40} e^{-\\frac{y}{40}} \\quad &y \\geq 0 \\\\\n0 \\quad &y &lt; 0\n\\end{cases}\n\\]\nThis gives us the joint density function:\n\\[\n\\begin{align*}\nf(x, y) &= f_1(x)f_2(y) \\\\\n&= \\left[\\begin{cases}\n\\frac{1}{40} e^{-\\frac{x}{40}} \\quad &x \\geq 0 \\\\\n0 \\quad &x &lt; 0\n\\end{cases} \\right] \\times \\left[ \\begin{cases}\n\\frac{1}{15} e^{-\\frac{y}{15}} \\quad &y \\geq 0 \\\\\n0 \\quad &y &lt; 0\n\\end{cases} \\right] \\\\\n&=\n\\begin{cases}\n\\frac{1}{600} e^{-\\frac{x}{40}}e^{-\\frac{y}{15}} \\quad &x,y \\geq 0 \\\\\n0 \\quad &x &lt; 0 \\,\\text{or}\\, y &lt; 0\n\\end{cases}\n\\end{align*}\n\\tag{2}\\]\nWe can set that up in code as well\n\npdf_function &lt;- function(x, y) {\n  table_wait &lt;- P_x(x, 15)\n  eating_time &lt;- P_x(y, 40)\n  total_time &lt;- table_wait * eating_time\n  return(total_time)\n}\n\nThis function allows us to find the probability of our table wait + eating time being a specific amount of minutes. For example, what’s the probability that it takes us 15 minutes to secure a table and 40 minutes to eat?\n\npdf_function(15, 40)\n\n[1] 0.0002255588\n\n\nThis is a really small number, which makes some level of intuitive sense – it seems remote that the whole restaurant experience will take exactly 55 minutes, no more no less.\nNow, returning to the question – we want to find the probability that the whole restaurant experience (waiting for a table and eating) takes an hour and a half (90 minutes) or less. Again, we’ll need to add up all of the probabilities from 0 to 90 minutes, implying that we will solve this with an integral. However, we are in a multivariate setting, so this will require multiple integration.\nTo set up the problem, we’ll plot the joint density function, the \\(xy\\) plane, and this 90 minute constraint.\n\n\nCode\nscene &lt;- list(\n  camera = list(eye = list(\n    x = 2, y = 1.1, z = 1.2\n  )),\n  xaxis = list(title = \"X\"),\n  yaxis = list(title = \"Y\"),\n  zaxis = list(title = \"P(X,Y)\")\n)\n# Define range for x and y\nx_range &lt;- seq(0, 50, length.out = 100)\ny_range &lt;- seq(0, 50, length.out = 100)\nxy_grid &lt;- expand.grid(x = x_range, y = y_range)\n\nz_values &lt;- pdf_function(xy_grid$x, xy_grid$y)\nz_plane &lt;- rep(0, nrow(xy_grid))\n\n# x y constraint\nx_range_constraint &lt;- seq(40, 50, length.out = 100)\ny_constraint &lt;- 90 - x_range_constraint\nxy_constraint_grid &lt;- expand.grid(x = x_range_constraint, y = y_constraint)\nz_constraint &lt;- rep(0, nrow(xy_grid))\n\n\n# Reshape z values to create a matrix for plotting\nz_matrix &lt;- matrix(z_values, nrow = length(x_range), ncol = length(y_range))\nz_constraint_matrix &lt;- matrix(z_constraint, nrow = length(x_range), ncol = length(y_constraint))\nz_plane_matrix &lt;- matrix(z_plane, nrow = length(x_range), ncol = length(y_range))\n\n\n# Create 3D plot using plot_ly\nplot_ly(x = x_range, y = y_range, z = z_plane_matrix, type = \"surface\") %&gt;% hide_colorbar() %&gt;%\n  add_trace(\n    x = x_range,\n    y = y_range,\n    z = z_matrix,\n    type = \"surface\",\n    colorscale = \"Cividis\",\n    colorbar = list(title = \"P(X,Y)\")\n  ) %&gt;%\n  add_trace(\n    x = x_range_constraint,\n    y = y_constraint,\n    z = 0,\n    type = \"scatter3d\",\n    name = \"X + Y &lt;= 90\",\n    mode=\"lines+markers\"\n  ) %&gt;% layout(scene = scene)\n\n\n\n\n\n\nLet’s first define a region of integration, \\(D\\). We are given \\(x+y \\leq 90\\), and given that we are talking about minutes, which are bounded \\([0, \\infty)\\), we can make that \\(0 \\leq x+y \\leq 90\\). With some algebraic manipulation, we can define the region as:\n\\[\nD = \\{(x,y) \\,|\\, 0 \\leq x \\leq90, 0 \\leq y \\leq 90-x  \\}\n\\]\nSetting that up as a definite integral, we get the following:\n\\[\nP(X+Y \\leq 90) = P( (X,Y) \\in D) = \\iint_D f(x,y) dA\n\\]\nGiven that there is no density for this function when \\(x\\) or \\(y\\) is less than zero, we can ignore the piece wise structure of the joint pdf and set up the integral using only the portion that defines the function on the domain \\(x \\in [0, \\infty)\\). This gives us:\n\\[\n\\begin{align*}\n&= \\int_0^{90} \\int_0^{90-x} \\frac{1}{600} e^{-\\frac{x}{40}}e^{-\\frac{y}{15}} dydx \\\\\n&= \\frac{1}{600} \\int_0^{90} \\int_0^{90-x} e^{-\\frac{x}{40}-\\frac{y}{15}} dydx\n\\end{align*}\n\\]\n\nSolving by hand\nComputing this integral is difficult, but we’ll walk through it. We will again use \\(u\\) substitution.\n\\[\n\\begin{align*}\nu &= -\\frac{x}{40}-\\frac{y}{15} \\\\\n\\frac{du}{dy} &= -\\frac{1}{15} \\rightarrow dy=-15du\\\\\n\\end{align*}\n\\] We can plug that in to finish the first integral:\n\\[\n\\begin{align*}\n&= \\frac{1}{600}\\int_0^{90} \\int_0^{90-x} e^{u} (-15)dudx \\\\\n&= \\frac{-15}{600}\\int_0^{90} \\int_0^{90-x} e^{u} dudx \\\\\n&= \\frac{-15}{600}\\int_0^{90} e^{u} \\Big|_0^{90-x}  dx \\\\\n&= \\frac{-15}{600}\\int_0^{90} e^{-\\frac{x}{40}-\\frac{y}{15}} \\Big|_0^{90-x}  dx \\\\\n&= \\frac{-15}{600}\\int_0^{90} e^{-\\frac{x}{40}-\\frac{90-x}{15}} - e^{-\\frac{x}{40}-\\frac{0}{15}}  dx \\\\\n&= \\frac{-15}{600}\\int_0^{90} e^{-\\frac{3x}{120}-\\frac{8(90-x)}{120}} - e^{-\\frac{x}{40}}dx \\\\\n&= \\frac{-15}{600}\\int_0^{90} e^{\\frac{-3x-720+8x}{120}} - e^{-\\frac{x}{40}}dx \\\\\n&= \\frac{-15}{600}\\int_0^{90} e^{\\frac{5x-720}{120}} - e^{-\\frac{x}{40}}dx \\\\\n&= \\frac{-15}{600}\\int_0^{90} e^{\\frac{x}{24}-6} - e^{-\\frac{x}{40}}dx\n\\end{align*}\n\\]\nThat was pretty difficult! Now we can continue on to the second integral and split it into two:\n\\[\n\\begin{align*}\n&= \\frac{-15}{600} \\left[ \\int_0^{90} e^{\\frac{x}{24}-6}dx - \\int_0^{90} e^{-\\frac{x}{40}}dx \\right]\n\\end{align*}\n\\]\nWe’ll solve these integrals with \\(u\\) and \\(v\\) substitution, with our terms defined as follows:\n\\[\n\\begin{align*}\nu &= \\frac{x}{24}-6 \\\\\n\\frac{du}{dx} &= \\frac{1}{24} \\rightarrow dx=24du\\\\\nv &= -\\frac{x}{40} \\\\\n\\frac{dv}{dx} &= -\\frac{1}{40} \\rightarrow dx=-40dv\\\\\n\\end{align*}\n\\]\nWhen we plug these terms back in, we can finally compute the definite integral.\n\\[\n\\begin{align*}\n&= \\frac{-15}{600} \\left[ 24 \\int_0^{90} e^{u}du + 40\\int_0^{90} e^v dv \\right] \\\\\n&= \\frac{-15}{600} \\left[ 24 e^{u}\\Big|_0^{90} + 40 e^v \\Big|_0^{90} \\right] \\\\\n&= \\frac{-15}{600} \\left[ 24 e^{\\frac{x}{24}-6}\\Big|_0^{90} + 40 e^{-\\frac{x}{40}}\\Big|_0^{90} \\right] \\\\\n&= \\frac{-15}{600} \\left[24(e^{\\frac{90}{24}-6} - e^{\\frac{0}{24}-6}) + 40(e^{-\\frac{90}{40}}-e^{-\\frac{0}{40}}) \\right] \\\\\n&= \\frac{-15}{600} \\left[24(e^{-2.25} - e^{-6}) + 40(e^{-2.25}-1) \\right] \\\\\n&= \\frac{-15}{600} \\left[ -33.3139396802807 \\right] \\\\\n&= 0.832848492007017\n\\end{align*}\n\\]\nThus, the probability that the total experience is 90 minutes or less is about 83%.\n\n\nSolving with sympy\nComputing that double integral by hand provides us with a lot of opportunities to make a small error and break the whole computation. In practice, we can solve problems like this in a much faster and less error-prone manner using symbolic computation software. We’ll again switch to python and use sympy.\nReturning to the problem,\n\\[\nP(X+Y \\leq 90) = \\frac{1}{600} \\int_0^{90} \\int_0^{90-x} e^{-\\frac{x}{40}-\\frac{y}{15}} dydx\n\\]\nWe’ll first define the function that we are integrating over.\n\n\n\npython\n\n# Define the variables\nx, y, = sp.symbols(\"x y\")\n# Define the function\nf_x_y = e ** (-x/40 - y/15)\nrender(f_x_y)\n\n\n\\[\\begin{equation}e^{- \\frac{x}{40} - \\frac{y}{15}}\\end{equation}\\]\n\n\nNow, we’ll compute the entire double integral in one line. Note that we chain the .integrate() method, each time over a different variable, to perform the multiple integration.\n\n\n\npython\n\n(1/600) * (\n  f_x_y\n  .integrate( (y, 0, 90-x) )\n  .integrate( (x, 0, 90) )\n  .evalf()\n  )\n\n\n0.832848492007017\n\n\nWe confirm that this is the same answer we arrived at by hand."
  },
  {
    "objectID": "posts/multiple-integration/index.html#deriving-a-generalizable-approach",
    "href": "posts/multiple-integration/index.html#deriving-a-generalizable-approach",
    "title": "Integrals in Probability",
    "section": "Deriving a generalizable approach",
    "text": "Deriving a generalizable approach\nLet’s look at another problem, but now, instead of solving just one specific problem, we’ll derive an equation that can easily solve any question structured like this so that don’t have to do so many integrals…\n\n[…] At a drive-thru restaurant, customers spend, on average, 3 minutes placing their orders and an additional 5 minutes paying for and picking up their meals. Assume that placing the order and paying for/picking up the meal are two independent events \\(X\\) and \\(Y\\). If the waiting times are modeled by the exponential probability densities. Find \\(P[X+Y \\leq 6]\\) and interpret the result.\n– (Strang and Herman 2016, chap. 5.2) - Question 119\n\nWe’ll again set up the joint density function: \\[\n\\begin{align*}\nf(x, y) &= f_1(x)f_2(y) \\\\\n&= \\left[\\begin{cases}\n\\frac{1}{3} e^{-\\frac{x}{3}} \\quad &x \\geq 0 \\\\\n0 \\quad &x &lt; 0\n\\end{cases} \\right] \\times \\left[ \\begin{cases}\n\\frac{1}{5} e^{-\\frac{y}{5}} \\quad &y \\geq 0 \\\\\n0 \\quad &y &lt; 0\n\\end{cases} \\right] \\\\\n&=\n\\begin{cases}\n\\frac{1}{15} e^{-\\frac{x}{3}}e^{-\\frac{y}{5}} \\quad &x,y \\geq 0 \\\\\n0 \\quad &x &lt; 0 \\,\\text{or}\\, y &lt; 0\n\\end{cases}\n\\end{align*}\n\\] and the region of integration appropriate for answering the question:\n\\[\nD = \\{(x,y) \\,|\\, 0 \\leq x \\leq6, 0 \\leq y \\leq 6-x  \\}\n\\] This leaves us with the integral: \\[\n\\begin{align*}\n\\frac{1}{15} \\int_0^{6} \\int_0^{6-x} e^{-\\frac{x}{3}-\\frac{y}{5}} dydx\n\\end{align*}\n\\]\nHowever, rather than solve this specifically, lets take the opportunity to derive a more general solution that can give us the target probability for any question phrased like this.\nWe’ll replace all the specific values:\n\n3, which is the average amount of minutes in the drive-thru, with \\(\\beta_1\\),\n\n5, which is the average amount of minutes spent ordering, with \\(\\beta_2\\), and\n\n6, which is the maximum number of minutes for the whole experience, with \\(c\\).\n\nThat gives us: \\[\n\\begin{align*}\n&= \\frac{1}{\\beta_1\\beta_2} \\int_0^{c} \\int_0^{c-x} e^{-\\frac{x}{\\beta_1}-\\frac{y}{\\beta_2}} dydx\n\\end{align*}\n\\]\n\nSolving by hand\nLet’s walk through one more painful double integral by-hand. We’ll start with \\(u\\) substitution:\n\\[\n\\begin{align*}\nu &= -\\frac{x}{\\beta_1}-\\frac{y}{\\beta_2} \\\\\n\\frac{du}{dy} &= -\\frac{1}{\\beta_2} \\rightarrow dy=-\\beta_2du\\\\\n\\end{align*}\n\\]\nAnd plug those in to solve the first definite integral:\n\\[\n\\begin{align*}\n&= \\frac{-1}{\\beta_1} \\int_0^{c} \\int_0^{c-x} e^{u} dudx \\\\\n&= \\frac{-1}{\\beta_1}\\int_0^{c} e^{-\\frac{x}{\\beta_1}-\\frac{y}{\\beta_2}} \\Big|_0^{c-x}  dx \\\\\n&= \\frac{-1}{\\beta_1}\\int_0^{c} e^{-\\frac{x}{\\beta_1}-\\frac{c-x}{\\beta_2}} - e^{-\\frac{x}{\\beta_1}-\\frac{0}{\\beta_2}}  dx \\\\\n&= \\frac{-1}{\\beta_1}\\int_0^{c} e^{-\\frac{\\beta_2x}{\\beta_1\\beta_2}-\\frac{\\beta_1(c-x)}{\\beta_1\\beta_2}} - e^{-\\frac{x}{\\beta_1}}dx \\\\\n&= \\frac{-1}{\\beta_1}\\int_0^{c} e^{\\frac{-\\beta_2x - \\beta_1c+\\beta_1x}{\\beta_1\\beta_2}} - e^{-\\frac{x}{\\beta_1}}dx \\\\\n&= \\frac{-1}{\\beta_1} \\left[ \\int_0^{c} e^{\\frac{(\\beta_1-\\beta_2)x - \\beta_1c}{\\beta_1\\beta_2}}dx - \\int_0^{c} e^{-\\frac{x}{\\beta_1}}dx \\right] \\\\\n\\end{align*}\n\\] Now we’ll use \\(u\\) and \\(v\\) substitution to solve the last definite integral:\n\\[\n\\begin{align*}\nu &= \\frac{(\\beta_1-\\beta_2)x - \\beta_1c}{\\beta_1\\beta_2} \\\\\n\\frac{du}{dx} &= \\frac{(\\beta_1-\\beta_2)}{\\beta_1\\beta_2} \\rightarrow dx=\\frac{\\beta_1\\beta_2}{(\\beta_1-\\beta_2)}du\\\\\nv &= -\\frac{x}{\\beta_1} \\\\\n\\frac{dv}{dx} &= -\\frac{1}{\\beta_1} \\rightarrow dx=-\\beta_1dv\\\\\n\\end{align*}\n\\]\nPlugging those in, we arrive at a general function of \\(\\beta_1\\), \\(\\beta_2\\), and \\(c\\).\n\\[\n\\begin{align*}\n&= \\frac{-1}{\\beta_1} \\left[ \\int_0^{c} e^{u}\\frac{\\beta_1\\beta_2}{(\\beta_1-\\beta_2)}du - \\int_0^{c} e^{v}(-\\beta_1dv) \\right] \\\\\n&= \\frac{-1}{\\beta_1} \\left[ \\frac{\\beta_1\\beta_2}{(\\beta_1-\\beta_2)}\\int_0^{c} e^{u}du - (-\\beta_1)\\int_0^{c} e^{v}dv \\right] \\\\\n&= \\frac{-1}{\\beta_1} \\left[ \\frac{\\beta_1\\beta_2}{(\\beta_1-\\beta_2)} (e^{\\frac{(\\beta_1-\\beta_2)x - \\beta_1c}{\\beta_1\\beta_2}}\\Big|_0^{c}) + \\beta_1 (e^{-\\frac{x}{\\beta_1}}\\Big|_0^{c}) \\right] \\\\\n&=\\left[ \\frac{-\\beta_2}{(\\beta_1-\\beta_2)} (e^{\\frac{(\\beta_1-\\beta_2)c - \\beta_1c}{\\beta_1\\beta_2}} - e^{\\frac{-\\beta_1c}{\\beta_1\\beta_2}}) - (e^{-\\frac{c}{\\beta_1}} - e^{-\\frac{0}{\\beta_1}}) \\right] \\\\\n&=  \\frac{-\\beta_2}{(\\beta_1-\\beta_2)} \\left[e^{\\frac{(\\beta_1-\\beta_2)c - \\beta_1c}{\\beta_1\\beta_2}} - e^{\\frac{-\\beta_1c}{\\beta_1\\beta_2}}\\right] - e^{-\\frac{c}{\\beta_1}} + 1  \\\\\nf(\\beta_1, \\beta_2, c)&=  \\frac{-\\beta_2}{(\\beta_1-\\beta_2)} \\left[e^{\\frac{-c}{\\beta_1}} - e^{\\frac{-c}{\\beta_2}}\\right] - e^{-\\frac{c}{\\beta_1}} + 1  \\\\\n\\end{align*}\n\\]\nNow we’ll code that equation up in R or the calculator of your choice.\n\ncdf_wait &lt;- function(beta1, beta2, c) {\n  (-beta2/(beta1-beta2)) * \n  (exp((-c)/beta1) - exp((-c)/beta2)) -\n  exp((-c)/beta1) + 1 \n}\n\nGoing back to our question – let’s see what’s the probability that the drive-thru experience takes 6 minutes or less if waiting to order takes 3 minutes on average and ordering takes 5 minutes on average. We can very easily calculate it with the flexible function \\(f(\\beta_1, \\beta_2, c)\\) that we just derived:\n\ncdf_wait(3, 5, 6)\n\n[1] 0.4500174\n\n\nThere’s a ~45% chance that a customer will spend 6 minutes or less in the drive-thru. We can also use our function, \\(f(\\beta_1, \\beta_2, c)\\), to easily compute and visualize the probability for any total drive-thru time given these two average times.\n\n\nCode\nplot_ly(\n  x = x_vec,\n  y = cdf_wait(3, 5, x_vec),\n  type = \"scatter\",\n  mode=\"lines+markers\",\n  name = \"CDF: F(a)\")\n\n\n\n\n\n\nAnd, going back to the first multiple integration question about Sydney’s restaurant Equation 2, we can now answer that with the same function.\n\ncdf_wait(15, 40, 90)\n\n[1] 0.8328485\n\n\nWe again can find the cumulative probability for any waiting time given the average time to get a table (15) and eat (40).\n\n\nCode\nplot_ly(\n  x = seq(0, 150),\n  y = cdf_wait(15, 40, seq(0, 150)),\n  type = \"scatter\",\n  mode=\"lines+markers\",\n  name = \"CDF: F(a)\")\n\n\n\n\n\n\n\n\nSolving with sympy\nFor the sake of completeness, we’ll derive that same generizable equation using sympy. Returning to the problem,\n\\[\n\\begin{align*}\nP(X + Y \\leq x) &= \\frac{1}{\\beta_1\\beta_2} \\int_0^{c} \\int_0^{c-x} e^{-\\frac{x}{\\beta_1}-\\frac{y}{\\beta_2}} dydx\n\\end{align*}\n\\]\nwe’ll define the constants and function in sympy.\n\n\n\npython\n\nx, y = sp.symbols('x y')\nbeta1, beta2, c = sp.symbols('beta1 beta2 c')\nf_x_y_general = e**(-x/beta1 - y/beta2)\nrender(f_x_y_general)\n\n\n\\[\\begin{equation}e^{- \\frac{y}{\\beta_{2}} - \\frac{x}{\\beta_{1}}}\\end{equation}\\]\n\n\nWe compute the double integral, and define our generalizable function as f_b1_b2_c.\n\n\n\npython\n\nf_b1_b2_c = ( 1/(beta1 * beta2) ) * (\n  f_x_y_general\n  .integrate( (y, 0, c - x) )\n  .integrate( (x, 0, c) )\n)\nrender(f_b1_b2_c.simplify())\n\n\n\\[\\begin{equation}\\begin{cases} \\frac{\\beta_{1} - \\beta_{1} e^{- \\frac{c}{\\beta_{1}}} - \\beta_{2} + \\beta_{2} e^{- \\frac{c}{\\beta_{2}}}}{\\beta_{1} - \\beta_{2}} & \\text{for}\\: \\beta_{1} \\neq \\beta_{2} \\\\1 - e^{- \\frac{c}{\\beta_{1}}} - \\frac{c}{\\beta_{1}} & \\text{otherwise} \\end{cases}\\end{equation}\\]\n\n\nNote that for more complex equations like this, sympy’s simplified output might not look like what you’d reach by hand. We can be more confident that this is indeed the same \\(f(\\beta_1, \\beta_2, c)\\) as the one we computed by hand above by testing it with a few problems and confirming the expected output.\nReturning to the drive-thru experience, where waiting to order takes 3 minutes on average and ordering takes 5, we can compute the probability that the whole experience takes 6 minutes or less as follows:\n\n\n\npython\n\nf_b1_b2_c.subs({beta1:  3, beta2: 5, c:6}).evalf()\n\n\n0.450017395074414\n\n\nThis is the same as what we get using the by-hand function.\n\ncdf_wait(3, 5, 6)\n\n[1] 0.4500174\n\n\nIn the case of the first problem, with a restaurant where ordering takes 40 minutes on average and eating takes 15, we can compute the probability that the whole experience takes 90 minutes or less just as easily:\n\n\n\npython\n\nf_b1_b2_c.subs({beta1:  40, beta2: 15, c:90}).evalf()\n\n\n0.832848492007017\n\n\nAgain, let’s check this with the by-hand version.\n\ncdf_wait(15, 40, 90)\n\n[1] 0.8328485\n\n\nGreat!"
  },
  {
    "objectID": "posts/multiple-integration/index.html#footnotes",
    "href": "posts/multiple-integration/index.html#footnotes",
    "title": "Integrals in Probability",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is not strictly necessary – I do it purely so that I can reference e in my code and match the problem notation↩︎"
  }
]